{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNaDbkwJAjn14mAYKYUsQ7h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajuX75/Basic-Transformer-Chat-Demo/blob/main/Basic_Transformer_Chat_Component_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdqYTyWTuerR",
        "outputId": "3fdbe5a0-5c6f-4241-8df2-5e5a83e9ae77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.18.0\n",
            "GPU is available.\n"
          ]
        }
      ],
      "source": [
        "# @title Cell 1: Imports\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import time\n",
        "import os\n",
        "import re\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "# Ensure GPU is available\n",
        "gpu_available = tf.config.list_physical_devices('GPU')\n",
        "if gpu_available:\n",
        "    print(\"GPU is available.\")\n",
        "    tf.config.experimental.set_memory_growth(gpu_available[0], True)\n",
        "else:\n",
        "    print(\"No GPU available, using CPU. Training will be slow.\")\n",
        "\n",
        "# Set up a basic random seed for reproducibility (optional)\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 2: Data Preparation (Using Local Cornell Corpus Zip)\n",
        "\n",
        "# --- Use Locally Uploaded Dataset ---\n",
        "# Assuming you have manually uploaded 'cornell_movie_dialogs_corpus.zip'\n",
        "# to your Colab working directory (/content/)\n",
        "zip_file_path = '/content/cornell_movie_dialogs_corpus.zip'\n",
        "\n",
        "# Directory to extract the contents into\n",
        "extraction_base_dir = '/content/cornell_dialogs_extracted/' # Choose a descriptive name\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "os.makedirs(extraction_base_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Extracting dataset from: {zip_file_path}\")\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extraction_base_dir)\n",
        "    print(f\"Dataset extracted to: {extraction_base_dir}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Zip file not found at {zip_file_path}\")\n",
        "    print(\"Please ensure 'cornell_movie_dialogs_corpus.zip' is uploaded to /content/\")\n",
        "    # Exit or handle error\n",
        "    raise # Re-raise the error to stop execution\n",
        "except zipfile.BadZipFile:\n",
        "    print(f\"Error: Bad zip file - could not extract {zip_file_path}\")\n",
        "    print(\"Please ensure the zip file is not corrupted.\")\n",
        "    raise\n",
        "\n",
        "\n",
        "# The actual data files are typically located inside the folder named 'cornell movie-dialogs corpus'\n",
        "# within the extracted contents. Note the spaces in the folder name.\n",
        "actual_data_folder_name_in_zip = \"cornell movie-dialogs corpus\"\n",
        "\n",
        "# Construct the full path to the directory containing the data files\n",
        "dataset_dir = os.path.join(extraction_base_dir, actual_data_folder_name_in_zip)\n",
        "\n",
        "print(f\"Actual data files are located in: {dataset_dir}\")\n",
        "\n",
        "# --- Load and Parse the Data ---\n",
        "\n",
        "# File paths within the actual data directory\n",
        "lines_filepath = os.path.join(dataset_dir, 'movie_lines.txt')\n",
        "conversations_filepath = os.path.join(dataset_dir, 'movie_conversations.txt')\n",
        "\n",
        "# Dictionary to map line IDs to text\n",
        "id2line = {}\n",
        "# Use 'latin-1' encoding as specified for this dataset\n",
        "try:\n",
        "    with open(lines_filepath, 'r', encoding='latin-1') as f:\n",
        "        for line in f:\n",
        "            parts = line.split(' +++$+++ ')\n",
        "            if len(parts) == 5:\n",
        "                # Format: lineID +++$+++ characterID ++++$+++ movieID ++++$+++ character ++++$+++ text\n",
        "                # Store line ID and the text\n",
        "                line_id = parts[0].strip()\n",
        "                text = parts[4].strip()\n",
        "                id2line[line_id] = text\n",
        "except FileNotFoundError:\n",
        "     print(f\"Error: Could not find movie_lines.txt at {lines_filepath}\")\n",
        "     print(\"Please check the extraction path and the folder name 'cornell movie-dialogs corpus' inside the zip.\")\n",
        "     # Exit or handle error appropriately if file isn't found after expected extraction\n",
        "     raise # Re-raise the error to stop execution\n",
        "\n",
        "\n",
        "# List of conversation line IDs\n",
        "conversations = []\n",
        "try:\n",
        "    with open(conversations_filepath, 'r', encoding='latin-1') as f:\n",
        "        for line in f:\n",
        "            parts = line.split(' +++$+++ ')\n",
        "            if len(parts) == 4:\n",
        "                # Format: characterID1 +++$+++ characterID2 ++++$+++ movieID ++++$+++ utterance sequence\n",
        "                # The last part is a string representation of a list of line IDs, e.g., \"['L1', 'L2', 'L3', ...]\"\n",
        "                line_ids_str = parts[3].strip()\n",
        "                # Safely convert string representation of list to a Python list\n",
        "                try:\n",
        "                    # Ensure we are parsing a string that looks like a Python list\n",
        "                    if line_ids_str.startswith('[') and line_ids_str.endswith(']'):\n",
        "                        line_ids = eval(line_ids_str) # Example: ['L194', 'L195', 'L196']\n",
        "                        # Ensure the result is actually a list before appending\n",
        "                        if isinstance(line_ids, list):\n",
        "                            conversations.append(line_ids)\n",
        "                        else:\n",
        "                             print(f\"Warning: eval did not return a list for '{line_ids_str}'. Skipping.\")\n",
        "                    else:\n",
        "                         print(f\"Warning: Conversation line IDs string did not start/end with brackets: '{line_ids_str}'. Skipping.\")\n",
        "\n",
        "                except Exception as e:\n",
        "                     print(f\"Warning: Could not parse line IDs in conversation: '{line_ids_str}' - {e}\")\n",
        "                     # Skip this conversation if parsing fails\n",
        "                     continue\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Could not find movie_conversations.txt at {conversations_filepath}\")\n",
        "    print(\"Please check the extraction path and the folder name 'cornell movie-dialogs corpus' inside the zip.\")\n",
        "    # Exit or handle error appropriately\n",
        "    raise # Re-raise the error to stop execution\n",
        "\n",
        "\n",
        "print(f\"Loaded {len(id2line)} lines and {len(conversations)} conversations.\")\n",
        "\n",
        "# --- Create Question-Answer Pairs ---\n",
        "\n",
        "# We will create pairs of (current turn, next turn)\n",
        "qa_pairs = []\n",
        "for conversation in conversations:\n",
        "    # Iterate through the conversation, creating pairs (line i, line i+1)\n",
        "    # Only iterate up to the second to last line to have a valid pair\n",
        "    for i in range(len(conversation) - 1):\n",
        "        input_line_id = conversation[i]\n",
        "        target_line_id = conversation[i+1]\n",
        "\n",
        "        # Get the text using the IDs, use .get with a default None to handle missing IDs gracefully\n",
        "        input_text = id2line.get(input_line_id)\n",
        "        target_text = id2line.get(target_line_id)\n",
        "\n",
        "        # Ensure both lines exist and are not empty strings after stripping\n",
        "        if input_text and target_text and input_text.strip() and target_text.strip():\n",
        "            qa_pairs.append([input_text, target_text])\n",
        "        # Optional: Log if a pair is skipped (can be noisy)\n",
        "        # else:\n",
        "        #     print(f\"Skipping pair for line IDs {input_line_id}, {target_line_id} due to missing or empty text.\")\n",
        "\n",
        "\n",
        "print(f\"Created {len(qa_pairs)} question-answer pairs.\")\n",
        "\n",
        "# --- Basic Preprocessing (Same as before, applied to new data) ---\n",
        "\n",
        "# 1. Clean and Normalize Text\n",
        "# Keep this function definition the same as the previous corrected version\n",
        "def clean_text(text):\n",
        "    text = str(text) # Ensure text is string type\n",
        "    text = text.lower()\n",
        "    # Keep letters, numbers, and some punctuation (like ? . ! , '), replace others with space\n",
        "    text = re.sub(r\"[^a-zA-Z0-9?.!,']+\", \" \", text) # Keep apostrophes for contractions\n",
        "    # Optional: clean up multiple spaces\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    # Ensure text is not empty after cleaning before adding tokens\n",
        "    if not text:\n",
        "        return '' # Return empty string if cleaning results in empty text\n",
        "\n",
        "    # Add start and end tokens (essential for seq2seq)\n",
        "    text = '<start> ' + text + ' <end>'\n",
        "    return text\n",
        "\n",
        "# Apply cleaning to all pairs\n",
        "# Create a new list to hold cleaned pairs, skipping pairs where cleaning resulted in empty text\n",
        "cleaned_data = []\n",
        "for q, a in qa_pairs:\n",
        "    cleaned_q = clean_text(q)\n",
        "    cleaned_a = clean_text(a)\n",
        "    # Only keep the pair if both cleaned question and answer are not empty\n",
        "    if cleaned_q and cleaned_a:\n",
        "        cleaned_data.append([cleaned_q, cleaned_a])\n",
        "    # else:\n",
        "    #     print(f\"Skipping pair: Original Q='{q}', A='{a}' resulted in empty cleaned text.\")\n",
        "\n",
        "\n",
        "print(f\"\\nCleaned {len(cleaned_data)} valid pairs after applying text cleaning.\")\n",
        "print(\"Cleaned Data Example:\", cleaned_data[0])\n",
        "\n",
        "# If no valid data remains, stop here\n",
        "if not cleaned_data:\n",
        "    raise ValueError(\"No valid question-answer pairs found after cleaning. Check data source and cleaning function.\")\n",
        "\n",
        "\n",
        "# 2. Create Vocabulary and Tokenizer\n",
        "# Use a smaller vocabulary size to manage memory in Colab, or let it be full initially\n",
        "# num_words=None means keep all words found\n",
        "# oov_token='<unk>' handles words not in vocabulary during prediction\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', split=' ', oov_token='<unk>')\n",
        "\n",
        "# Fit on all cleaned text\n",
        "tokenizer.fit_on_texts([q + ' ' + a for q, a in cleaned_data])\n",
        "\n",
        "# Add special tokens to word_index if not already there (clean_text adds them before fit)\n",
        "# This ensures their IDs are properly mapped. Keras Tokenizer assigns IDs from 1 up.\n",
        "# 0 is reserved for padding. <unk> is handled by oov_token.\n",
        "# Check if special tokens were included in vocabulary during fitting\n",
        "if '<start>' not in tokenizer.word_index:\n",
        "    print(\"Warning: '<start>' not found in vocabulary after fitting.\")\n",
        "if '<end>' not in tokenizer.word_index:\n",
        "    print(\"Warning: '<end>' not found in vocabulary after fitting.\")\n",
        "if '<unk>' not in tokenizer.word_index:\n",
        "    # If oov_token was used, <unk> should be in word_index\n",
        "     print(\"Warning: '<unk>' not found in vocabulary after fitting, despite using oov_token.\")\n",
        "\n",
        "\n",
        "# Convert text to sequences of integers\n",
        "input_sequences = tokenizer.texts_to_sequences([q for q, a in cleaned_data])\n",
        "target_sequences = tokenizer.texts_to_sequences([a for q, a in cleaned_data])\n",
        "\n",
        "print(\"\\nOriginal sequences example:\", input_sequences[0])\n",
        "print(\"Target sequences example:\", target_sequences[0])\n",
        "\n",
        "# 3. Padding sequences to a fixed length\n",
        "# Determine a reasonable max length (e.g., 95th percentile of sequence lengths)\n",
        "all_lengths = [len(seq) for seq in input_sequences + target_sequences]\n",
        "# Handle case where all_lengths is empty or contains only 0s after filtering\n",
        "if not all_lengths or max(all_lengths) == 0:\n",
        "     print(\"Error: No sequences with length > 0 found. Cannot determine MAX_LENGTH.\")\n",
        "     MAX_LENGTH = 10 # Set a default or handle as error (e.g., raise ValueError)\n",
        "else:\n",
        "    # Calculate a percentile length, cap at a reasonable value if needed for memory\n",
        "    MAX_LENGTH = int(np.percentile(all_lengths, 95))\n",
        "    # Optional: Set a hard cap on max length if 95th percentile is too large\n",
        "    # MAX_LENGTH = min(MAX_LENGTH, 80) # Example cap at 80 tokens - adjust based on memory\n",
        "\n",
        "print(f\"Calculated MAX_LENGTH (95th percentile capped/uncapped): {MAX_LENGTH}\")\n",
        "print(f\"Number of pairs to pad: {len(input_sequences)}\")\n",
        "\n",
        "# Apply padding\n",
        "input_padded = tf.keras.preprocessing.sequence.pad_sequences(input_sequences,\n",
        "                                                             maxlen=MAX_LENGTH,\n",
        "                                                             padding='post')\n",
        "target_padded = tf.keras.preprocessing.sequence.pad_sequences(target_sequences,\n",
        "                                                              maxlen=MAX_LENGTH,\n",
        "                                                              padding='post')\n",
        "\n",
        "print(\"\\nPadded input sequence example:\", input_padded[0])\n",
        "print(\"Padded target sequence example:\", target_padded[0])\n",
        "\n",
        "# The vocabulary size is based on the number of unique words found by the tokenizer + 1 for padding (ID 0)\n",
        "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
        "print(\"Vocabulary size:\", VOCAB_SIZE) # Should be much larger now\n",
        "print(\"Number of final training pairs:\", len(input_padded))\n",
        "\n",
        "# 4. Create tf.data.Dataset\n",
        "# Use a smaller buffer size for shuffling if BUFFER_SIZE is too large for memory\n",
        "BUFFER_SIZE = 20000 # A good balance for shuffling a large dataset without huge memory\n",
        "BATCH_SIZE = 64 # Use a larger batch size, adjust based on GPU memory\n",
        "\n",
        "# Ensure input_padded and target_padded are numpy arrays or tensors for from_tensor_slices\n",
        "input_padded = np.array(input_padded)\n",
        "target_padded = np.array(target_padded)\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_padded, target_padded))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.AUTOTUNE) # Overlap data preprocessing and model execution\n",
        "\n",
        "# Calculate total steps *after* batching\n",
        "# Use the correct constant for unknown cardinality\n",
        "total_steps_per_epoch = tf.data.experimental.cardinality(dataset).numpy()\n",
        "# CORRECTED LINE: Use tf.data.UNKNOWN_CARDINALITY instead of tf.data.experimental.UNKNOWN\n",
        "if total_steps_per_epoch == tf.data.UNKNOWN_CARDINALITY:\n",
        "     print(\"Warning: Dataset cardinality is unknown. Using estimated steps.\")\n",
        "     total_steps_per_epoch = len(input_padded) // BATCH_SIZE\n",
        "     if len(input_padded) % BATCH_SIZE != 0:\n",
        "         total_steps_per_epoch += 1 # Account for partial last batch\n",
        "     # Ensure total_steps_per_epoch is at least 1 if data exists\n",
        "     total_steps_per_epoch = max(1, total_steps_per_epoch)\n",
        "\n",
        "\n",
        "print(\"\\nDataset prepared.\")\n",
        "\n",
        "# Pass total_steps_per_epoch to Cell 11 via a global variable (or ensure Cell 11 calculates it)\n",
        "# Let's make it a global variable accessible to the next cell\n",
        "global total_steps_per_epoch_global\n",
        "total_steps_per_epoch_global = total_steps_per_epoch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "bDX8OL2ru9j_",
        "outputId": "1f810a78-718a-457c-b1ec-66de06c5fbe3"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset from: /content/cornell_movie_dialogs_corpus.zip\n",
            "Dataset extracted to: /content/cornell_dialogs_extracted/\n",
            "Actual data files are located in: /content/cornell_dialogs_extracted/cornell movie-dialogs corpus\n",
            "Loaded 304713 lines and 83097 conversations.\n",
            "Created 221282 question-answer pairs.\n",
            "\n",
            "Cleaned 221282 valid pairs after applying text cleaning.\n",
            "Cleaned Data Example: ['<start> can we make this quick? roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad. again. <end>', \"<start> well, i thought we'd start with pronunciation, if that's okay with you. <end>\"]\n",
            "\n",
            "Original sequences example: [2, 47, 23, 100, 22, 24158, 84339, 84340, 9, 6338, 20175, 35, 416, 71, 4898, 84341, 1286, 630, 58, 33, 6, 84342, 336, 3]\n",
            "Target sequences example: [2, 79, 5, 152, 676, 361, 31, 53075, 41, 44, 790, 31, 55, 3]\n",
            "Calculated MAX_LENGTH (95th percentile capped/uncapped): 33\n",
            "Number of pairs to pad: 221282\n",
            "\n",
            "Padded input sequence example: [    2    47    23   100    22 24158 84339 84340     9  6338 20175    35\n",
            "   416    71  4898 84341  1286   630    58    33     6 84342   336     3\n",
            "     0     0     0     0     0     0     0     0     0]\n",
            "Padded target sequence example: [    2    79     5   152   676   361    31 53075    41    44   790    31\n",
            "    55     3     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0]\n",
            "Vocabulary size: 121388\n",
            "Number of final training pairs: 221282\n",
            "\n",
            "Dataset prepared.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 3: Hyperparameters and Model Parameters\n",
        "\n",
        "# These values are now determined in Cell 2 based on the Cornell dataset\n",
        "# Make sure you run Cell 2 *before* running this cell for the values to be correct.\n",
        "VOCAB_SIZE = VOCAB_SIZE       # From Cell 2\n",
        "MAX_LENGTH = MAX_LENGTH       # From Cell 2\n",
        "BATCH_SIZE = BATCH_SIZE       # From Cell 2\n",
        "\n",
        "EMBEDDING_DIM = 128\n",
        "UNITS = 512\n",
        "NUM_HEADS = 8\n",
        "D_MODEL = EMBEDDING_DIM # Dimension of the model (embedding size)\n",
        "FFN_UNITS = UNITS # Dimension of the feed-forward network inner layer\n",
        "\n",
        "# Training parameters\n",
        "EPOCHS = 20 # Reduced epochs for quicker demo with larger data, increase if needed"
      ],
      "metadata": {
        "id": "9XKPNn8fv_tX"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 4: Positional Encoding\n",
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                            np.arange(d_model)[np.newaxis, :],\n",
        "                            d_model)\n",
        "\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "# Example visualization (optional)\n",
        "# pos_encoding_example = positional_encoding(50, 128)\n",
        "# print(pos_encoding_example.shape) # Expected: (1, 50, 128)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "UH8kfyvwwELO"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 5: Masking\n",
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    # add extra dimensions to add the padding to the attention logits.\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :] # Shape (batch_size, 1, 1, seq_len)\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    # mask is a lower triangular matrix.\n",
        "    # We return the *upper* triangular part which becomes 1s\n",
        "    # where we *want* to mask (look ahead tokens).\n",
        "    return tf.cast(mask, tf.float32) # Shape (size, size)\n",
        "\n",
        "# Combine masks for the decoder's first attention layer\n",
        "def create_masks(inp, tar):\n",
        "    # Encoder padding mask\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    # Used in the 2nd attention block in the decoder.\n",
        "    # This padding mask is used to mask the encoder outputs.\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    # Used in the 1st attention block in the decoder.\n",
        "    # It is used to pad and mask future tokens in the input received by the decoder.\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "metadata": {
        "cellView": "form",
        "id": "xP9kOP_IwHBG"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 6: Core Transformer Components (Layers)\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Ensure d_model is divisible by num_heads\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads # Depth of each attention head\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model) # Query weight matrix\n",
        "        self.wk = tf.keras.layers.Dense(d_model) # Key weight matrix\n",
        "        self.wv = tf.keras.layers.Dense(d_model) # Value weight matrix\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model) # Output dense layer\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "           Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def scaled_dot_product_attention(self, q, k, v, mask):\n",
        "        \"\"\"Calculate the attention weights.\n",
        "        q, k, v must have matching leading dimensions.\n",
        "        k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "        The mask has different shapes depending on its type (padding or look ahead)\n",
        "        but both shapes are broadcastable.\n",
        "\n",
        "        Args:\n",
        "          q: query shape == (..., seq_len_q, depth)\n",
        "          k: key shape == (..., seq_len_k, depth)\n",
        "          v: value shape == (..., seq_len_v, depth_v)\n",
        "          mask: Float tensor with shape broadcastable\n",
        "                to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "          output, attention_weights\n",
        "        \"\"\"\n",
        "        # Matmul(Q, K_T)\n",
        "        matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "        # scale by sqrt(depth)\n",
        "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "        # add the mask to the scaled attention logits.\n",
        "        if mask is not None:\n",
        "            # Add a large negative number to masked positions.\n",
        "            # When softmax is applied, these positions get nearly zero weight.\n",
        "            scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "        # softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1.\n",
        "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "        output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth_v)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = self.scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        # Concatenate heads\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "        concat_attention = tf.reshape(scaled_attention,\n",
        "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        # Final linear layer\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "class PointWiseFeedForwardNetwork(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, dff):\n",
        "        super(PointWiseFeedForwardNetwork, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(dff, activation='relu') # Inner layer\n",
        "        self.dense2 = tf.keras.layers.Dense(d_model) # Output layer\n",
        "\n",
        "    def call(self, x):\n",
        "        # Applies the network independently to each position\n",
        "        return self.dense2(self.dense1(x))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "A-N-ZyWAwRp-"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 7: Building the Transformer Block\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        # Self-Attention Layer\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        # Point-wise Feed-Forward Network\n",
        "        self.ffn = PointWiseFeedForwardNetwork(d_model, dff)\n",
        "\n",
        "        # Layer Normalization (applied *after* residual connection and dropout in standard impl)\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        # Dropout for regularization\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        # 1. Self-Attention Block\n",
        "        # x = Input embedding + positional encoding\n",
        "        # Attention(Q, K, V) = Attention(x, x, x)\n",
        "        attn_output, _ = self.mha(x, x, x, mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "\n",
        "        # 2. Add & Norm 1 (Residual Connection + Layer Norm)\n",
        "        # x + Dropout(Attention(x)) followed by LayerNorm\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        # 3. Feed Forward Block\n",
        "        ffn_output = self.ffn(out1) # Point-wise FFN applied to output of Add & Norm 1\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "\n",
        "        # 4. Add & Norm 2 (Residual Connection + Layer Norm)\n",
        "        # out1 + Dropout(FFN(out1)) followed by LayerNorm\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        return out2 # Output of the encoder layer\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        # Masked Multi-Head Self-Attention (for decoder input)\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        # Multi-Head Attention (Encoder-Decoder Attention)\n",
        "        # Queries come from decoder input, Keys/Values come from encoder output\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        # Point-wise Feed-Forward Network\n",
        "        self.ffn = PointWiseFeedForwardNetwork(d_model, dff)\n",
        "\n",
        "        # Layer Normalization\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        # 1. Masked Multi-Head Self-Attention (Q=x, K=x, V=x)\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(x + attn1) # Add & Norm 1\n",
        "\n",
        "        # 2. Multi-Head Attention (Encoder-Decoder Attention) (Q=out1, K=enc_output, V=enc_output)\n",
        "        # Here, queries come from the output of the first attention block in the decoder\n",
        "        # and keys/values come from the encoder output.\n",
        "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(out1 + attn2) # Add & Norm 2\n",
        "\n",
        "        # 3. Point-wise Feed-Forward Network\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(out2 + ffn_output) # Add & Norm 3\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2"
      ],
      "metadata": {
        "cellView": "form",
        "id": "lYCl-bXqwVV-"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 8: The Full Transformer Model (Corrected Structure)\n",
        "# This cell outlines the structure. The full implementation is complex.\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "                 maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding,\n",
        "                                                self.d_model)\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
        "                           for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    # Corrected call signature to explicitly name 'training' and 'mask'\n",
        "    def call(self, x, training=False, mask=None): # Added default values for clarity\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        # Add embedding and positional encoding.\n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32)) # Scale embedding\n",
        "        # Add positional encoding (broadcasting to batch)\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training) # Pass training=training here\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            # Pass training=training and mask=mask to each encoder layer\n",
        "            x = self.enc_layers[i](x, training=training, mask=mask)\n",
        "\n",
        "        return x  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "                 maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding,\n",
        "                                                d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
        "                           for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    # Corrected call signature to explicitly name 'training' and masks\n",
        "    def call(self, x, enc_output, training=False, look_ahead_mask=None, padding_mask=None): # Added default values\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        # Add embedding and positional encoding.\n",
        "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32)) # Scale embedding\n",
        "        # Add positional encoding (broadcasting to batch)\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training) # Pass training=training here\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            # Pass training and masks as keyword arguments to each decoder layer\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output,\n",
        "                                                   training=training, # Pass as keyword\n",
        "                                                   look_ahead_mask=look_ahead_mask, # Pass as keyword\n",
        "                                                   padding_mask=padding_mask) # Pass as keyword\n",
        "\n",
        "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "        # x.shape == (batch_size, target_seq_len, d_model)\n",
        "        return x, attention_weights\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
        "                 input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n",
        "                               input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
        "                               target_vocab_size, pe_target, rate)\n",
        "\n",
        "        # Final linear layer to predict vocabulary tokens\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "    # The call signature of Transformer itself is correct (using keywords already based on previous fix)\n",
        "    # This error was *within* this call method, when calling self.encoder/decoder\n",
        "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "        # inp = input sequence (question)\n",
        "        # tar = target sequence (answer - shifted right)\n",
        "\n",
        "        # Encode the input sequence\n",
        "        # !!! FIX: Pass training and mask as keyword arguments to the encoder !!!\n",
        "        enc_output = self.encoder(inp, training=training, mask=enc_padding_mask) # (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "        # Decode the target sequence\n",
        "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "        # !!! FIX: Pass training and masks as keyword arguments to the decoder !!!\n",
        "        dec_output, attention_weights = self.decoder(\n",
        "            tar,\n",
        "            enc_output,\n",
        "            training=training,         # Pass as keyword\n",
        "            look_ahead_mask=look_ahead_mask, # Pass as keyword\n",
        "            padding_mask=dec_padding_mask) # Pass as keyword (using the param name from Decoder.call)\n",
        "\n",
        "        # Final linear layer\n",
        "        final_output = self.final_layer(dec_output) # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "        return final_output, attention_weights\n",
        "\n",
        "# Instantiate the model (using small number of layers for demo)\n",
        "NUM_LAYERS = 2 # Number of encoder and decoder layers\n",
        "# Make sure D_MODEL, NUM_HEADS, FFN_UNITS, VOCAB_SIZE, MAX_LENGTH are defined in Cell 3\n",
        "transformer = Transformer(NUM_LAYERS, D_MODEL, NUM_HEADS, FFN_UNITS,\n",
        "                          VOCAB_SIZE, VOCAB_SIZE, # Using same vocab for input/target\n",
        "                          MAX_LENGTH, MAX_LENGTH) # Max length for positional encoding\n",
        "\n",
        "print(\"Transformer model instantiated.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "uF_m4mLbwYQe",
        "outputId": "07a4a920-c25b-45c2-e881-d873959866b5"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer model instantiated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 9: Optimizer and Loss Function\n",
        "# Custom Learning Rate Schedule (as in original Transformer paper)\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = tf.cast(d_model, dtype=tf.float32)\n",
        "        self.warmup_steps = tf.cast(warmup_steps, dtype=tf.float32)\n",
        "\n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, dtype=tf.float32)\n",
        "        arg1 = tf.math.rsqrt(step) # 1 / sqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5) # step * warmup_steps^-1.5\n",
        "\n",
        "        # Learning rate = d_model^-0.5 * min(arg1, arg2)\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "# Initialize Learning Rate Scheduler and Optimizer\n",
        "learning_rate = CustomSchedule(D_MODEL)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "# Loss Function (Sparse Categorical Crossentropy)\n",
        "# We don't want to calculate loss on padding tokens (label 0)\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    # real: target sequence shifted left (the actual next tokens)\n",
        "    # pred: model's output logits for each token position\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0)) # Create mask where real != 0\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask # Apply mask: loss is 0 for padded positions\n",
        "\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask) # Compute mean loss over non-padded tokens\n",
        "\n",
        "# Metrics (Optional but good)\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "dvT7DnrkwcUG"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 10: Training Step and Checkpointing\n",
        "# Checkpoint manager (for saving/loading model weights)\n",
        "checkpoint_path = \"./checkpoints/transformer_basic\"\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# If a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print('Latest checkpoint restored!!')\n",
        "else:\n",
        "    print('No checkpoint found. Starting fresh.')\n",
        "\n",
        "# The @tf.function decorator compiles the function into a TensorFlow graph\n",
        "# for faster execution.\n",
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "    # tar has shape (batch_size, seq_len)\n",
        "    # We need to split it into decoder input (tar_inp) and target (tar_real)\n",
        "    # Decoder input is the target sequence shifted right by 1 (including <start>)\n",
        "    tar_inp = tar[:, :-1] # Exclude the last token\n",
        "    # Target is the actual next tokens (from index 1 to the end, including <end>)\n",
        "    tar_real = tar[:, 1:] # Exclude the first token (<start>)\n",
        "\n",
        "    # Create masks for this batch\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Forward pass through the model\n",
        "        predictions, _ = transformer(inp, tar_inp,\n",
        "                             training=True, # Pass as keyword\n",
        "                             enc_padding_mask=enc_padding_mask, # Good practice for masks too\n",
        "                             look_ahead_mask=combined_mask,     # Use the correct name from Transformer.call signature\n",
        "                             dec_padding_mask=dec_padding_mask)\n",
        "\n",
        "        # Calculate loss (compare predictions for tar_inp with actual next tokens tar_real)\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    # Calculate gradients\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "\n",
        "    # Apply gradients\n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    # Update metrics\n",
        "    train_loss(loss)\n",
        "    # Accuracy: compare predicted token (argmax) with real token\n",
        "    train_accuracy(tar_real, predictions)\n",
        "\n",
        "# Test the train step with a batch (optional)\n",
        "# for (batch_inp, batch_tar) in dataset.take(1):\n",
        "#     train_step(batch_inp, batch_tar)\n",
        "#     print(\"Initial loss:\", train_loss.result())\n",
        "#     print(\"Initial accuracy:\", train_accuracy.result())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "bAMmssN4wfKW",
        "outputId": "2a51903b-479b-4b9b-daaf-898a4f93e1b7"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No checkpoint found. Starting fresh.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 11: Training Loop (with fixes)\n",
        "\n",
        "print(\"Starting training...\")\n",
        "\n",
        "# Ensure these variables are defined in previous cells and accessible:\n",
        "# - EPOCHS\n",
        "# - dataset (the tf.data.Dataset)\n",
        "# - train_step (the @tf.function for a single training step)\n",
        "# - train_loss (tf.keras.metrics.Mean instance) - WILL BE RE-INITIALIZED\n",
        "# - train_accuracy (tf.keras.metrics.SparseCategoricalAccuracy instance) - WILL BE RE-INITIALIZED\n",
        "# - ckpt_manager (tf.train.CheckpointManager instance)\n",
        "# - time (imported)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    # WORKAROUND FOR AttributeError: 'Metric' object has no attribute 'reset_states'\n",
        "    # Instead of resetting, re-create the metric objects.\n",
        "    # This is less efficient but achieves the goal of starting each epoch with fresh metrics.\n",
        "    # The train_step @tf.function should still be able to reference these if defined\n",
        "    # at the module level above the loop.\n",
        "    print(f\"Epoch {epoch + 1}: Re-initializing metrics as a workaround.\")\n",
        "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "\n",
        "    # Iterate over the dataset batches\n",
        "    for (batch, (inp, tar)) in enumerate(dataset):\n",
        "        # Perform one training step defined in Cell 10\n",
        "        # Make sure the train_step definition in Cell 10 is updated\n",
        "        # to pass 'training=True' and named mask arguments!\n",
        "        train_step(inp, tar)\n",
        "\n",
        "        # Print progress every few batches (adjust frequency based on dataset size)\n",
        "        # For this very small dataset, printing every batch is reasonable.\n",
        "        # You can check the batch number against len(list(dataset)) if needed,\n",
        "        # but iterating over the dataset object directly is usually better.\n",
        "        if batch % 1 == 0: # Print every batch\n",
        "            # Note: Accessing len(list(dataset)) here will consume the dataset iterator.\n",
        "            # For large datasets, avoid this. Just show batch number.\n",
        "            # print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "             print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "\n",
        "    # Save a checkpoint after each epoch (optional, but good practice)\n",
        "    # Using ckpt_manager defined in Cell 10\n",
        "    if (epoch + 1) % 5 == 0: # Save every N epochs (e.g., 5)\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print(f'Saving checkpoint for epoch {epoch + 1} at {ckpt_save_path}')\n",
        "\n",
        "    # Print end-of-epoch summary\n",
        "    print(f'Epoch {epoch + 1} complete - Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "    print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')\n",
        "\n",
        "print(\"Training finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "5Hn-krWXwi1u",
        "outputId": "d11762c0-7ceb-4f0b-dabd-e03c00712c50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 1: Re-initializing metrics as a workaround.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['transformer_2/encoder_2/embedding_4/embeddings', 'transformer_2/decoder_2/embedding_5/embeddings', 'transformer_2/dense_98/kernel', 'transformer_2/dense_98/kernel', 'transformer_2/dense_98/bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 11.7116 Accuracy 0.0000\n",
            "Epoch 1 Batch 1 Loss 11.7119 Accuracy 0.0000\n",
            "Epoch 1 Batch 2 Loss 11.7118 Accuracy 0.0000\n",
            "Epoch 1 Batch 3 Loss 11.7117 Accuracy 0.0000\n",
            "Epoch 1 Batch 4 Loss 11.7113 Accuracy 0.0000\n",
            "Epoch 1 Batch 5 Loss 11.7112 Accuracy 0.0000\n",
            "Epoch 1 Batch 6 Loss 11.7109 Accuracy 0.0000\n",
            "Epoch 1 Batch 7 Loss 11.7105 Accuracy 0.0000\n",
            "Epoch 1 Batch 8 Loss 11.7102 Accuracy 0.0000\n",
            "Epoch 1 Batch 9 Loss 11.7099 Accuracy 0.0000\n",
            "Epoch 1 Batch 10 Loss 11.7098 Accuracy 0.0000\n",
            "Epoch 1 Batch 11 Loss 11.7096 Accuracy 0.0000\n",
            "Epoch 1 Batch 12 Loss 11.7091 Accuracy 0.0000\n",
            "Epoch 1 Batch 13 Loss 11.7089 Accuracy 0.0000\n",
            "Epoch 1 Batch 14 Loss 11.7083 Accuracy 0.0000\n",
            "Epoch 1 Batch 15 Loss 11.7081 Accuracy 0.0000\n",
            "Epoch 1 Batch 16 Loss 11.7078 Accuracy 0.0000\n",
            "Epoch 1 Batch 17 Loss 11.7074 Accuracy 0.0000\n",
            "Epoch 1 Batch 18 Loss 11.7072 Accuracy 0.0000\n",
            "Epoch 1 Batch 19 Loss 11.7068 Accuracy 0.0000\n",
            "Epoch 1 Batch 20 Loss 11.7065 Accuracy 0.0000\n",
            "Epoch 1 Batch 21 Loss 11.7062 Accuracy 0.0000\n",
            "Epoch 1 Batch 22 Loss 11.7057 Accuracy 0.0000\n",
            "Epoch 1 Batch 23 Loss 11.7052 Accuracy 0.0000\n",
            "Epoch 1 Batch 24 Loss 11.7047 Accuracy 0.0000\n",
            "Epoch 1 Batch 25 Loss 11.7041 Accuracy 0.0000\n",
            "Epoch 1 Batch 26 Loss 11.7036 Accuracy 0.0000\n",
            "Epoch 1 Batch 27 Loss 11.7031 Accuracy 0.0000\n",
            "Epoch 1 Batch 28 Loss 11.7027 Accuracy 0.0000\n",
            "Epoch 1 Batch 29 Loss 11.7022 Accuracy 0.0000\n",
            "Epoch 1 Batch 30 Loss 11.7017 Accuracy 0.0000\n",
            "Epoch 1 Batch 31 Loss 11.7011 Accuracy 0.0000\n",
            "Epoch 1 Batch 32 Loss 11.7006 Accuracy 0.0000\n",
            "Epoch 1 Batch 33 Loss 11.7001 Accuracy 0.0000\n",
            "Epoch 1 Batch 34 Loss 11.6994 Accuracy 0.0000\n",
            "Epoch 1 Batch 35 Loss 11.6987 Accuracy 0.0000\n",
            "Epoch 1 Batch 36 Loss 11.6981 Accuracy 0.0000\n",
            "Epoch 1 Batch 37 Loss 11.6974 Accuracy 0.0000\n",
            "Epoch 1 Batch 38 Loss 11.6967 Accuracy 0.0000\n",
            "Epoch 1 Batch 39 Loss 11.6960 Accuracy 0.0000\n",
            "Epoch 1 Batch 40 Loss 11.6952 Accuracy 0.0000\n",
            "Epoch 1 Batch 41 Loss 11.6944 Accuracy 0.0000\n",
            "Epoch 1 Batch 42 Loss 11.6935 Accuracy 0.0000\n",
            "Epoch 1 Batch 43 Loss 11.6927 Accuracy 0.0000\n",
            "Epoch 1 Batch 44 Loss 11.6919 Accuracy 0.0000\n",
            "Epoch 1 Batch 45 Loss 11.6910 Accuracy 0.0000\n",
            "Epoch 1 Batch 46 Loss 11.6902 Accuracy 0.0000\n",
            "Epoch 1 Batch 47 Loss 11.6893 Accuracy 0.0000\n",
            "Epoch 1 Batch 48 Loss 11.6884 Accuracy 0.0001\n",
            "Epoch 1 Batch 49 Loss 11.6874 Accuracy 0.0001\n",
            "Epoch 1 Batch 50 Loss 11.6864 Accuracy 0.0002\n",
            "Epoch 1 Batch 51 Loss 11.6855 Accuracy 0.0002\n",
            "Epoch 1 Batch 52 Loss 11.6846 Accuracy 0.0004\n",
            "Epoch 1 Batch 53 Loss 11.6835 Accuracy 0.0006\n",
            "Epoch 1 Batch 54 Loss 11.6825 Accuracy 0.0009\n",
            "Epoch 1 Batch 55 Loss 11.6814 Accuracy 0.0011\n",
            "Epoch 1 Batch 56 Loss 11.6803 Accuracy 0.0015\n",
            "Epoch 1 Batch 57 Loss 11.6792 Accuracy 0.0018\n",
            "Epoch 1 Batch 58 Loss 11.6780 Accuracy 0.0021\n",
            "Epoch 1 Batch 59 Loss 11.6768 Accuracy 0.0025\n",
            "Epoch 1 Batch 60 Loss 11.6757 Accuracy 0.0029\n",
            "Epoch 1 Batch 61 Loss 11.6745 Accuracy 0.0033\n",
            "Epoch 1 Batch 62 Loss 11.6734 Accuracy 0.0037\n",
            "Epoch 1 Batch 63 Loss 11.6720 Accuracy 0.0041\n",
            "Epoch 1 Batch 64 Loss 11.6707 Accuracy 0.0045\n",
            "Epoch 1 Batch 65 Loss 11.6695 Accuracy 0.0049\n",
            "Epoch 1 Batch 66 Loss 11.6682 Accuracy 0.0053\n",
            "Epoch 1 Batch 67 Loss 11.6668 Accuracy 0.0057\n",
            "Epoch 1 Batch 68 Loss 11.6656 Accuracy 0.0060\n",
            "Epoch 1 Batch 69 Loss 11.6642 Accuracy 0.0064\n",
            "Epoch 1 Batch 70 Loss 11.6628 Accuracy 0.0067\n",
            "Epoch 1 Batch 71 Loss 11.6615 Accuracy 0.0071\n",
            "Epoch 1 Batch 72 Loss 11.6600 Accuracy 0.0074\n",
            "Epoch 1 Batch 73 Loss 11.6586 Accuracy 0.0077\n",
            "Epoch 1 Batch 74 Loss 11.6571 Accuracy 0.0080\n",
            "Epoch 1 Batch 75 Loss 11.6556 Accuracy 0.0084\n",
            "Epoch 1 Batch 76 Loss 11.6541 Accuracy 0.0086\n",
            "Epoch 1 Batch 77 Loss 11.6526 Accuracy 0.0089\n",
            "Epoch 1 Batch 78 Loss 11.6510 Accuracy 0.0092\n",
            "Epoch 1 Batch 79 Loss 11.6495 Accuracy 0.0095\n",
            "Epoch 1 Batch 80 Loss 11.6478 Accuracy 0.0098\n",
            "Epoch 1 Batch 81 Loss 11.6461 Accuracy 0.0100\n",
            "Epoch 1 Batch 82 Loss 11.6444 Accuracy 0.0103\n",
            "Epoch 1 Batch 83 Loss 11.6427 Accuracy 0.0105\n",
            "Epoch 1 Batch 84 Loss 11.6408 Accuracy 0.0108\n",
            "Epoch 1 Batch 85 Loss 11.6392 Accuracy 0.0110\n",
            "Epoch 1 Batch 86 Loss 11.6375 Accuracy 0.0112\n",
            "Epoch 1 Batch 87 Loss 11.6357 Accuracy 0.0115\n",
            "Epoch 1 Batch 88 Loss 11.6339 Accuracy 0.0117\n",
            "Epoch 1 Batch 89 Loss 11.6321 Accuracy 0.0119\n",
            "Epoch 1 Batch 90 Loss 11.6302 Accuracy 0.0121\n",
            "Epoch 1 Batch 91 Loss 11.6284 Accuracy 0.0123\n",
            "Epoch 1 Batch 92 Loss 11.6265 Accuracy 0.0125\n",
            "Epoch 1 Batch 93 Loss 11.6246 Accuracy 0.0127\n",
            "Epoch 1 Batch 94 Loss 11.6227 Accuracy 0.0129\n",
            "Epoch 1 Batch 95 Loss 11.6207 Accuracy 0.0131\n",
            "Epoch 1 Batch 96 Loss 11.6188 Accuracy 0.0133\n",
            "Epoch 1 Batch 97 Loss 11.6167 Accuracy 0.0135\n",
            "Epoch 1 Batch 98 Loss 11.6147 Accuracy 0.0137\n",
            "Epoch 1 Batch 99 Loss 11.6128 Accuracy 0.0138\n",
            "Epoch 1 Batch 100 Loss 11.6107 Accuracy 0.0140\n",
            "Epoch 1 Batch 101 Loss 11.6086 Accuracy 0.0142\n",
            "Epoch 1 Batch 102 Loss 11.6064 Accuracy 0.0144\n",
            "Epoch 1 Batch 103 Loss 11.6043 Accuracy 0.0145\n",
            "Epoch 1 Batch 104 Loss 11.6022 Accuracy 0.0147\n",
            "Epoch 1 Batch 105 Loss 11.6001 Accuracy 0.0148\n",
            "Epoch 1 Batch 106 Loss 11.5977 Accuracy 0.0150\n",
            "Epoch 1 Batch 107 Loss 11.5955 Accuracy 0.0151\n",
            "Epoch 1 Batch 108 Loss 11.5932 Accuracy 0.0153\n",
            "Epoch 1 Batch 109 Loss 11.5908 Accuracy 0.0154\n",
            "Epoch 1 Batch 110 Loss 11.5885 Accuracy 0.0156\n",
            "Epoch 1 Batch 111 Loss 11.5863 Accuracy 0.0157\n",
            "Epoch 1 Batch 112 Loss 11.5840 Accuracy 0.0158\n",
            "Epoch 1 Batch 113 Loss 11.5815 Accuracy 0.0160\n",
            "Epoch 1 Batch 114 Loss 11.5791 Accuracy 0.0161\n",
            "Epoch 1 Batch 115 Loss 11.5766 Accuracy 0.0162\n",
            "Epoch 1 Batch 116 Loss 11.5742 Accuracy 0.0164\n",
            "Epoch 1 Batch 117 Loss 11.5718 Accuracy 0.0165\n",
            "Epoch 1 Batch 118 Loss 11.5694 Accuracy 0.0166\n",
            "Epoch 1 Batch 119 Loss 11.5666 Accuracy 0.0167\n",
            "Epoch 1 Batch 120 Loss 11.5642 Accuracy 0.0169\n",
            "Epoch 1 Batch 121 Loss 11.5617 Accuracy 0.0170\n",
            "Epoch 1 Batch 122 Loss 11.5592 Accuracy 0.0171\n",
            "Epoch 1 Batch 123 Loss 11.5566 Accuracy 0.0172\n",
            "Epoch 1 Batch 124 Loss 11.5537 Accuracy 0.0173\n",
            "Epoch 1 Batch 125 Loss 11.5510 Accuracy 0.0174\n",
            "Epoch 1 Batch 126 Loss 11.5483 Accuracy 0.0175\n",
            "Epoch 1 Batch 127 Loss 11.5456 Accuracy 0.0177\n",
            "Epoch 1 Batch 128 Loss 11.5429 Accuracy 0.0178\n",
            "Epoch 1 Batch 129 Loss 11.5402 Accuracy 0.0179\n",
            "Epoch 1 Batch 130 Loss 11.5375 Accuracy 0.0180\n",
            "Epoch 1 Batch 131 Loss 11.5347 Accuracy 0.0181\n",
            "Epoch 1 Batch 132 Loss 11.5320 Accuracy 0.0182\n",
            "Epoch 1 Batch 133 Loss 11.5292 Accuracy 0.0183\n",
            "Epoch 1 Batch 134 Loss 11.5263 Accuracy 0.0184\n",
            "Epoch 1 Batch 135 Loss 11.5235 Accuracy 0.0185\n",
            "Epoch 1 Batch 136 Loss 11.5207 Accuracy 0.0185\n",
            "Epoch 1 Batch 137 Loss 11.5178 Accuracy 0.0186\n",
            "Epoch 1 Batch 138 Loss 11.5151 Accuracy 0.0187\n",
            "Epoch 1 Batch 139 Loss 11.5121 Accuracy 0.0188\n",
            "Epoch 1 Batch 140 Loss 11.5092 Accuracy 0.0189\n",
            "Epoch 1 Batch 141 Loss 11.5062 Accuracy 0.0190\n",
            "Epoch 1 Batch 142 Loss 11.5031 Accuracy 0.0191\n",
            "Epoch 1 Batch 143 Loss 11.5000 Accuracy 0.0192\n",
            "Epoch 1 Batch 144 Loss 11.4969 Accuracy 0.0192\n",
            "Epoch 1 Batch 145 Loss 11.4939 Accuracy 0.0193\n",
            "Epoch 1 Batch 146 Loss 11.4907 Accuracy 0.0194\n",
            "Epoch 1 Batch 147 Loss 11.4875 Accuracy 0.0195\n",
            "Epoch 1 Batch 148 Loss 11.4845 Accuracy 0.0196\n",
            "Epoch 1 Batch 149 Loss 11.4813 Accuracy 0.0196\n",
            "Epoch 1 Batch 150 Loss 11.4782 Accuracy 0.0197\n",
            "Epoch 1 Batch 151 Loss 11.4750 Accuracy 0.0198\n",
            "Epoch 1 Batch 152 Loss 11.4716 Accuracy 0.0199\n",
            "Epoch 1 Batch 153 Loss 11.4684 Accuracy 0.0199\n",
            "Epoch 1 Batch 154 Loss 11.4650 Accuracy 0.0200\n",
            "Epoch 1 Batch 155 Loss 11.4619 Accuracy 0.0201\n",
            "Epoch 1 Batch 156 Loss 11.4586 Accuracy 0.0202\n",
            "Epoch 1 Batch 157 Loss 11.4553 Accuracy 0.0202\n",
            "Epoch 1 Batch 158 Loss 11.4520 Accuracy 0.0203\n",
            "Epoch 1 Batch 159 Loss 11.4486 Accuracy 0.0204\n",
            "Epoch 1 Batch 160 Loss 11.4453 Accuracy 0.0204\n",
            "Epoch 1 Batch 161 Loss 11.4419 Accuracy 0.0205\n",
            "Epoch 1 Batch 162 Loss 11.4385 Accuracy 0.0206\n",
            "Epoch 1 Batch 163 Loss 11.4352 Accuracy 0.0206\n",
            "Epoch 1 Batch 164 Loss 11.4318 Accuracy 0.0207\n",
            "Epoch 1 Batch 165 Loss 11.4284 Accuracy 0.0208\n",
            "Epoch 1 Batch 166 Loss 11.4250 Accuracy 0.0208\n",
            "Epoch 1 Batch 167 Loss 11.4214 Accuracy 0.0209\n",
            "Epoch 1 Batch 168 Loss 11.4180 Accuracy 0.0210\n",
            "Epoch 1 Batch 169 Loss 11.4144 Accuracy 0.0210\n",
            "Epoch 1 Batch 170 Loss 11.4107 Accuracy 0.0211\n",
            "Epoch 1 Batch 171 Loss 11.4071 Accuracy 0.0211\n",
            "Epoch 1 Batch 172 Loss 11.4034 Accuracy 0.0212\n",
            "Epoch 1 Batch 173 Loss 11.3998 Accuracy 0.0212\n",
            "Epoch 1 Batch 174 Loss 11.3960 Accuracy 0.0213\n",
            "Epoch 1 Batch 175 Loss 11.3924 Accuracy 0.0214\n",
            "Epoch 1 Batch 176 Loss 11.3886 Accuracy 0.0214\n",
            "Epoch 1 Batch 177 Loss 11.3850 Accuracy 0.0215\n",
            "Epoch 1 Batch 178 Loss 11.3815 Accuracy 0.0215\n",
            "Epoch 1 Batch 179 Loss 11.3779 Accuracy 0.0216\n",
            "Epoch 1 Batch 180 Loss 11.3743 Accuracy 0.0216\n",
            "Epoch 1 Batch 181 Loss 11.3704 Accuracy 0.0217\n",
            "Epoch 1 Batch 182 Loss 11.3665 Accuracy 0.0217\n",
            "Epoch 1 Batch 183 Loss 11.3625 Accuracy 0.0218\n",
            "Epoch 1 Batch 184 Loss 11.3585 Accuracy 0.0218\n",
            "Epoch 1 Batch 185 Loss 11.3546 Accuracy 0.0219\n",
            "Epoch 1 Batch 186 Loss 11.3505 Accuracy 0.0219\n",
            "Epoch 1 Batch 187 Loss 11.3466 Accuracy 0.0220\n",
            "Epoch 1 Batch 188 Loss 11.3428 Accuracy 0.0220\n",
            "Epoch 1 Batch 189 Loss 11.3388 Accuracy 0.0221\n",
            "Epoch 1 Batch 190 Loss 11.3347 Accuracy 0.0221\n",
            "Epoch 1 Batch 191 Loss 11.3307 Accuracy 0.0222\n",
            "Epoch 1 Batch 192 Loss 11.3268 Accuracy 0.0222\n",
            "Epoch 1 Batch 193 Loss 11.3231 Accuracy 0.0223\n",
            "Epoch 1 Batch 194 Loss 11.3190 Accuracy 0.0223\n",
            "Epoch 1 Batch 195 Loss 11.3150 Accuracy 0.0224\n",
            "Epoch 1 Batch 196 Loss 11.3109 Accuracy 0.0224\n",
            "Epoch 1 Batch 197 Loss 11.3069 Accuracy 0.0225\n",
            "Epoch 1 Batch 198 Loss 11.3028 Accuracy 0.0225\n",
            "Epoch 1 Batch 199 Loss 11.2987 Accuracy 0.0225\n",
            "Epoch 1 Batch 200 Loss 11.2946 Accuracy 0.0226\n",
            "Epoch 1 Batch 201 Loss 11.2904 Accuracy 0.0226\n",
            "Epoch 1 Batch 202 Loss 11.2862 Accuracy 0.0227\n",
            "Epoch 1 Batch 203 Loss 11.2821 Accuracy 0.0227\n",
            "Epoch 1 Batch 204 Loss 11.2780 Accuracy 0.0228\n",
            "Epoch 1 Batch 205 Loss 11.2736 Accuracy 0.0228\n",
            "Epoch 1 Batch 206 Loss 11.2693 Accuracy 0.0228\n",
            "Epoch 1 Batch 207 Loss 11.2651 Accuracy 0.0229\n",
            "Epoch 1 Batch 208 Loss 11.2610 Accuracy 0.0229\n",
            "Epoch 1 Batch 209 Loss 11.2568 Accuracy 0.0230\n",
            "Epoch 1 Batch 210 Loss 11.2524 Accuracy 0.0230\n",
            "Epoch 1 Batch 211 Loss 11.2483 Accuracy 0.0230\n",
            "Epoch 1 Batch 212 Loss 11.2439 Accuracy 0.0231\n",
            "Epoch 1 Batch 213 Loss 11.2395 Accuracy 0.0231\n",
            "Epoch 1 Batch 214 Loss 11.2350 Accuracy 0.0232\n",
            "Epoch 1 Batch 215 Loss 11.2306 Accuracy 0.0232\n",
            "Epoch 1 Batch 216 Loss 11.2260 Accuracy 0.0232\n",
            "Epoch 1 Batch 217 Loss 11.2216 Accuracy 0.0233\n",
            "Epoch 1 Batch 218 Loss 11.2172 Accuracy 0.0233\n",
            "Epoch 1 Batch 219 Loss 11.2127 Accuracy 0.0233\n",
            "Epoch 1 Batch 220 Loss 11.2084 Accuracy 0.0234\n",
            "Epoch 1 Batch 221 Loss 11.2039 Accuracy 0.0234\n",
            "Epoch 1 Batch 222 Loss 11.1994 Accuracy 0.0234\n",
            "Epoch 1 Batch 223 Loss 11.1949 Accuracy 0.0235\n",
            "Epoch 1 Batch 224 Loss 11.1906 Accuracy 0.0235\n",
            "Epoch 1 Batch 225 Loss 11.1859 Accuracy 0.0235\n",
            "Epoch 1 Batch 226 Loss 11.1813 Accuracy 0.0236\n",
            "Epoch 1 Batch 227 Loss 11.1768 Accuracy 0.0236\n",
            "Epoch 1 Batch 228 Loss 11.1721 Accuracy 0.0237\n",
            "Epoch 1 Batch 229 Loss 11.1673 Accuracy 0.0237\n",
            "Epoch 1 Batch 230 Loss 11.1627 Accuracy 0.0237\n",
            "Epoch 1 Batch 231 Loss 11.1580 Accuracy 0.0237\n",
            "Epoch 1 Batch 232 Loss 11.1533 Accuracy 0.0238\n",
            "Epoch 1 Batch 233 Loss 11.1487 Accuracy 0.0238\n",
            "Epoch 1 Batch 234 Loss 11.1439 Accuracy 0.0238\n",
            "Epoch 1 Batch 235 Loss 11.1391 Accuracy 0.0239\n",
            "Epoch 1 Batch 236 Loss 11.1345 Accuracy 0.0239\n",
            "Epoch 1 Batch 237 Loss 11.1297 Accuracy 0.0239\n",
            "Epoch 1 Batch 238 Loss 11.1251 Accuracy 0.0240\n",
            "Epoch 1 Batch 239 Loss 11.1202 Accuracy 0.0240\n",
            "Epoch 1 Batch 240 Loss 11.1155 Accuracy 0.0240\n",
            "Epoch 1 Batch 241 Loss 11.1108 Accuracy 0.0241\n",
            "Epoch 1 Batch 242 Loss 11.1059 Accuracy 0.0241\n",
            "Epoch 1 Batch 243 Loss 11.1010 Accuracy 0.0241\n",
            "Epoch 1 Batch 244 Loss 11.0960 Accuracy 0.0241\n",
            "Epoch 1 Batch 245 Loss 11.0909 Accuracy 0.0242\n",
            "Epoch 1 Batch 246 Loss 11.0859 Accuracy 0.0242\n",
            "Epoch 1 Batch 247 Loss 11.0807 Accuracy 0.0242\n",
            "Epoch 1 Batch 248 Loss 11.0756 Accuracy 0.0243\n",
            "Epoch 1 Batch 249 Loss 11.0708 Accuracy 0.0243\n",
            "Epoch 1 Batch 250 Loss 11.0659 Accuracy 0.0243\n",
            "Epoch 1 Batch 251 Loss 11.0610 Accuracy 0.0243\n",
            "Epoch 1 Batch 252 Loss 11.0560 Accuracy 0.0244\n",
            "Epoch 1 Batch 253 Loss 11.0510 Accuracy 0.0244\n",
            "Epoch 1 Batch 254 Loss 11.0461 Accuracy 0.0244\n",
            "Epoch 1 Batch 255 Loss 11.0411 Accuracy 0.0245\n",
            "Epoch 1 Batch 256 Loss 11.0359 Accuracy 0.0245\n",
            "Epoch 1 Batch 257 Loss 11.0309 Accuracy 0.0245\n",
            "Epoch 1 Batch 258 Loss 11.0257 Accuracy 0.0245\n",
            "Epoch 1 Batch 259 Loss 11.0207 Accuracy 0.0246\n",
            "Epoch 1 Batch 260 Loss 11.0156 Accuracy 0.0246\n",
            "Epoch 1 Batch 261 Loss 11.0105 Accuracy 0.0246\n",
            "Epoch 1 Batch 262 Loss 11.0056 Accuracy 0.0246\n",
            "Epoch 1 Batch 263 Loss 11.0004 Accuracy 0.0247\n",
            "Epoch 1 Batch 264 Loss 10.9953 Accuracy 0.0247\n",
            "Epoch 1 Batch 265 Loss 10.9903 Accuracy 0.0247\n",
            "Epoch 1 Batch 266 Loss 10.9853 Accuracy 0.0247\n",
            "Epoch 1 Batch 267 Loss 10.9797 Accuracy 0.0248\n",
            "Epoch 1 Batch 268 Loss 10.9744 Accuracy 0.0248\n",
            "Epoch 1 Batch 269 Loss 10.9692 Accuracy 0.0248\n",
            "Epoch 1 Batch 270 Loss 10.9638 Accuracy 0.0248\n",
            "Epoch 1 Batch 271 Loss 10.9582 Accuracy 0.0249\n",
            "Epoch 1 Batch 272 Loss 10.9533 Accuracy 0.0249\n",
            "Epoch 1 Batch 273 Loss 10.9483 Accuracy 0.0249\n",
            "Epoch 1 Batch 274 Loss 10.9430 Accuracy 0.0249\n",
            "Epoch 1 Batch 275 Loss 10.9375 Accuracy 0.0249\n",
            "Epoch 1 Batch 276 Loss 10.9321 Accuracy 0.0250\n",
            "Epoch 1 Batch 277 Loss 10.9265 Accuracy 0.0250\n",
            "Epoch 1 Batch 278 Loss 10.9212 Accuracy 0.0250\n",
            "Epoch 1 Batch 279 Loss 10.9159 Accuracy 0.0250\n",
            "Epoch 1 Batch 280 Loss 10.9105 Accuracy 0.0251\n",
            "Epoch 1 Batch 281 Loss 10.9051 Accuracy 0.0251\n",
            "Epoch 1 Batch 282 Loss 10.8996 Accuracy 0.0251\n",
            "Epoch 1 Batch 283 Loss 10.8942 Accuracy 0.0251\n",
            "Epoch 1 Batch 284 Loss 10.8885 Accuracy 0.0251\n",
            "Epoch 1 Batch 285 Loss 10.8831 Accuracy 0.0252\n",
            "Epoch 1 Batch 286 Loss 10.8776 Accuracy 0.0252\n",
            "Epoch 1 Batch 287 Loss 10.8721 Accuracy 0.0252\n",
            "Epoch 1 Batch 288 Loss 10.8667 Accuracy 0.0252\n",
            "Epoch 1 Batch 289 Loss 10.8610 Accuracy 0.0252\n",
            "Epoch 1 Batch 290 Loss 10.8554 Accuracy 0.0253\n",
            "Epoch 1 Batch 291 Loss 10.8495 Accuracy 0.0253\n",
            "Epoch 1 Batch 292 Loss 10.8438 Accuracy 0.0253\n",
            "Epoch 1 Batch 293 Loss 10.8385 Accuracy 0.0253\n",
            "Epoch 1 Batch 294 Loss 10.8329 Accuracy 0.0254\n",
            "Epoch 1 Batch 295 Loss 10.8274 Accuracy 0.0254\n",
            "Epoch 1 Batch 296 Loss 10.8220 Accuracy 0.0254\n",
            "Epoch 1 Batch 297 Loss 10.8161 Accuracy 0.0254\n",
            "Epoch 1 Batch 298 Loss 10.8104 Accuracy 0.0254\n",
            "Epoch 1 Batch 299 Loss 10.8048 Accuracy 0.0254\n",
            "Epoch 1 Batch 300 Loss 10.7989 Accuracy 0.0255\n",
            "Epoch 1 Batch 301 Loss 10.7932 Accuracy 0.0255\n",
            "Epoch 1 Batch 302 Loss 10.7876 Accuracy 0.0255\n",
            "Epoch 1 Batch 303 Loss 10.7821 Accuracy 0.0255\n",
            "Epoch 1 Batch 304 Loss 10.7764 Accuracy 0.0255\n",
            "Epoch 1 Batch 305 Loss 10.7709 Accuracy 0.0256\n",
            "Epoch 1 Batch 306 Loss 10.7652 Accuracy 0.0256\n",
            "Epoch 1 Batch 307 Loss 10.7595 Accuracy 0.0256\n",
            "Epoch 1 Batch 308 Loss 10.7539 Accuracy 0.0256\n",
            "Epoch 1 Batch 309 Loss 10.7484 Accuracy 0.0256\n",
            "Epoch 1 Batch 310 Loss 10.7426 Accuracy 0.0257\n",
            "Epoch 1 Batch 311 Loss 10.7367 Accuracy 0.0257\n",
            "Epoch 1 Batch 312 Loss 10.7309 Accuracy 0.0257\n",
            "Epoch 1 Batch 313 Loss 10.7250 Accuracy 0.0257\n",
            "Epoch 1 Batch 314 Loss 10.7193 Accuracy 0.0257\n",
            "Epoch 1 Batch 315 Loss 10.7136 Accuracy 0.0257\n",
            "Epoch 1 Batch 316 Loss 10.7080 Accuracy 0.0258\n",
            "Epoch 1 Batch 317 Loss 10.7021 Accuracy 0.0258\n",
            "Epoch 1 Batch 318 Loss 10.6961 Accuracy 0.0258\n",
            "Epoch 1 Batch 319 Loss 10.6902 Accuracy 0.0258\n",
            "Epoch 1 Batch 320 Loss 10.6843 Accuracy 0.0258\n",
            "Epoch 1 Batch 321 Loss 10.6782 Accuracy 0.0258\n",
            "Epoch 1 Batch 322 Loss 10.6722 Accuracy 0.0259\n",
            "Epoch 1 Batch 323 Loss 10.6663 Accuracy 0.0259\n",
            "Epoch 1 Batch 324 Loss 10.6604 Accuracy 0.0259\n",
            "Epoch 1 Batch 325 Loss 10.6543 Accuracy 0.0259\n",
            "Epoch 1 Batch 326 Loss 10.6482 Accuracy 0.0259\n",
            "Epoch 1 Batch 327 Loss 10.6424 Accuracy 0.0259\n",
            "Epoch 1 Batch 328 Loss 10.6363 Accuracy 0.0260\n",
            "Epoch 1 Batch 329 Loss 10.6304 Accuracy 0.0260\n",
            "Epoch 1 Batch 330 Loss 10.6246 Accuracy 0.0260\n",
            "Epoch 1 Batch 331 Loss 10.6183 Accuracy 0.0260\n",
            "Epoch 1 Batch 332 Loss 10.6120 Accuracy 0.0260\n",
            "Epoch 1 Batch 333 Loss 10.6060 Accuracy 0.0260\n",
            "Epoch 1 Batch 334 Loss 10.6000 Accuracy 0.0261\n",
            "Epoch 1 Batch 335 Loss 10.5936 Accuracy 0.0261\n",
            "Epoch 1 Batch 336 Loss 10.5876 Accuracy 0.0261\n",
            "Epoch 1 Batch 337 Loss 10.5817 Accuracy 0.0261\n",
            "Epoch 1 Batch 338 Loss 10.5756 Accuracy 0.0261\n",
            "Epoch 1 Batch 339 Loss 10.5695 Accuracy 0.0261\n",
            "Epoch 1 Batch 340 Loss 10.5631 Accuracy 0.0261\n",
            "Epoch 1 Batch 341 Loss 10.5572 Accuracy 0.0262\n",
            "Epoch 1 Batch 342 Loss 10.5512 Accuracy 0.0262\n",
            "Epoch 1 Batch 343 Loss 10.5456 Accuracy 0.0262\n",
            "Epoch 1 Batch 344 Loss 10.5394 Accuracy 0.0262\n",
            "Epoch 1 Batch 345 Loss 10.5337 Accuracy 0.0262\n",
            "Epoch 1 Batch 346 Loss 10.5276 Accuracy 0.0262\n",
            "Epoch 1 Batch 347 Loss 10.5216 Accuracy 0.0262\n",
            "Epoch 1 Batch 348 Loss 10.5160 Accuracy 0.0263\n",
            "Epoch 1 Batch 349 Loss 10.5099 Accuracy 0.0263\n",
            "Epoch 1 Batch 350 Loss 10.5040 Accuracy 0.0263\n",
            "Epoch 1 Batch 351 Loss 10.4979 Accuracy 0.0263\n",
            "Epoch 1 Batch 352 Loss 10.4918 Accuracy 0.0263\n",
            "Epoch 1 Batch 353 Loss 10.4857 Accuracy 0.0263\n",
            "Epoch 1 Batch 354 Loss 10.4795 Accuracy 0.0263\n",
            "Epoch 1 Batch 355 Loss 10.4731 Accuracy 0.0264\n",
            "Epoch 1 Batch 356 Loss 10.4670 Accuracy 0.0264\n",
            "Epoch 1 Batch 357 Loss 10.4606 Accuracy 0.0264\n",
            "Epoch 1 Batch 358 Loss 10.4543 Accuracy 0.0264\n",
            "Epoch 1 Batch 359 Loss 10.4484 Accuracy 0.0264\n",
            "Epoch 1 Batch 360 Loss 10.4418 Accuracy 0.0264\n",
            "Epoch 1 Batch 361 Loss 10.4357 Accuracy 0.0264\n",
            "Epoch 1 Batch 362 Loss 10.4293 Accuracy 0.0265\n",
            "Epoch 1 Batch 363 Loss 10.4234 Accuracy 0.0265\n",
            "Epoch 1 Batch 364 Loss 10.4176 Accuracy 0.0265\n",
            "Epoch 1 Batch 365 Loss 10.4110 Accuracy 0.0265\n",
            "Epoch 1 Batch 366 Loss 10.4049 Accuracy 0.0265\n",
            "Epoch 1 Batch 367 Loss 10.3989 Accuracy 0.0265\n",
            "Epoch 1 Batch 368 Loss 10.3927 Accuracy 0.0265\n",
            "Epoch 1 Batch 369 Loss 10.3864 Accuracy 0.0265\n",
            "Epoch 1 Batch 370 Loss 10.3802 Accuracy 0.0266\n",
            "Epoch 1 Batch 371 Loss 10.3740 Accuracy 0.0266\n",
            "Epoch 1 Batch 372 Loss 10.3678 Accuracy 0.0266\n",
            "Epoch 1 Batch 373 Loss 10.3618 Accuracy 0.0266\n",
            "Epoch 1 Batch 374 Loss 10.3559 Accuracy 0.0266\n",
            "Epoch 1 Batch 375 Loss 10.3498 Accuracy 0.0266\n",
            "Epoch 1 Batch 376 Loss 10.3436 Accuracy 0.0266\n",
            "Epoch 1 Batch 377 Loss 10.3375 Accuracy 0.0266\n",
            "Epoch 1 Batch 378 Loss 10.3314 Accuracy 0.0267\n",
            "Epoch 1 Batch 379 Loss 10.3256 Accuracy 0.0267\n",
            "Epoch 1 Batch 380 Loss 10.3189 Accuracy 0.0267\n",
            "Epoch 1 Batch 381 Loss 10.3129 Accuracy 0.0267\n",
            "Epoch 1 Batch 382 Loss 10.3068 Accuracy 0.0267\n",
            "Epoch 1 Batch 383 Loss 10.3008 Accuracy 0.0267\n",
            "Epoch 1 Batch 384 Loss 10.2942 Accuracy 0.0267\n",
            "Epoch 1 Batch 385 Loss 10.2880 Accuracy 0.0267\n",
            "Epoch 1 Batch 386 Loss 10.2815 Accuracy 0.0268\n",
            "Epoch 1 Batch 387 Loss 10.2753 Accuracy 0.0268\n",
            "Epoch 1 Batch 388 Loss 10.2690 Accuracy 0.0268\n",
            "Epoch 1 Batch 389 Loss 10.2628 Accuracy 0.0268\n",
            "Epoch 1 Batch 390 Loss 10.2564 Accuracy 0.0268\n",
            "Epoch 1 Batch 391 Loss 10.2501 Accuracy 0.0268\n",
            "Epoch 1 Batch 392 Loss 10.2436 Accuracy 0.0268\n",
            "Epoch 1 Batch 393 Loss 10.2375 Accuracy 0.0268\n",
            "Epoch 1 Batch 394 Loss 10.2309 Accuracy 0.0268\n",
            "Epoch 1 Batch 395 Loss 10.2248 Accuracy 0.0269\n",
            "Epoch 1 Batch 396 Loss 10.2188 Accuracy 0.0269\n",
            "Epoch 1 Batch 397 Loss 10.2128 Accuracy 0.0269\n",
            "Epoch 1 Batch 398 Loss 10.2064 Accuracy 0.0269\n",
            "Epoch 1 Batch 399 Loss 10.2005 Accuracy 0.0269\n",
            "Epoch 1 Batch 400 Loss 10.1944 Accuracy 0.0269\n",
            "Epoch 1 Batch 401 Loss 10.1883 Accuracy 0.0269\n",
            "Epoch 1 Batch 402 Loss 10.1818 Accuracy 0.0269\n",
            "Epoch 1 Batch 403 Loss 10.1752 Accuracy 0.0269\n",
            "Epoch 1 Batch 404 Loss 10.1693 Accuracy 0.0270\n",
            "Epoch 1 Batch 405 Loss 10.1630 Accuracy 0.0270\n",
            "Epoch 1 Batch 406 Loss 10.1568 Accuracy 0.0270\n",
            "Epoch 1 Batch 407 Loss 10.1512 Accuracy 0.0270\n",
            "Epoch 1 Batch 408 Loss 10.1448 Accuracy 0.0270\n",
            "Epoch 1 Batch 409 Loss 10.1392 Accuracy 0.0270\n",
            "Epoch 1 Batch 410 Loss 10.1334 Accuracy 0.0270\n",
            "Epoch 1 Batch 411 Loss 10.1273 Accuracy 0.0270\n",
            "Epoch 1 Batch 412 Loss 10.1211 Accuracy 0.0270\n",
            "Epoch 1 Batch 413 Loss 10.1147 Accuracy 0.0270\n",
            "Epoch 1 Batch 414 Loss 10.1085 Accuracy 0.0271\n",
            "Epoch 1 Batch 415 Loss 10.1025 Accuracy 0.0271\n",
            "Epoch 1 Batch 416 Loss 10.0965 Accuracy 0.0271\n",
            "Epoch 1 Batch 417 Loss 10.0900 Accuracy 0.0271\n",
            "Epoch 1 Batch 418 Loss 10.0837 Accuracy 0.0271\n",
            "Epoch 1 Batch 419 Loss 10.0773 Accuracy 0.0271\n",
            "Epoch 1 Batch 420 Loss 10.0712 Accuracy 0.0271\n",
            "Epoch 1 Batch 421 Loss 10.0651 Accuracy 0.0271\n",
            "Epoch 1 Batch 422 Loss 10.0589 Accuracy 0.0271\n",
            "Epoch 1 Batch 423 Loss 10.0525 Accuracy 0.0271\n",
            "Epoch 1 Batch 424 Loss 10.0465 Accuracy 0.0272\n",
            "Epoch 1 Batch 425 Loss 10.0403 Accuracy 0.0272\n",
            "Epoch 1 Batch 426 Loss 10.0341 Accuracy 0.0272\n",
            "Epoch 1 Batch 427 Loss 10.0281 Accuracy 0.0272\n",
            "Epoch 1 Batch 428 Loss 10.0225 Accuracy 0.0272\n",
            "Epoch 1 Batch 429 Loss 10.0168 Accuracy 0.0272\n",
            "Epoch 1 Batch 430 Loss 10.0110 Accuracy 0.0272\n",
            "Epoch 1 Batch 431 Loss 10.0051 Accuracy 0.0272\n",
            "Epoch 1 Batch 432 Loss 9.9990 Accuracy 0.0272\n",
            "Epoch 1 Batch 433 Loss 9.9929 Accuracy 0.0272\n",
            "Epoch 1 Batch 434 Loss 9.9870 Accuracy 0.0272\n",
            "Epoch 1 Batch 435 Loss 9.9809 Accuracy 0.0273\n",
            "Epoch 1 Batch 436 Loss 9.9751 Accuracy 0.0273\n",
            "Epoch 1 Batch 437 Loss 9.9690 Accuracy 0.0273\n",
            "Epoch 1 Batch 438 Loss 9.9631 Accuracy 0.0273\n",
            "Epoch 1 Batch 439 Loss 9.9565 Accuracy 0.0273\n",
            "Epoch 1 Batch 440 Loss 9.9505 Accuracy 0.0273\n",
            "Epoch 1 Batch 441 Loss 9.9449 Accuracy 0.0273\n",
            "Epoch 1 Batch 442 Loss 9.9395 Accuracy 0.0273\n",
            "Epoch 1 Batch 443 Loss 9.9338 Accuracy 0.0273\n",
            "Epoch 1 Batch 444 Loss 9.9274 Accuracy 0.0273\n",
            "Epoch 1 Batch 445 Loss 9.9216 Accuracy 0.0273\n",
            "Epoch 1 Batch 446 Loss 9.9154 Accuracy 0.0274\n",
            "Epoch 1 Batch 447 Loss 9.9096 Accuracy 0.0274\n",
            "Epoch 1 Batch 448 Loss 9.9035 Accuracy 0.0274\n",
            "Epoch 1 Batch 449 Loss 9.8978 Accuracy 0.0274\n",
            "Epoch 1 Batch 450 Loss 9.8922 Accuracy 0.0274\n",
            "Epoch 1 Batch 451 Loss 9.8863 Accuracy 0.0274\n",
            "Epoch 1 Batch 452 Loss 9.8802 Accuracy 0.0274\n",
            "Epoch 1 Batch 453 Loss 9.8745 Accuracy 0.0274\n",
            "Epoch 1 Batch 454 Loss 9.8688 Accuracy 0.0274\n",
            "Epoch 1 Batch 455 Loss 9.8636 Accuracy 0.0274\n",
            "Epoch 1 Batch 456 Loss 9.8584 Accuracy 0.0274\n",
            "Epoch 1 Batch 457 Loss 9.8526 Accuracy 0.0275\n",
            "Epoch 1 Batch 458 Loss 9.8468 Accuracy 0.0275\n",
            "Epoch 1 Batch 459 Loss 9.8407 Accuracy 0.0275\n",
            "Epoch 1 Batch 460 Loss 9.8350 Accuracy 0.0275\n",
            "Epoch 1 Batch 461 Loss 9.8291 Accuracy 0.0275\n",
            "Epoch 1 Batch 462 Loss 9.8234 Accuracy 0.0275\n",
            "Epoch 1 Batch 463 Loss 9.8176 Accuracy 0.0275\n",
            "Epoch 1 Batch 464 Loss 9.8119 Accuracy 0.0275\n",
            "Epoch 1 Batch 465 Loss 9.8062 Accuracy 0.0275\n",
            "Epoch 1 Batch 466 Loss 9.8008 Accuracy 0.0275\n",
            "Epoch 1 Batch 467 Loss 9.7950 Accuracy 0.0275\n",
            "Epoch 1 Batch 468 Loss 9.7892 Accuracy 0.0275\n",
            "Epoch 1 Batch 469 Loss 9.7835 Accuracy 0.0275\n",
            "Epoch 1 Batch 470 Loss 9.7780 Accuracy 0.0276\n",
            "Epoch 1 Batch 471 Loss 9.7726 Accuracy 0.0276\n",
            "Epoch 1 Batch 472 Loss 9.7673 Accuracy 0.0276\n",
            "Epoch 1 Batch 473 Loss 9.7619 Accuracy 0.0276\n",
            "Epoch 1 Batch 474 Loss 9.7567 Accuracy 0.0276\n",
            "Epoch 1 Batch 475 Loss 9.7512 Accuracy 0.0276\n",
            "Epoch 1 Batch 476 Loss 9.7455 Accuracy 0.0276\n",
            "Epoch 1 Batch 477 Loss 9.7400 Accuracy 0.0276\n",
            "Epoch 1 Batch 478 Loss 9.7343 Accuracy 0.0276\n",
            "Epoch 1 Batch 479 Loss 9.7287 Accuracy 0.0276\n",
            "Epoch 1 Batch 480 Loss 9.7233 Accuracy 0.0276\n",
            "Epoch 1 Batch 481 Loss 9.7181 Accuracy 0.0276\n",
            "Epoch 1 Batch 482 Loss 9.7128 Accuracy 0.0276\n",
            "Epoch 1 Batch 483 Loss 9.7077 Accuracy 0.0277\n",
            "Epoch 1 Batch 484 Loss 9.7024 Accuracy 0.0277\n",
            "Epoch 1 Batch 485 Loss 9.6970 Accuracy 0.0277\n",
            "Epoch 1 Batch 486 Loss 9.6916 Accuracy 0.0277\n",
            "Epoch 1 Batch 487 Loss 9.6867 Accuracy 0.0277\n",
            "Epoch 1 Batch 488 Loss 9.6811 Accuracy 0.0277\n",
            "Epoch 1 Batch 489 Loss 9.6758 Accuracy 0.0277\n",
            "Epoch 1 Batch 490 Loss 9.6705 Accuracy 0.0277\n",
            "Epoch 1 Batch 491 Loss 9.6653 Accuracy 0.0277\n",
            "Epoch 1 Batch 492 Loss 9.6594 Accuracy 0.0277\n",
            "Epoch 1 Batch 493 Loss 9.6541 Accuracy 0.0277\n",
            "Epoch 1 Batch 494 Loss 9.6484 Accuracy 0.0277\n",
            "Epoch 1 Batch 495 Loss 9.6432 Accuracy 0.0277\n",
            "Epoch 1 Batch 496 Loss 9.6381 Accuracy 0.0277\n",
            "Epoch 1 Batch 497 Loss 9.6331 Accuracy 0.0278\n",
            "Epoch 1 Batch 498 Loss 9.6280 Accuracy 0.0278\n",
            "Epoch 1 Batch 499 Loss 9.6224 Accuracy 0.0278\n",
            "Epoch 1 Batch 500 Loss 9.6183 Accuracy 0.0278\n",
            "Epoch 1 Batch 501 Loss 9.6134 Accuracy 0.0278\n",
            "Epoch 1 Batch 502 Loss 9.6087 Accuracy 0.0278\n",
            "Epoch 1 Batch 503 Loss 9.6032 Accuracy 0.0278\n",
            "Epoch 1 Batch 504 Loss 9.5984 Accuracy 0.0278\n",
            "Epoch 1 Batch 505 Loss 9.5932 Accuracy 0.0278\n",
            "Epoch 1 Batch 506 Loss 9.5884 Accuracy 0.0278\n",
            "Epoch 1 Batch 507 Loss 9.5835 Accuracy 0.0278\n",
            "Epoch 1 Batch 508 Loss 9.5781 Accuracy 0.0278\n",
            "Epoch 1 Batch 509 Loss 9.5730 Accuracy 0.0278\n",
            "Epoch 1 Batch 510 Loss 9.5686 Accuracy 0.0278\n",
            "Epoch 1 Batch 511 Loss 9.5637 Accuracy 0.0279\n",
            "Epoch 1 Batch 512 Loss 9.5589 Accuracy 0.0279\n",
            "Epoch 1 Batch 513 Loss 9.5535 Accuracy 0.0279\n",
            "Epoch 1 Batch 514 Loss 9.5488 Accuracy 0.0279\n",
            "Epoch 1 Batch 515 Loss 9.5434 Accuracy 0.0279\n",
            "Epoch 1 Batch 516 Loss 9.5385 Accuracy 0.0279\n",
            "Epoch 1 Batch 517 Loss 9.5335 Accuracy 0.0279\n",
            "Epoch 1 Batch 518 Loss 9.5290 Accuracy 0.0279\n",
            "Epoch 1 Batch 519 Loss 9.5241 Accuracy 0.0279\n",
            "Epoch 1 Batch 520 Loss 9.5193 Accuracy 0.0279\n",
            "Epoch 1 Batch 521 Loss 9.5147 Accuracy 0.0279\n",
            "Epoch 1 Batch 522 Loss 9.5099 Accuracy 0.0279\n",
            "Epoch 1 Batch 523 Loss 9.5054 Accuracy 0.0279\n",
            "Epoch 1 Batch 524 Loss 9.5006 Accuracy 0.0279\n",
            "Epoch 1 Batch 525 Loss 9.4958 Accuracy 0.0279\n",
            "Epoch 1 Batch 526 Loss 9.4912 Accuracy 0.0279\n",
            "Epoch 1 Batch 527 Loss 9.4865 Accuracy 0.0280\n",
            "Epoch 1 Batch 528 Loss 9.4815 Accuracy 0.0280\n",
            "Epoch 1 Batch 529 Loss 9.4766 Accuracy 0.0280\n",
            "Epoch 1 Batch 530 Loss 9.4718 Accuracy 0.0280\n",
            "Epoch 1 Batch 531 Loss 9.4677 Accuracy 0.0280\n",
            "Epoch 1 Batch 532 Loss 9.4631 Accuracy 0.0280\n",
            "Epoch 1 Batch 533 Loss 9.4583 Accuracy 0.0280\n",
            "Epoch 1 Batch 534 Loss 9.4535 Accuracy 0.0280\n",
            "Epoch 1 Batch 535 Loss 9.4490 Accuracy 0.0280\n",
            "Epoch 1 Batch 536 Loss 9.4445 Accuracy 0.0280\n",
            "Epoch 1 Batch 537 Loss 9.4400 Accuracy 0.0280\n",
            "Epoch 1 Batch 538 Loss 9.4356 Accuracy 0.0280\n",
            "Epoch 1 Batch 539 Loss 9.4310 Accuracy 0.0280\n",
            "Epoch 1 Batch 540 Loss 9.4263 Accuracy 0.0280\n",
            "Epoch 1 Batch 541 Loss 9.4217 Accuracy 0.0280\n",
            "Epoch 1 Batch 542 Loss 9.4174 Accuracy 0.0280\n",
            "Epoch 1 Batch 543 Loss 9.4129 Accuracy 0.0281\n",
            "Epoch 1 Batch 544 Loss 9.4083 Accuracy 0.0281\n",
            "Epoch 1 Batch 545 Loss 9.4039 Accuracy 0.0281\n",
            "Epoch 1 Batch 546 Loss 9.3998 Accuracy 0.0281\n",
            "Epoch 1 Batch 547 Loss 9.3956 Accuracy 0.0281\n",
            "Epoch 1 Batch 548 Loss 9.3910 Accuracy 0.0281\n",
            "Epoch 1 Batch 549 Loss 9.3868 Accuracy 0.0281\n",
            "Epoch 1 Batch 550 Loss 9.3825 Accuracy 0.0281\n",
            "Epoch 1 Batch 551 Loss 9.3777 Accuracy 0.0281\n",
            "Epoch 1 Batch 552 Loss 9.3730 Accuracy 0.0281\n",
            "Epoch 1 Batch 553 Loss 9.3691 Accuracy 0.0281\n",
            "Epoch 1 Batch 554 Loss 9.3649 Accuracy 0.0281\n",
            "Epoch 1 Batch 555 Loss 9.3604 Accuracy 0.0281\n",
            "Epoch 1 Batch 556 Loss 9.3561 Accuracy 0.0281\n",
            "Epoch 1 Batch 557 Loss 9.3517 Accuracy 0.0281\n",
            "Epoch 1 Batch 558 Loss 9.3473 Accuracy 0.0281\n",
            "Epoch 1 Batch 559 Loss 9.3427 Accuracy 0.0281\n",
            "Epoch 1 Batch 560 Loss 9.3388 Accuracy 0.0281\n",
            "Epoch 1 Batch 561 Loss 9.3343 Accuracy 0.0282\n",
            "Epoch 1 Batch 562 Loss 9.3304 Accuracy 0.0282\n",
            "Epoch 1 Batch 563 Loss 9.3261 Accuracy 0.0282\n",
            "Epoch 1 Batch 564 Loss 9.3219 Accuracy 0.0282\n",
            "Epoch 1 Batch 565 Loss 9.3178 Accuracy 0.0282\n",
            "Epoch 1 Batch 566 Loss 9.3137 Accuracy 0.0282\n",
            "Epoch 1 Batch 567 Loss 9.3094 Accuracy 0.0282\n",
            "Epoch 1 Batch 568 Loss 9.3050 Accuracy 0.0282\n",
            "Epoch 1 Batch 569 Loss 9.3007 Accuracy 0.0282\n",
            "Epoch 1 Batch 570 Loss 9.2965 Accuracy 0.0282\n",
            "Epoch 1 Batch 571 Loss 9.2926 Accuracy 0.0282\n",
            "Epoch 1 Batch 572 Loss 9.2886 Accuracy 0.0282\n",
            "Epoch 1 Batch 573 Loss 9.2847 Accuracy 0.0282\n",
            "Epoch 1 Batch 574 Loss 9.2810 Accuracy 0.0282\n",
            "Epoch 1 Batch 575 Loss 9.2770 Accuracy 0.0282\n",
            "Epoch 1 Batch 576 Loss 9.2732 Accuracy 0.0282\n",
            "Epoch 1 Batch 577 Loss 9.2692 Accuracy 0.0282\n",
            "Epoch 1 Batch 578 Loss 9.2654 Accuracy 0.0282\n",
            "Epoch 1 Batch 579 Loss 9.2619 Accuracy 0.0282\n",
            "Epoch 1 Batch 580 Loss 9.2578 Accuracy 0.0283\n",
            "Epoch 1 Batch 581 Loss 9.2538 Accuracy 0.0283\n",
            "Epoch 1 Batch 582 Loss 9.2504 Accuracy 0.0283\n",
            "Epoch 1 Batch 583 Loss 9.2469 Accuracy 0.0283\n",
            "Epoch 1 Batch 584 Loss 9.2434 Accuracy 0.0283\n",
            "Epoch 1 Batch 585 Loss 9.2393 Accuracy 0.0283\n",
            "Epoch 1 Batch 586 Loss 9.2351 Accuracy 0.0283\n",
            "Epoch 1 Batch 587 Loss 9.2310 Accuracy 0.0283\n",
            "Epoch 1 Batch 588 Loss 9.2274 Accuracy 0.0283\n",
            "Epoch 1 Batch 589 Loss 9.2237 Accuracy 0.0283\n",
            "Epoch 1 Batch 590 Loss 9.2198 Accuracy 0.0283\n",
            "Epoch 1 Batch 591 Loss 9.2159 Accuracy 0.0283\n",
            "Epoch 1 Batch 592 Loss 9.2118 Accuracy 0.0283\n",
            "Epoch 1 Batch 593 Loss 9.2082 Accuracy 0.0283\n",
            "Epoch 1 Batch 594 Loss 9.2043 Accuracy 0.0283\n",
            "Epoch 1 Batch 595 Loss 9.2005 Accuracy 0.0283\n",
            "Epoch 1 Batch 596 Loss 9.1966 Accuracy 0.0283\n",
            "Epoch 1 Batch 597 Loss 9.1929 Accuracy 0.0283\n",
            "Epoch 1 Batch 598 Loss 9.1894 Accuracy 0.0283\n",
            "Epoch 1 Batch 599 Loss 9.1856 Accuracy 0.0283\n",
            "Epoch 1 Batch 600 Loss 9.1820 Accuracy 0.0284\n",
            "Epoch 1 Batch 601 Loss 9.1784 Accuracy 0.0284\n",
            "Epoch 1 Batch 602 Loss 9.1748 Accuracy 0.0284\n",
            "Epoch 1 Batch 603 Loss 9.1715 Accuracy 0.0284\n",
            "Epoch 1 Batch 604 Loss 9.1676 Accuracy 0.0284\n",
            "Epoch 1 Batch 605 Loss 9.1642 Accuracy 0.0284\n",
            "Epoch 1 Batch 606 Loss 9.1607 Accuracy 0.0284\n",
            "Epoch 1 Batch 607 Loss 9.1572 Accuracy 0.0284\n",
            "Epoch 1 Batch 608 Loss 9.1535 Accuracy 0.0284\n",
            "Epoch 1 Batch 609 Loss 9.1496 Accuracy 0.0284\n",
            "Epoch 1 Batch 610 Loss 9.1458 Accuracy 0.0284\n",
            "Epoch 1 Batch 611 Loss 9.1423 Accuracy 0.0284\n",
            "Epoch 1 Batch 612 Loss 9.1388 Accuracy 0.0284\n",
            "Epoch 1 Batch 613 Loss 9.1352 Accuracy 0.0284\n",
            "Epoch 1 Batch 614 Loss 9.1318 Accuracy 0.0284\n",
            "Epoch 1 Batch 615 Loss 9.1281 Accuracy 0.0284\n",
            "Epoch 1 Batch 616 Loss 9.1245 Accuracy 0.0284\n",
            "Epoch 1 Batch 617 Loss 9.1213 Accuracy 0.0284\n",
            "Epoch 1 Batch 618 Loss 9.1178 Accuracy 0.0284\n",
            "Epoch 1 Batch 619 Loss 9.1144 Accuracy 0.0284\n",
            "Epoch 1 Batch 620 Loss 9.1109 Accuracy 0.0284\n",
            "Epoch 1 Batch 621 Loss 9.1076 Accuracy 0.0285\n",
            "Epoch 1 Batch 622 Loss 9.1043 Accuracy 0.0285\n",
            "Epoch 1 Batch 623 Loss 9.1008 Accuracy 0.0285\n",
            "Epoch 1 Batch 624 Loss 9.0977 Accuracy 0.0285\n",
            "Epoch 1 Batch 625 Loss 9.0941 Accuracy 0.0285\n",
            "Epoch 1 Batch 626 Loss 9.0908 Accuracy 0.0285\n",
            "Epoch 1 Batch 627 Loss 9.0871 Accuracy 0.0285\n",
            "Epoch 1 Batch 628 Loss 9.0837 Accuracy 0.0285\n",
            "Epoch 1 Batch 629 Loss 9.0802 Accuracy 0.0285\n",
            "Epoch 1 Batch 630 Loss 9.0765 Accuracy 0.0285\n",
            "Epoch 1 Batch 631 Loss 9.0729 Accuracy 0.0285\n",
            "Epoch 1 Batch 632 Loss 9.0695 Accuracy 0.0285\n",
            "Epoch 1 Batch 633 Loss 9.0664 Accuracy 0.0285\n",
            "Epoch 1 Batch 634 Loss 9.0628 Accuracy 0.0285\n",
            "Epoch 1 Batch 635 Loss 9.0592 Accuracy 0.0285\n",
            "Epoch 1 Batch 636 Loss 9.0561 Accuracy 0.0285\n",
            "Epoch 1 Batch 637 Loss 9.0526 Accuracy 0.0285\n",
            "Epoch 1 Batch 638 Loss 9.0494 Accuracy 0.0285\n",
            "Epoch 1 Batch 639 Loss 9.0459 Accuracy 0.0285\n",
            "Epoch 1 Batch 640 Loss 9.0425 Accuracy 0.0285\n",
            "Epoch 1 Batch 641 Loss 9.0395 Accuracy 0.0285\n",
            "Epoch 1 Batch 642 Loss 9.0361 Accuracy 0.0285\n",
            "Epoch 1 Batch 643 Loss 9.0326 Accuracy 0.0285\n",
            "Epoch 1 Batch 644 Loss 9.0290 Accuracy 0.0286\n",
            "Epoch 1 Batch 645 Loss 9.0255 Accuracy 0.0286\n",
            "Epoch 1 Batch 646 Loss 9.0223 Accuracy 0.0286\n",
            "Epoch 1 Batch 647 Loss 9.0194 Accuracy 0.0286\n",
            "Epoch 1 Batch 648 Loss 9.0160 Accuracy 0.0286\n",
            "Epoch 1 Batch 649 Loss 9.0127 Accuracy 0.0286\n",
            "Epoch 1 Batch 650 Loss 9.0095 Accuracy 0.0286\n",
            "Epoch 1 Batch 651 Loss 9.0063 Accuracy 0.0286\n",
            "Epoch 1 Batch 652 Loss 9.0029 Accuracy 0.0286\n",
            "Epoch 1 Batch 653 Loss 9.0001 Accuracy 0.0286\n",
            "Epoch 1 Batch 654 Loss 8.9968 Accuracy 0.0286\n",
            "Epoch 1 Batch 655 Loss 8.9938 Accuracy 0.0286\n",
            "Epoch 1 Batch 656 Loss 8.9907 Accuracy 0.0286\n",
            "Epoch 1 Batch 657 Loss 8.9875 Accuracy 0.0286\n",
            "Epoch 1 Batch 658 Loss 8.9844 Accuracy 0.0286\n",
            "Epoch 1 Batch 659 Loss 8.9813 Accuracy 0.0286\n",
            "Epoch 1 Batch 660 Loss 8.9783 Accuracy 0.0286\n",
            "Epoch 1 Batch 661 Loss 8.9750 Accuracy 0.0286\n",
            "Epoch 1 Batch 662 Loss 8.9721 Accuracy 0.0286\n",
            "Epoch 1 Batch 663 Loss 8.9688 Accuracy 0.0286\n",
            "Epoch 1 Batch 664 Loss 8.9657 Accuracy 0.0286\n",
            "Epoch 1 Batch 665 Loss 8.9627 Accuracy 0.0286\n",
            "Epoch 1 Batch 666 Loss 8.9597 Accuracy 0.0286\n",
            "Epoch 1 Batch 667 Loss 8.9568 Accuracy 0.0286\n",
            "Epoch 1 Batch 668 Loss 8.9537 Accuracy 0.0286\n",
            "Epoch 1 Batch 669 Loss 8.9507 Accuracy 0.0287\n",
            "Epoch 1 Batch 670 Loss 8.9477 Accuracy 0.0287\n",
            "Epoch 1 Batch 671 Loss 8.9447 Accuracy 0.0287\n",
            "Epoch 1 Batch 672 Loss 8.9417 Accuracy 0.0287\n",
            "Epoch 1 Batch 673 Loss 8.9387 Accuracy 0.0287\n",
            "Epoch 1 Batch 674 Loss 8.9354 Accuracy 0.0287\n",
            "Epoch 1 Batch 675 Loss 8.9325 Accuracy 0.0287\n",
            "Epoch 1 Batch 676 Loss 8.9295 Accuracy 0.0287\n",
            "Epoch 1 Batch 677 Loss 8.9266 Accuracy 0.0287\n",
            "Epoch 1 Batch 678 Loss 8.9239 Accuracy 0.0287\n",
            "Epoch 1 Batch 679 Loss 8.9210 Accuracy 0.0287\n",
            "Epoch 1 Batch 680 Loss 8.9179 Accuracy 0.0287\n",
            "Epoch 1 Batch 681 Loss 8.9150 Accuracy 0.0287\n",
            "Epoch 1 Batch 682 Loss 8.9123 Accuracy 0.0287\n",
            "Epoch 1 Batch 683 Loss 8.9091 Accuracy 0.0287\n",
            "Epoch 1 Batch 684 Loss 8.9058 Accuracy 0.0287\n",
            "Epoch 1 Batch 685 Loss 8.9026 Accuracy 0.0287\n",
            "Epoch 1 Batch 686 Loss 8.8996 Accuracy 0.0288\n",
            "Epoch 1 Batch 687 Loss 8.8965 Accuracy 0.0288\n",
            "Epoch 1 Batch 688 Loss 8.8932 Accuracy 0.0288\n",
            "Epoch 1 Batch 689 Loss 8.8904 Accuracy 0.0288\n",
            "Epoch 1 Batch 690 Loss 8.8876 Accuracy 0.0288\n",
            "Epoch 1 Batch 691 Loss 8.8846 Accuracy 0.0288\n",
            "Epoch 1 Batch 692 Loss 8.8819 Accuracy 0.0288\n",
            "Epoch 1 Batch 693 Loss 8.8790 Accuracy 0.0288\n",
            "Epoch 1 Batch 694 Loss 8.8761 Accuracy 0.0288\n",
            "Epoch 1 Batch 695 Loss 8.8735 Accuracy 0.0288\n",
            "Epoch 1 Batch 696 Loss 8.8708 Accuracy 0.0288\n",
            "Epoch 1 Batch 697 Loss 8.8679 Accuracy 0.0288\n",
            "Epoch 1 Batch 698 Loss 8.8649 Accuracy 0.0288\n",
            "Epoch 1 Batch 699 Loss 8.8618 Accuracy 0.0289\n",
            "Epoch 1 Batch 700 Loss 8.8588 Accuracy 0.0289\n",
            "Epoch 1 Batch 701 Loss 8.8558 Accuracy 0.0289\n",
            "Epoch 1 Batch 702 Loss 8.8530 Accuracy 0.0289\n",
            "Epoch 1 Batch 703 Loss 8.8504 Accuracy 0.0289\n",
            "Epoch 1 Batch 704 Loss 8.8479 Accuracy 0.0289\n",
            "Epoch 1 Batch 705 Loss 8.8450 Accuracy 0.0289\n",
            "Epoch 1 Batch 706 Loss 8.8420 Accuracy 0.0289\n",
            "Epoch 1 Batch 707 Loss 8.8390 Accuracy 0.0289\n",
            "Epoch 1 Batch 708 Loss 8.8366 Accuracy 0.0289\n",
            "Epoch 1 Batch 709 Loss 8.8340 Accuracy 0.0289\n",
            "Epoch 1 Batch 710 Loss 8.8309 Accuracy 0.0289\n",
            "Epoch 1 Batch 711 Loss 8.8281 Accuracy 0.0290\n",
            "Epoch 1 Batch 712 Loss 8.8252 Accuracy 0.0290\n",
            "Epoch 1 Batch 713 Loss 8.8225 Accuracy 0.0290\n",
            "Epoch 1 Batch 714 Loss 8.8199 Accuracy 0.0290\n",
            "Epoch 1 Batch 715 Loss 8.8170 Accuracy 0.0290\n",
            "Epoch 1 Batch 716 Loss 8.8145 Accuracy 0.0290\n",
            "Epoch 1 Batch 717 Loss 8.8118 Accuracy 0.0290\n",
            "Epoch 1 Batch 718 Loss 8.8089 Accuracy 0.0290\n",
            "Epoch 1 Batch 719 Loss 8.8063 Accuracy 0.0290\n",
            "Epoch 1 Batch 720 Loss 8.8043 Accuracy 0.0290\n",
            "Epoch 1 Batch 721 Loss 8.8013 Accuracy 0.0290\n",
            "Epoch 1 Batch 722 Loss 8.7986 Accuracy 0.0290\n",
            "Epoch 1 Batch 723 Loss 8.7957 Accuracy 0.0291\n",
            "Epoch 1 Batch 724 Loss 8.7932 Accuracy 0.0291\n",
            "Epoch 1 Batch 725 Loss 8.7904 Accuracy 0.0291\n",
            "Epoch 1 Batch 726 Loss 8.7876 Accuracy 0.0291\n",
            "Epoch 1 Batch 727 Loss 8.7849 Accuracy 0.0291\n",
            "Epoch 1 Batch 728 Loss 8.7815 Accuracy 0.0291\n",
            "Epoch 1 Batch 729 Loss 8.7786 Accuracy 0.0291\n",
            "Epoch 1 Batch 730 Loss 8.7762 Accuracy 0.0291\n",
            "Epoch 1 Batch 731 Loss 8.7734 Accuracy 0.0291\n",
            "Epoch 1 Batch 732 Loss 8.7707 Accuracy 0.0291\n",
            "Epoch 1 Batch 733 Loss 8.7682 Accuracy 0.0291\n",
            "Epoch 1 Batch 734 Loss 8.7654 Accuracy 0.0292\n",
            "Epoch 1 Batch 735 Loss 8.7628 Accuracy 0.0292\n",
            "Epoch 1 Batch 736 Loss 8.7605 Accuracy 0.0292\n",
            "Epoch 1 Batch 737 Loss 8.7579 Accuracy 0.0292\n",
            "Epoch 1 Batch 738 Loss 8.7554 Accuracy 0.0292\n",
            "Epoch 1 Batch 739 Loss 8.7527 Accuracy 0.0292\n",
            "Epoch 1 Batch 740 Loss 8.7502 Accuracy 0.0292\n",
            "Epoch 1 Batch 741 Loss 8.7472 Accuracy 0.0292\n",
            "Epoch 1 Batch 742 Loss 8.7448 Accuracy 0.0292\n",
            "Epoch 1 Batch 743 Loss 8.7423 Accuracy 0.0292\n",
            "Epoch 1 Batch 744 Loss 8.7398 Accuracy 0.0292\n",
            "Epoch 1 Batch 745 Loss 8.7371 Accuracy 0.0293\n",
            "Epoch 1 Batch 746 Loss 8.7343 Accuracy 0.0293\n",
            "Epoch 1 Batch 747 Loss 8.7317 Accuracy 0.0293\n",
            "Epoch 1 Batch 748 Loss 8.7289 Accuracy 0.0293\n",
            "Epoch 1 Batch 749 Loss 8.7261 Accuracy 0.0293\n",
            "Epoch 1 Batch 750 Loss 8.7234 Accuracy 0.0293\n",
            "Epoch 1 Batch 751 Loss 8.7211 Accuracy 0.0293\n",
            "Epoch 1 Batch 752 Loss 8.7184 Accuracy 0.0293\n",
            "Epoch 1 Batch 753 Loss 8.7156 Accuracy 0.0293\n",
            "Epoch 1 Batch 754 Loss 8.7130 Accuracy 0.0293\n",
            "Epoch 1 Batch 755 Loss 8.7106 Accuracy 0.0294\n",
            "Epoch 1 Batch 756 Loss 8.7081 Accuracy 0.0294\n",
            "Epoch 1 Batch 757 Loss 8.7059 Accuracy 0.0294\n",
            "Epoch 1 Batch 758 Loss 8.7034 Accuracy 0.0294\n",
            "Epoch 1 Batch 759 Loss 8.7010 Accuracy 0.0294\n",
            "Epoch 1 Batch 760 Loss 8.6985 Accuracy 0.0294\n",
            "Epoch 1 Batch 761 Loss 8.6956 Accuracy 0.0294\n",
            "Epoch 1 Batch 762 Loss 8.6930 Accuracy 0.0294\n",
            "Epoch 1 Batch 763 Loss 8.6902 Accuracy 0.0294\n",
            "Epoch 1 Batch 764 Loss 8.6877 Accuracy 0.0294\n",
            "Epoch 1 Batch 765 Loss 8.6851 Accuracy 0.0295\n",
            "Epoch 1 Batch 766 Loss 8.6824 Accuracy 0.0295\n",
            "Epoch 1 Batch 767 Loss 8.6798 Accuracy 0.0295\n",
            "Epoch 1 Batch 768 Loss 8.6771 Accuracy 0.0295\n",
            "Epoch 1 Batch 769 Loss 8.6742 Accuracy 0.0295\n",
            "Epoch 1 Batch 770 Loss 8.6717 Accuracy 0.0295\n",
            "Epoch 1 Batch 771 Loss 8.6690 Accuracy 0.0295\n",
            "Epoch 1 Batch 772 Loss 8.6661 Accuracy 0.0296\n",
            "Epoch 1 Batch 773 Loss 8.6639 Accuracy 0.0296\n",
            "Epoch 1 Batch 774 Loss 8.6614 Accuracy 0.0296\n",
            "Epoch 1 Batch 775 Loss 8.6590 Accuracy 0.0296\n",
            "Epoch 1 Batch 776 Loss 8.6569 Accuracy 0.0296\n",
            "Epoch 1 Batch 777 Loss 8.6546 Accuracy 0.0296\n",
            "Epoch 1 Batch 778 Loss 8.6522 Accuracy 0.0296\n",
            "Epoch 1 Batch 779 Loss 8.6498 Accuracy 0.0296\n",
            "Epoch 1 Batch 780 Loss 8.6473 Accuracy 0.0296\n",
            "Epoch 1 Batch 781 Loss 8.6449 Accuracy 0.0297\n",
            "Epoch 1 Batch 782 Loss 8.6426 Accuracy 0.0297\n",
            "Epoch 1 Batch 783 Loss 8.6400 Accuracy 0.0297\n",
            "Epoch 1 Batch 784 Loss 8.6376 Accuracy 0.0297\n",
            "Epoch 1 Batch 785 Loss 8.6353 Accuracy 0.0297\n",
            "Epoch 1 Batch 786 Loss 8.6328 Accuracy 0.0297\n",
            "Epoch 1 Batch 787 Loss 8.6305 Accuracy 0.0297\n",
            "Epoch 1 Batch 788 Loss 8.6279 Accuracy 0.0297\n",
            "Epoch 1 Batch 789 Loss 8.6251 Accuracy 0.0298\n",
            "Epoch 1 Batch 790 Loss 8.6226 Accuracy 0.0298\n",
            "Epoch 1 Batch 791 Loss 8.6200 Accuracy 0.0298\n",
            "Epoch 1 Batch 792 Loss 8.6174 Accuracy 0.0298\n",
            "Epoch 1 Batch 793 Loss 8.6150 Accuracy 0.0298\n",
            "Epoch 1 Batch 794 Loss 8.6125 Accuracy 0.0298\n",
            "Epoch 1 Batch 795 Loss 8.6100 Accuracy 0.0298\n",
            "Epoch 1 Batch 796 Loss 8.6074 Accuracy 0.0299\n",
            "Epoch 1 Batch 797 Loss 8.6050 Accuracy 0.0299\n",
            "Epoch 1 Batch 798 Loss 8.6024 Accuracy 0.0299\n",
            "Epoch 1 Batch 799 Loss 8.6001 Accuracy 0.0299\n",
            "Epoch 1 Batch 800 Loss 8.5975 Accuracy 0.0299\n",
            "Epoch 1 Batch 801 Loss 8.5950 Accuracy 0.0299\n",
            "Epoch 1 Batch 802 Loss 8.5927 Accuracy 0.0299\n",
            "Epoch 1 Batch 803 Loss 8.5904 Accuracy 0.0300\n",
            "Epoch 1 Batch 804 Loss 8.5881 Accuracy 0.0300\n",
            "Epoch 1 Batch 805 Loss 8.5856 Accuracy 0.0300\n",
            "Epoch 1 Batch 806 Loss 8.5834 Accuracy 0.0300\n",
            "Epoch 1 Batch 807 Loss 8.5808 Accuracy 0.0300\n",
            "Epoch 1 Batch 808 Loss 8.5784 Accuracy 0.0301\n",
            "Epoch 1 Batch 809 Loss 8.5758 Accuracy 0.0301\n",
            "Epoch 1 Batch 810 Loss 8.5736 Accuracy 0.0301\n",
            "Epoch 1 Batch 811 Loss 8.5714 Accuracy 0.0301\n",
            "Epoch 1 Batch 812 Loss 8.5687 Accuracy 0.0301\n",
            "Epoch 1 Batch 813 Loss 8.5664 Accuracy 0.0301\n",
            "Epoch 1 Batch 814 Loss 8.5641 Accuracy 0.0301\n",
            "Epoch 1 Batch 815 Loss 8.5618 Accuracy 0.0302\n",
            "Epoch 1 Batch 816 Loss 8.5590 Accuracy 0.0302\n",
            "Epoch 1 Batch 817 Loss 8.5564 Accuracy 0.0302\n",
            "Epoch 1 Batch 818 Loss 8.5541 Accuracy 0.0302\n",
            "Epoch 1 Batch 819 Loss 8.5518 Accuracy 0.0302\n",
            "Epoch 1 Batch 820 Loss 8.5497 Accuracy 0.0302\n",
            "Epoch 1 Batch 821 Loss 8.5473 Accuracy 0.0303\n",
            "Epoch 1 Batch 822 Loss 8.5451 Accuracy 0.0303\n",
            "Epoch 1 Batch 823 Loss 8.5428 Accuracy 0.0303\n",
            "Epoch 1 Batch 824 Loss 8.5403 Accuracy 0.0303\n",
            "Epoch 1 Batch 825 Loss 8.5378 Accuracy 0.0303\n",
            "Epoch 1 Batch 826 Loss 8.5356 Accuracy 0.0303\n",
            "Epoch 1 Batch 827 Loss 8.5333 Accuracy 0.0304\n",
            "Epoch 1 Batch 828 Loss 8.5312 Accuracy 0.0304\n",
            "Epoch 1 Batch 829 Loss 8.5288 Accuracy 0.0304\n",
            "Epoch 1 Batch 830 Loss 8.5265 Accuracy 0.0304\n",
            "Epoch 1 Batch 831 Loss 8.5241 Accuracy 0.0304\n",
            "Epoch 1 Batch 832 Loss 8.5218 Accuracy 0.0305\n",
            "Epoch 1 Batch 833 Loss 8.5195 Accuracy 0.0305\n",
            "Epoch 1 Batch 834 Loss 8.5172 Accuracy 0.0305\n",
            "Epoch 1 Batch 835 Loss 8.5148 Accuracy 0.0305\n",
            "Epoch 1 Batch 836 Loss 8.5126 Accuracy 0.0305\n",
            "Epoch 1 Batch 837 Loss 8.5103 Accuracy 0.0305\n",
            "Epoch 1 Batch 838 Loss 8.5082 Accuracy 0.0306\n",
            "Epoch 1 Batch 839 Loss 8.5059 Accuracy 0.0306\n",
            "Epoch 1 Batch 840 Loss 8.5037 Accuracy 0.0306\n",
            "Epoch 1 Batch 841 Loss 8.5017 Accuracy 0.0306\n",
            "Epoch 1 Batch 842 Loss 8.4994 Accuracy 0.0306\n",
            "Epoch 1 Batch 843 Loss 8.4972 Accuracy 0.0306\n",
            "Epoch 1 Batch 844 Loss 8.4950 Accuracy 0.0307\n",
            "Epoch 1 Batch 845 Loss 8.4931 Accuracy 0.0307\n",
            "Epoch 1 Batch 846 Loss 8.4910 Accuracy 0.0307\n",
            "Epoch 1 Batch 847 Loss 8.4890 Accuracy 0.0307\n",
            "Epoch 1 Batch 848 Loss 8.4866 Accuracy 0.0307\n",
            "Epoch 1 Batch 849 Loss 8.4844 Accuracy 0.0307\n",
            "Epoch 1 Batch 850 Loss 8.4824 Accuracy 0.0308\n",
            "Epoch 1 Batch 851 Loss 8.4803 Accuracy 0.0308\n",
            "Epoch 1 Batch 852 Loss 8.4783 Accuracy 0.0308\n",
            "Epoch 1 Batch 853 Loss 8.4764 Accuracy 0.0308\n",
            "Epoch 1 Batch 854 Loss 8.4742 Accuracy 0.0308\n",
            "Epoch 1 Batch 855 Loss 8.4723 Accuracy 0.0309\n",
            "Epoch 1 Batch 856 Loss 8.4701 Accuracy 0.0309\n",
            "Epoch 1 Batch 857 Loss 8.4678 Accuracy 0.0309\n",
            "Epoch 1 Batch 858 Loss 8.4658 Accuracy 0.0309\n",
            "Epoch 1 Batch 859 Loss 8.4638 Accuracy 0.0309\n",
            "Epoch 1 Batch 860 Loss 8.4617 Accuracy 0.0309\n",
            "Epoch 1 Batch 861 Loss 8.4594 Accuracy 0.0310\n",
            "Epoch 1 Batch 862 Loss 8.4572 Accuracy 0.0310\n",
            "Epoch 1 Batch 863 Loss 8.4549 Accuracy 0.0310\n",
            "Epoch 1 Batch 864 Loss 8.4530 Accuracy 0.0310\n",
            "Epoch 1 Batch 865 Loss 8.4510 Accuracy 0.0310\n",
            "Epoch 1 Batch 866 Loss 8.4490 Accuracy 0.0310\n",
            "Epoch 1 Batch 867 Loss 8.4468 Accuracy 0.0311\n",
            "Epoch 1 Batch 868 Loss 8.4451 Accuracy 0.0311\n",
            "Epoch 1 Batch 869 Loss 8.4428 Accuracy 0.0311\n",
            "Epoch 1 Batch 870 Loss 8.4403 Accuracy 0.0311\n",
            "Epoch 1 Batch 871 Loss 8.4383 Accuracy 0.0311\n",
            "Epoch 1 Batch 872 Loss 8.4363 Accuracy 0.0311\n",
            "Epoch 1 Batch 873 Loss 8.4341 Accuracy 0.0312\n",
            "Epoch 1 Batch 874 Loss 8.4321 Accuracy 0.0312\n",
            "Epoch 1 Batch 875 Loss 8.4304 Accuracy 0.0312\n",
            "Epoch 1 Batch 876 Loss 8.4283 Accuracy 0.0312\n",
            "Epoch 1 Batch 877 Loss 8.4264 Accuracy 0.0312\n",
            "Epoch 1 Batch 878 Loss 8.4242 Accuracy 0.0313\n",
            "Epoch 1 Batch 879 Loss 8.4223 Accuracy 0.0313\n",
            "Epoch 1 Batch 880 Loss 8.4205 Accuracy 0.0313\n",
            "Epoch 1 Batch 881 Loss 8.4183 Accuracy 0.0313\n",
            "Epoch 1 Batch 882 Loss 8.4159 Accuracy 0.0313\n",
            "Epoch 1 Batch 883 Loss 8.4137 Accuracy 0.0313\n",
            "Epoch 1 Batch 884 Loss 8.4112 Accuracy 0.0314\n",
            "Epoch 1 Batch 885 Loss 8.4093 Accuracy 0.0314\n",
            "Epoch 1 Batch 886 Loss 8.4070 Accuracy 0.0314\n",
            "Epoch 1 Batch 887 Loss 8.4047 Accuracy 0.0314\n",
            "Epoch 1 Batch 888 Loss 8.4024 Accuracy 0.0314\n",
            "Epoch 1 Batch 889 Loss 8.4003 Accuracy 0.0315\n",
            "Epoch 1 Batch 890 Loss 8.3986 Accuracy 0.0315\n",
            "Epoch 1 Batch 891 Loss 8.3965 Accuracy 0.0315\n",
            "Epoch 1 Batch 892 Loss 8.3945 Accuracy 0.0315\n",
            "Epoch 1 Batch 893 Loss 8.3925 Accuracy 0.0315\n",
            "Epoch 1 Batch 894 Loss 8.3905 Accuracy 0.0315\n",
            "Epoch 1 Batch 895 Loss 8.3884 Accuracy 0.0316\n",
            "Epoch 1 Batch 896 Loss 8.3866 Accuracy 0.0316\n",
            "Epoch 1 Batch 897 Loss 8.3846 Accuracy 0.0316\n",
            "Epoch 1 Batch 898 Loss 8.3828 Accuracy 0.0316\n",
            "Epoch 1 Batch 899 Loss 8.3810 Accuracy 0.0316\n",
            "Epoch 1 Batch 900 Loss 8.3792 Accuracy 0.0316\n",
            "Epoch 1 Batch 901 Loss 8.3770 Accuracy 0.0317\n",
            "Epoch 1 Batch 902 Loss 8.3750 Accuracy 0.0317\n",
            "Epoch 1 Batch 903 Loss 8.3730 Accuracy 0.0317\n",
            "Epoch 1 Batch 904 Loss 8.3710 Accuracy 0.0317\n",
            "Epoch 1 Batch 905 Loss 8.3690 Accuracy 0.0317\n",
            "Epoch 1 Batch 906 Loss 8.3672 Accuracy 0.0317\n",
            "Epoch 1 Batch 907 Loss 8.3655 Accuracy 0.0318\n",
            "Epoch 1 Batch 908 Loss 8.3636 Accuracy 0.0318\n",
            "Epoch 1 Batch 909 Loss 8.3615 Accuracy 0.0318\n",
            "Epoch 1 Batch 910 Loss 8.3594 Accuracy 0.0318\n",
            "Epoch 1 Batch 911 Loss 8.3574 Accuracy 0.0318\n",
            "Epoch 1 Batch 912 Loss 8.3558 Accuracy 0.0318\n",
            "Epoch 1 Batch 913 Loss 8.3537 Accuracy 0.0319\n",
            "Epoch 1 Batch 914 Loss 8.3517 Accuracy 0.0319\n",
            "Epoch 1 Batch 915 Loss 8.3495 Accuracy 0.0319\n",
            "Epoch 1 Batch 916 Loss 8.3475 Accuracy 0.0319\n",
            "Epoch 1 Batch 917 Loss 8.3456 Accuracy 0.0319\n",
            "Epoch 1 Batch 918 Loss 8.3439 Accuracy 0.0320\n",
            "Epoch 1 Batch 919 Loss 8.3416 Accuracy 0.0320\n",
            "Epoch 1 Batch 920 Loss 8.3398 Accuracy 0.0320\n",
            "Epoch 1 Batch 921 Loss 8.3379 Accuracy 0.0320\n",
            "Epoch 1 Batch 922 Loss 8.3360 Accuracy 0.0320\n",
            "Epoch 1 Batch 923 Loss 8.3341 Accuracy 0.0320\n",
            "Epoch 1 Batch 924 Loss 8.3318 Accuracy 0.0321\n",
            "Epoch 1 Batch 925 Loss 8.3297 Accuracy 0.0321\n",
            "Epoch 1 Batch 926 Loss 8.3278 Accuracy 0.0321\n",
            "Epoch 1 Batch 927 Loss 8.3259 Accuracy 0.0321\n",
            "Epoch 1 Batch 928 Loss 8.3240 Accuracy 0.0321\n",
            "Epoch 1 Batch 929 Loss 8.3220 Accuracy 0.0321\n",
            "Epoch 1 Batch 930 Loss 8.3198 Accuracy 0.0322\n",
            "Epoch 1 Batch 931 Loss 8.3178 Accuracy 0.0322\n",
            "Epoch 1 Batch 932 Loss 8.3156 Accuracy 0.0322\n",
            "Epoch 1 Batch 933 Loss 8.3134 Accuracy 0.0322\n",
            "Epoch 1 Batch 934 Loss 8.3113 Accuracy 0.0322\n",
            "Epoch 1 Batch 935 Loss 8.3097 Accuracy 0.0322\n",
            "Epoch 1 Batch 936 Loss 8.3077 Accuracy 0.0323\n",
            "Epoch 1 Batch 937 Loss 8.3061 Accuracy 0.0323\n",
            "Epoch 1 Batch 938 Loss 8.3040 Accuracy 0.0323\n",
            "Epoch 1 Batch 939 Loss 8.3020 Accuracy 0.0323\n",
            "Epoch 1 Batch 940 Loss 8.3002 Accuracy 0.0323\n",
            "Epoch 1 Batch 941 Loss 8.2980 Accuracy 0.0323\n",
            "Epoch 1 Batch 942 Loss 8.2960 Accuracy 0.0324\n",
            "Epoch 1 Batch 943 Loss 8.2940 Accuracy 0.0324\n",
            "Epoch 1 Batch 944 Loss 8.2918 Accuracy 0.0324\n",
            "Epoch 1 Batch 945 Loss 8.2900 Accuracy 0.0324\n",
            "Epoch 1 Batch 946 Loss 8.2881 Accuracy 0.0324\n",
            "Epoch 1 Batch 947 Loss 8.2861 Accuracy 0.0325\n",
            "Epoch 1 Batch 948 Loss 8.2844 Accuracy 0.0325\n",
            "Epoch 1 Batch 949 Loss 8.2828 Accuracy 0.0325\n",
            "Epoch 1 Batch 950 Loss 8.2808 Accuracy 0.0325\n",
            "Epoch 1 Batch 951 Loss 8.2789 Accuracy 0.0325\n",
            "Epoch 1 Batch 952 Loss 8.2771 Accuracy 0.0325\n",
            "Epoch 1 Batch 953 Loss 8.2754 Accuracy 0.0326\n",
            "Epoch 1 Batch 954 Loss 8.2738 Accuracy 0.0326\n",
            "Epoch 1 Batch 955 Loss 8.2721 Accuracy 0.0326\n",
            "Epoch 1 Batch 956 Loss 8.2702 Accuracy 0.0326\n",
            "Epoch 1 Batch 957 Loss 8.2683 Accuracy 0.0326\n",
            "Epoch 1 Batch 958 Loss 8.2664 Accuracy 0.0327\n",
            "Epoch 1 Batch 959 Loss 8.2645 Accuracy 0.0327\n",
            "Epoch 1 Batch 960 Loss 8.2627 Accuracy 0.0327\n",
            "Epoch 1 Batch 961 Loss 8.2607 Accuracy 0.0327\n",
            "Epoch 1 Batch 962 Loss 8.2588 Accuracy 0.0327\n",
            "Epoch 1 Batch 963 Loss 8.2572 Accuracy 0.0327\n",
            "Epoch 1 Batch 964 Loss 8.2553 Accuracy 0.0328\n",
            "Epoch 1 Batch 965 Loss 8.2536 Accuracy 0.0328\n",
            "Epoch 1 Batch 966 Loss 8.2516 Accuracy 0.0328\n",
            "Epoch 1 Batch 967 Loss 8.2498 Accuracy 0.0328\n",
            "Epoch 1 Batch 968 Loss 8.2479 Accuracy 0.0328\n",
            "Epoch 1 Batch 969 Loss 8.2460 Accuracy 0.0328\n",
            "Epoch 1 Batch 970 Loss 8.2441 Accuracy 0.0329\n",
            "Epoch 1 Batch 971 Loss 8.2422 Accuracy 0.0329\n",
            "Epoch 1 Batch 972 Loss 8.2406 Accuracy 0.0329\n",
            "Epoch 1 Batch 973 Loss 8.2386 Accuracy 0.0329\n",
            "Epoch 1 Batch 974 Loss 8.2368 Accuracy 0.0329\n",
            "Epoch 1 Batch 975 Loss 8.2349 Accuracy 0.0330\n",
            "Epoch 1 Batch 976 Loss 8.2333 Accuracy 0.0330\n",
            "Epoch 1 Batch 977 Loss 8.2316 Accuracy 0.0330\n",
            "Epoch 1 Batch 978 Loss 8.2299 Accuracy 0.0330\n",
            "Epoch 1 Batch 979 Loss 8.2282 Accuracy 0.0330\n",
            "Epoch 1 Batch 980 Loss 8.2263 Accuracy 0.0330\n",
            "Epoch 1 Batch 981 Loss 8.2247 Accuracy 0.0331\n",
            "Epoch 1 Batch 982 Loss 8.2228 Accuracy 0.0331\n",
            "Epoch 1 Batch 983 Loss 8.2209 Accuracy 0.0331\n",
            "Epoch 1 Batch 984 Loss 8.2194 Accuracy 0.0331\n",
            "Epoch 1 Batch 985 Loss 8.2174 Accuracy 0.0331\n",
            "Epoch 1 Batch 986 Loss 8.2158 Accuracy 0.0331\n",
            "Epoch 1 Batch 987 Loss 8.2143 Accuracy 0.0332\n",
            "Epoch 1 Batch 988 Loss 8.2124 Accuracy 0.0332\n",
            "Epoch 1 Batch 989 Loss 8.2106 Accuracy 0.0332\n",
            "Epoch 1 Batch 990 Loss 8.2087 Accuracy 0.0332\n",
            "Epoch 1 Batch 991 Loss 8.2072 Accuracy 0.0332\n",
            "Epoch 1 Batch 992 Loss 8.2056 Accuracy 0.0332\n",
            "Epoch 1 Batch 993 Loss 8.2038 Accuracy 0.0333\n",
            "Epoch 1 Batch 994 Loss 8.2018 Accuracy 0.0333\n",
            "Epoch 1 Batch 995 Loss 8.2002 Accuracy 0.0333\n",
            "Epoch 1 Batch 996 Loss 8.1983 Accuracy 0.0333\n",
            "Epoch 1 Batch 997 Loss 8.1966 Accuracy 0.0333\n",
            "Epoch 1 Batch 998 Loss 8.1949 Accuracy 0.0334\n",
            "Epoch 1 Batch 999 Loss 8.1935 Accuracy 0.0334\n",
            "Epoch 1 Batch 1000 Loss 8.1918 Accuracy 0.0334\n",
            "Epoch 1 Batch 1001 Loss 8.1900 Accuracy 0.0334\n",
            "Epoch 1 Batch 1002 Loss 8.1882 Accuracy 0.0334\n",
            "Epoch 1 Batch 1003 Loss 8.1866 Accuracy 0.0335\n",
            "Epoch 1 Batch 1004 Loss 8.1848 Accuracy 0.0335\n",
            "Epoch 1 Batch 1005 Loss 8.1831 Accuracy 0.0335\n",
            "Epoch 1 Batch 1006 Loss 8.1813 Accuracy 0.0335\n",
            "Epoch 1 Batch 1007 Loss 8.1795 Accuracy 0.0335\n",
            "Epoch 1 Batch 1008 Loss 8.1778 Accuracy 0.0336\n",
            "Epoch 1 Batch 1009 Loss 8.1762 Accuracy 0.0336\n",
            "Epoch 1 Batch 1010 Loss 8.1746 Accuracy 0.0336\n",
            "Epoch 1 Batch 1011 Loss 8.1730 Accuracy 0.0336\n",
            "Epoch 1 Batch 1012 Loss 8.1713 Accuracy 0.0336\n",
            "Epoch 1 Batch 1013 Loss 8.1695 Accuracy 0.0336\n",
            "Epoch 1 Batch 1014 Loss 8.1677 Accuracy 0.0337\n",
            "Epoch 1 Batch 1015 Loss 8.1661 Accuracy 0.0337\n",
            "Epoch 1 Batch 1016 Loss 8.1641 Accuracy 0.0337\n",
            "Epoch 1 Batch 1017 Loss 8.1625 Accuracy 0.0337\n",
            "Epoch 1 Batch 1018 Loss 8.1609 Accuracy 0.0337\n",
            "Epoch 1 Batch 1019 Loss 8.1594 Accuracy 0.0338\n",
            "Epoch 1 Batch 1020 Loss 8.1577 Accuracy 0.0338\n",
            "Epoch 1 Batch 1021 Loss 8.1558 Accuracy 0.0338\n",
            "Epoch 1 Batch 1022 Loss 8.1541 Accuracy 0.0338\n",
            "Epoch 1 Batch 1023 Loss 8.1522 Accuracy 0.0338\n",
            "Epoch 1 Batch 1024 Loss 8.1504 Accuracy 0.0339\n",
            "Epoch 1 Batch 1025 Loss 8.1487 Accuracy 0.0339\n",
            "Epoch 1 Batch 1026 Loss 8.1470 Accuracy 0.0339\n",
            "Epoch 1 Batch 1027 Loss 8.1450 Accuracy 0.0339\n",
            "Epoch 1 Batch 1028 Loss 8.1432 Accuracy 0.0339\n",
            "Epoch 1 Batch 1029 Loss 8.1417 Accuracy 0.0340\n",
            "Epoch 1 Batch 1030 Loss 8.1400 Accuracy 0.0340\n",
            "Epoch 1 Batch 1031 Loss 8.1383 Accuracy 0.0340\n",
            "Epoch 1 Batch 1032 Loss 8.1366 Accuracy 0.0340\n",
            "Epoch 1 Batch 1033 Loss 8.1345 Accuracy 0.0340\n",
            "Epoch 1 Batch 1034 Loss 8.1326 Accuracy 0.0340\n",
            "Epoch 1 Batch 1035 Loss 8.1311 Accuracy 0.0341\n",
            "Epoch 1 Batch 1036 Loss 8.1293 Accuracy 0.0341\n",
            "Epoch 1 Batch 1037 Loss 8.1276 Accuracy 0.0341\n",
            "Epoch 1 Batch 1038 Loss 8.1260 Accuracy 0.0341\n",
            "Epoch 1 Batch 1039 Loss 8.1242 Accuracy 0.0341\n",
            "Epoch 1 Batch 1040 Loss 8.1224 Accuracy 0.0342\n",
            "Epoch 1 Batch 1041 Loss 8.1210 Accuracy 0.0342\n",
            "Epoch 1 Batch 1042 Loss 8.1194 Accuracy 0.0342\n",
            "Epoch 1 Batch 1043 Loss 8.1176 Accuracy 0.0342\n",
            "Epoch 1 Batch 1044 Loss 8.1158 Accuracy 0.0342\n",
            "Epoch 1 Batch 1045 Loss 8.1144 Accuracy 0.0342\n",
            "Epoch 1 Batch 1046 Loss 8.1128 Accuracy 0.0342\n",
            "Epoch 1 Batch 1047 Loss 8.1110 Accuracy 0.0343\n",
            "Epoch 1 Batch 1048 Loss 8.1094 Accuracy 0.0343\n",
            "Epoch 1 Batch 1049 Loss 8.1079 Accuracy 0.0343\n",
            "Epoch 1 Batch 1050 Loss 8.1061 Accuracy 0.0343\n",
            "Epoch 1 Batch 1051 Loss 8.1044 Accuracy 0.0343\n",
            "Epoch 1 Batch 1052 Loss 8.1028 Accuracy 0.0344\n",
            "Epoch 1 Batch 1053 Loss 8.1009 Accuracy 0.0344\n",
            "Epoch 1 Batch 1054 Loss 8.0994 Accuracy 0.0344\n",
            "Epoch 1 Batch 1055 Loss 8.0976 Accuracy 0.0344\n",
            "Epoch 1 Batch 1056 Loss 8.0962 Accuracy 0.0344\n",
            "Epoch 1 Batch 1057 Loss 8.0944 Accuracy 0.0345\n",
            "Epoch 1 Batch 1058 Loss 8.0929 Accuracy 0.0345\n",
            "Epoch 1 Batch 1059 Loss 8.0913 Accuracy 0.0345\n",
            "Epoch 1 Batch 1060 Loss 8.0897 Accuracy 0.0345\n",
            "Epoch 1 Batch 1061 Loss 8.0883 Accuracy 0.0345\n",
            "Epoch 1 Batch 1062 Loss 8.0867 Accuracy 0.0345\n",
            "Epoch 1 Batch 1063 Loss 8.0851 Accuracy 0.0345\n",
            "Epoch 1 Batch 1064 Loss 8.0836 Accuracy 0.0346\n",
            "Epoch 1 Batch 1065 Loss 8.0818 Accuracy 0.0346\n",
            "Epoch 1 Batch 1066 Loss 8.0801 Accuracy 0.0346\n",
            "Epoch 1 Batch 1067 Loss 8.0786 Accuracy 0.0346\n",
            "Epoch 1 Batch 1068 Loss 8.0770 Accuracy 0.0346\n",
            "Epoch 1 Batch 1069 Loss 8.0752 Accuracy 0.0347\n",
            "Epoch 1 Batch 1070 Loss 8.0738 Accuracy 0.0347\n",
            "Epoch 1 Batch 1071 Loss 8.0721 Accuracy 0.0347\n",
            "Epoch 1 Batch 1072 Loss 8.0703 Accuracy 0.0347\n",
            "Epoch 1 Batch 1073 Loss 8.0688 Accuracy 0.0347\n",
            "Epoch 1 Batch 1074 Loss 8.0673 Accuracy 0.0347\n",
            "Epoch 1 Batch 1075 Loss 8.0656 Accuracy 0.0348\n",
            "Epoch 1 Batch 1076 Loss 8.0638 Accuracy 0.0348\n",
            "Epoch 1 Batch 1077 Loss 8.0624 Accuracy 0.0348\n",
            "Epoch 1 Batch 1078 Loss 8.0606 Accuracy 0.0348\n",
            "Epoch 1 Batch 1079 Loss 8.0589 Accuracy 0.0348\n",
            "Epoch 1 Batch 1080 Loss 8.0575 Accuracy 0.0349\n",
            "Epoch 1 Batch 1081 Loss 8.0559 Accuracy 0.0349\n",
            "Epoch 1 Batch 1082 Loss 8.0538 Accuracy 0.0349\n",
            "Epoch 1 Batch 1083 Loss 8.0524 Accuracy 0.0349\n",
            "Epoch 1 Batch 1084 Loss 8.0509 Accuracy 0.0349\n",
            "Epoch 1 Batch 1085 Loss 8.0492 Accuracy 0.0349\n",
            "Epoch 1 Batch 1086 Loss 8.0478 Accuracy 0.0349\n",
            "Epoch 1 Batch 1087 Loss 8.0459 Accuracy 0.0350\n",
            "Epoch 1 Batch 1088 Loss 8.0443 Accuracy 0.0350\n",
            "Epoch 1 Batch 1089 Loss 8.0426 Accuracy 0.0350\n",
            "Epoch 1 Batch 1090 Loss 8.0407 Accuracy 0.0350\n",
            "Epoch 1 Batch 1091 Loss 8.0392 Accuracy 0.0350\n",
            "Epoch 1 Batch 1092 Loss 8.0379 Accuracy 0.0351\n",
            "Epoch 1 Batch 1093 Loss 8.0362 Accuracy 0.0351\n",
            "Epoch 1 Batch 1094 Loss 8.0347 Accuracy 0.0351\n",
            "Epoch 1 Batch 1095 Loss 8.0332 Accuracy 0.0351\n",
            "Epoch 1 Batch 1096 Loss 8.0318 Accuracy 0.0351\n",
            "Epoch 1 Batch 1097 Loss 8.0303 Accuracy 0.0351\n",
            "Epoch 1 Batch 1098 Loss 8.0286 Accuracy 0.0351\n",
            "Epoch 1 Batch 1099 Loss 8.0271 Accuracy 0.0352\n",
            "Epoch 1 Batch 1100 Loss 8.0257 Accuracy 0.0352\n",
            "Epoch 1 Batch 1101 Loss 8.0243 Accuracy 0.0352\n",
            "Epoch 1 Batch 1102 Loss 8.0230 Accuracy 0.0352\n",
            "Epoch 1 Batch 1103 Loss 8.0215 Accuracy 0.0352\n",
            "Epoch 1 Batch 1104 Loss 8.0200 Accuracy 0.0352\n",
            "Epoch 1 Batch 1105 Loss 8.0184 Accuracy 0.0353\n",
            "Epoch 1 Batch 1106 Loss 8.0167 Accuracy 0.0353\n",
            "Epoch 1 Batch 1107 Loss 8.0152 Accuracy 0.0353\n",
            "Epoch 1 Batch 1108 Loss 8.0136 Accuracy 0.0353\n",
            "Epoch 1 Batch 1109 Loss 8.0122 Accuracy 0.0353\n",
            "Epoch 1 Batch 1110 Loss 8.0105 Accuracy 0.0353\n",
            "Epoch 1 Batch 1111 Loss 8.0090 Accuracy 0.0354\n",
            "Epoch 1 Batch 1112 Loss 8.0076 Accuracy 0.0354\n",
            "Epoch 1 Batch 1113 Loss 8.0061 Accuracy 0.0354\n",
            "Epoch 1 Batch 1114 Loss 8.0045 Accuracy 0.0354\n",
            "Epoch 1 Batch 1115 Loss 8.0029 Accuracy 0.0354\n",
            "Epoch 1 Batch 1116 Loss 8.0014 Accuracy 0.0354\n",
            "Epoch 1 Batch 1117 Loss 7.9999 Accuracy 0.0355\n",
            "Epoch 1 Batch 1118 Loss 7.9984 Accuracy 0.0355\n",
            "Epoch 1 Batch 1119 Loss 7.9972 Accuracy 0.0355\n",
            "Epoch 1 Batch 1120 Loss 7.9961 Accuracy 0.0355\n",
            "Epoch 1 Batch 1121 Loss 7.9948 Accuracy 0.0355\n",
            "Epoch 1 Batch 1122 Loss 7.9932 Accuracy 0.0356\n",
            "Epoch 1 Batch 1123 Loss 7.9917 Accuracy 0.0356\n",
            "Epoch 1 Batch 1124 Loss 7.9902 Accuracy 0.0356\n",
            "Epoch 1 Batch 1125 Loss 7.9886 Accuracy 0.0356\n",
            "Epoch 1 Batch 1126 Loss 7.9871 Accuracy 0.0356\n",
            "Epoch 1 Batch 1127 Loss 7.9857 Accuracy 0.0356\n",
            "Epoch 1 Batch 1128 Loss 7.9840 Accuracy 0.0357\n",
            "Epoch 1 Batch 1129 Loss 7.9825 Accuracy 0.0357\n",
            "Epoch 1 Batch 1130 Loss 7.9807 Accuracy 0.0357\n",
            "Epoch 1 Batch 1131 Loss 7.9793 Accuracy 0.0357\n",
            "Epoch 1 Batch 1132 Loss 7.9778 Accuracy 0.0357\n",
            "Epoch 1 Batch 1133 Loss 7.9768 Accuracy 0.0357\n",
            "Epoch 1 Batch 1134 Loss 7.9753 Accuracy 0.0358\n",
            "Epoch 1 Batch 1135 Loss 7.9741 Accuracy 0.0358\n",
            "Epoch 1 Batch 1136 Loss 7.9728 Accuracy 0.0358\n",
            "Epoch 1 Batch 1137 Loss 7.9715 Accuracy 0.0358\n",
            "Epoch 1 Batch 1138 Loss 7.9697 Accuracy 0.0358\n",
            "Epoch 1 Batch 1139 Loss 7.9683 Accuracy 0.0358\n",
            "Epoch 1 Batch 1140 Loss 7.9667 Accuracy 0.0358\n",
            "Epoch 1 Batch 1141 Loss 7.9653 Accuracy 0.0359\n",
            "Epoch 1 Batch 1142 Loss 7.9639 Accuracy 0.0359\n",
            "Epoch 1 Batch 1143 Loss 7.9625 Accuracy 0.0359\n",
            "Epoch 1 Batch 1144 Loss 7.9612 Accuracy 0.0359\n",
            "Epoch 1 Batch 1145 Loss 7.9598 Accuracy 0.0359\n",
            "Epoch 1 Batch 1146 Loss 7.9584 Accuracy 0.0360\n",
            "Epoch 1 Batch 1147 Loss 7.9569 Accuracy 0.0360\n",
            "Epoch 1 Batch 1148 Loss 7.9550 Accuracy 0.0360\n",
            "Epoch 1 Batch 1149 Loss 7.9535 Accuracy 0.0360\n",
            "Epoch 1 Batch 1150 Loss 7.9521 Accuracy 0.0360\n",
            "Epoch 1 Batch 1151 Loss 7.9507 Accuracy 0.0360\n",
            "Epoch 1 Batch 1152 Loss 7.9496 Accuracy 0.0361\n",
            "Epoch 1 Batch 1153 Loss 7.9480 Accuracy 0.0361\n",
            "Epoch 1 Batch 1154 Loss 7.9466 Accuracy 0.0361\n",
            "Epoch 1 Batch 1155 Loss 7.9451 Accuracy 0.0361\n",
            "Epoch 1 Batch 1156 Loss 7.9438 Accuracy 0.0361\n",
            "Epoch 1 Batch 1157 Loss 7.9424 Accuracy 0.0361\n",
            "Epoch 1 Batch 1158 Loss 7.9411 Accuracy 0.0361\n",
            "Epoch 1 Batch 1159 Loss 7.9398 Accuracy 0.0362\n",
            "Epoch 1 Batch 1160 Loss 7.9380 Accuracy 0.0362\n",
            "Epoch 1 Batch 1161 Loss 7.9367 Accuracy 0.0362\n",
            "Epoch 1 Batch 1162 Loss 7.9355 Accuracy 0.0362\n",
            "Epoch 1 Batch 1163 Loss 7.9339 Accuracy 0.0362\n",
            "Epoch 1 Batch 1164 Loss 7.9325 Accuracy 0.0362\n",
            "Epoch 1 Batch 1165 Loss 7.9310 Accuracy 0.0362\n",
            "Epoch 1 Batch 1166 Loss 7.9294 Accuracy 0.0363\n",
            "Epoch 1 Batch 1167 Loss 7.9280 Accuracy 0.0363\n",
            "Epoch 1 Batch 1168 Loss 7.9267 Accuracy 0.0363\n",
            "Epoch 1 Batch 1169 Loss 7.9254 Accuracy 0.0363\n",
            "Epoch 1 Batch 1170 Loss 7.9238 Accuracy 0.0363\n",
            "Epoch 1 Batch 1171 Loss 7.9224 Accuracy 0.0364\n",
            "Epoch 1 Batch 1172 Loss 7.9210 Accuracy 0.0364\n",
            "Epoch 1 Batch 1173 Loss 7.9196 Accuracy 0.0364\n",
            "Epoch 1 Batch 1174 Loss 7.9183 Accuracy 0.0364\n",
            "Epoch 1 Batch 1175 Loss 7.9169 Accuracy 0.0364\n",
            "Epoch 1 Batch 1176 Loss 7.9155 Accuracy 0.0364\n",
            "Epoch 1 Batch 1177 Loss 7.9142 Accuracy 0.0365\n",
            "Epoch 1 Batch 1178 Loss 7.9128 Accuracy 0.0365\n",
            "Epoch 1 Batch 1179 Loss 7.9115 Accuracy 0.0365\n",
            "Epoch 1 Batch 1180 Loss 7.9103 Accuracy 0.0365\n",
            "Epoch 1 Batch 1181 Loss 7.9088 Accuracy 0.0365\n",
            "Epoch 1 Batch 1182 Loss 7.9077 Accuracy 0.0365\n",
            "Epoch 1 Batch 1183 Loss 7.9064 Accuracy 0.0365\n",
            "Epoch 1 Batch 1184 Loss 7.9051 Accuracy 0.0366\n",
            "Epoch 1 Batch 1185 Loss 7.9038 Accuracy 0.0366\n",
            "Epoch 1 Batch 1186 Loss 7.9026 Accuracy 0.0366\n",
            "Epoch 1 Batch 1187 Loss 7.9012 Accuracy 0.0366\n",
            "Epoch 1 Batch 1188 Loss 7.8998 Accuracy 0.0366\n",
            "Epoch 1 Batch 1189 Loss 7.8983 Accuracy 0.0366\n",
            "Epoch 1 Batch 1190 Loss 7.8968 Accuracy 0.0367\n",
            "Epoch 1 Batch 1191 Loss 7.8955 Accuracy 0.0367\n",
            "Epoch 1 Batch 1192 Loss 7.8941 Accuracy 0.0367\n",
            "Epoch 1 Batch 1193 Loss 7.8929 Accuracy 0.0367\n",
            "Epoch 1 Batch 1194 Loss 7.8916 Accuracy 0.0367\n",
            "Epoch 1 Batch 1195 Loss 7.8901 Accuracy 0.0367\n",
            "Epoch 1 Batch 1196 Loss 7.8887 Accuracy 0.0367\n",
            "Epoch 1 Batch 1197 Loss 7.8873 Accuracy 0.0368\n",
            "Epoch 1 Batch 1198 Loss 7.8857 Accuracy 0.0368\n",
            "Epoch 1 Batch 1199 Loss 7.8844 Accuracy 0.0368\n",
            "Epoch 1 Batch 1200 Loss 7.8832 Accuracy 0.0368\n",
            "Epoch 1 Batch 1201 Loss 7.8817 Accuracy 0.0368\n",
            "Epoch 1 Batch 1202 Loss 7.8804 Accuracy 0.0368\n",
            "Epoch 1 Batch 1203 Loss 7.8789 Accuracy 0.0369\n",
            "Epoch 1 Batch 1204 Loss 7.8774 Accuracy 0.0369\n",
            "Epoch 1 Batch 1205 Loss 7.8763 Accuracy 0.0369\n",
            "Epoch 1 Batch 1206 Loss 7.8749 Accuracy 0.0369\n",
            "Epoch 1 Batch 1207 Loss 7.8736 Accuracy 0.0369\n",
            "Epoch 1 Batch 1208 Loss 7.8720 Accuracy 0.0369\n",
            "Epoch 1 Batch 1209 Loss 7.8706 Accuracy 0.0369\n",
            "Epoch 1 Batch 1210 Loss 7.8692 Accuracy 0.0370\n",
            "Epoch 1 Batch 1211 Loss 7.8678 Accuracy 0.0370\n",
            "Epoch 1 Batch 1212 Loss 7.8666 Accuracy 0.0370\n",
            "Epoch 1 Batch 1213 Loss 7.8654 Accuracy 0.0370\n",
            "Epoch 1 Batch 1214 Loss 7.8640 Accuracy 0.0370\n",
            "Epoch 1 Batch 1215 Loss 7.8626 Accuracy 0.0370\n",
            "Epoch 1 Batch 1216 Loss 7.8614 Accuracy 0.0371\n",
            "Epoch 1 Batch 1217 Loss 7.8598 Accuracy 0.0371\n",
            "Epoch 1 Batch 1218 Loss 7.8585 Accuracy 0.0371\n",
            "Epoch 1 Batch 1219 Loss 7.8575 Accuracy 0.0371\n",
            "Epoch 1 Batch 1220 Loss 7.8561 Accuracy 0.0371\n",
            "Epoch 1 Batch 1221 Loss 7.8546 Accuracy 0.0371\n",
            "Epoch 1 Batch 1222 Loss 7.8531 Accuracy 0.0372\n",
            "Epoch 1 Batch 1223 Loss 7.8519 Accuracy 0.0372\n",
            "Epoch 1 Batch 1224 Loss 7.8507 Accuracy 0.0372\n",
            "Epoch 1 Batch 1225 Loss 7.8493 Accuracy 0.0372\n",
            "Epoch 1 Batch 1226 Loss 7.8481 Accuracy 0.0372\n",
            "Epoch 1 Batch 1227 Loss 7.8470 Accuracy 0.0372\n",
            "Epoch 1 Batch 1228 Loss 7.8454 Accuracy 0.0372\n",
            "Epoch 1 Batch 1229 Loss 7.8441 Accuracy 0.0372\n",
            "Epoch 1 Batch 1230 Loss 7.8428 Accuracy 0.0373\n",
            "Epoch 1 Batch 1231 Loss 7.8414 Accuracy 0.0373\n",
            "Epoch 1 Batch 1232 Loss 7.8405 Accuracy 0.0373\n",
            "Epoch 1 Batch 1233 Loss 7.8392 Accuracy 0.0373\n",
            "Epoch 1 Batch 1234 Loss 7.8380 Accuracy 0.0373\n",
            "Epoch 1 Batch 1235 Loss 7.8367 Accuracy 0.0373\n",
            "Epoch 1 Batch 1236 Loss 7.8353 Accuracy 0.0374\n",
            "Epoch 1 Batch 1237 Loss 7.8341 Accuracy 0.0374\n",
            "Epoch 1 Batch 1238 Loss 7.8327 Accuracy 0.0374\n",
            "Epoch 1 Batch 1239 Loss 7.8315 Accuracy 0.0374\n",
            "Epoch 1 Batch 1240 Loss 7.8299 Accuracy 0.0374\n",
            "Epoch 1 Batch 1241 Loss 7.8287 Accuracy 0.0374\n",
            "Epoch 1 Batch 1242 Loss 7.8273 Accuracy 0.0374\n",
            "Epoch 1 Batch 1243 Loss 7.8261 Accuracy 0.0375\n",
            "Epoch 1 Batch 1244 Loss 7.8250 Accuracy 0.0375\n",
            "Epoch 1 Batch 1245 Loss 7.8237 Accuracy 0.0375\n",
            "Epoch 1 Batch 1246 Loss 7.8224 Accuracy 0.0375\n",
            "Epoch 1 Batch 1247 Loss 7.8209 Accuracy 0.0375\n",
            "Epoch 1 Batch 1248 Loss 7.8195 Accuracy 0.0375\n",
            "Epoch 1 Batch 1249 Loss 7.8180 Accuracy 0.0376\n",
            "Epoch 1 Batch 1250 Loss 7.8167 Accuracy 0.0376\n",
            "Epoch 1 Batch 1251 Loss 7.8156 Accuracy 0.0376\n",
            "Epoch 1 Batch 1252 Loss 7.8145 Accuracy 0.0376\n",
            "Epoch 1 Batch 1253 Loss 7.8132 Accuracy 0.0376\n",
            "Epoch 1 Batch 1254 Loss 7.8121 Accuracy 0.0376\n",
            "Epoch 1 Batch 1255 Loss 7.8108 Accuracy 0.0377\n",
            "Epoch 1 Batch 1256 Loss 7.8095 Accuracy 0.0377\n",
            "Epoch 1 Batch 1257 Loss 7.8086 Accuracy 0.0377\n",
            "Epoch 1 Batch 1258 Loss 7.8073 Accuracy 0.0377\n",
            "Epoch 1 Batch 1259 Loss 7.8059 Accuracy 0.0377\n",
            "Epoch 1 Batch 1260 Loss 7.8043 Accuracy 0.0377\n",
            "Epoch 1 Batch 1261 Loss 7.8032 Accuracy 0.0377\n",
            "Epoch 1 Batch 1262 Loss 7.8020 Accuracy 0.0378\n",
            "Epoch 1 Batch 1263 Loss 7.8008 Accuracy 0.0378\n",
            "Epoch 1 Batch 1264 Loss 7.7994 Accuracy 0.0378\n",
            "Epoch 1 Batch 1265 Loss 7.7982 Accuracy 0.0378\n",
            "Epoch 1 Batch 1266 Loss 7.7970 Accuracy 0.0378\n",
            "Epoch 1 Batch 1267 Loss 7.7959 Accuracy 0.0378\n",
            "Epoch 1 Batch 1268 Loss 7.7947 Accuracy 0.0379\n",
            "Epoch 1 Batch 1269 Loss 7.7936 Accuracy 0.0379\n",
            "Epoch 1 Batch 1270 Loss 7.7925 Accuracy 0.0379\n",
            "Epoch 1 Batch 1271 Loss 7.7914 Accuracy 0.0379\n",
            "Epoch 1 Batch 1272 Loss 7.7902 Accuracy 0.0379\n",
            "Epoch 1 Batch 1273 Loss 7.7888 Accuracy 0.0379\n",
            "Epoch 1 Batch 1274 Loss 7.7877 Accuracy 0.0379\n",
            "Epoch 1 Batch 1275 Loss 7.7865 Accuracy 0.0379\n",
            "Epoch 1 Batch 1276 Loss 7.7851 Accuracy 0.0380\n",
            "Epoch 1 Batch 1277 Loss 7.7841 Accuracy 0.0380\n",
            "Epoch 1 Batch 1278 Loss 7.7831 Accuracy 0.0380\n",
            "Epoch 1 Batch 1279 Loss 7.7821 Accuracy 0.0380\n",
            "Epoch 1 Batch 1280 Loss 7.7808 Accuracy 0.0380\n",
            "Epoch 1 Batch 1281 Loss 7.7798 Accuracy 0.0380\n",
            "Epoch 1 Batch 1282 Loss 7.7787 Accuracy 0.0380\n",
            "Epoch 1 Batch 1283 Loss 7.7775 Accuracy 0.0380\n",
            "Epoch 1 Batch 1284 Loss 7.7764 Accuracy 0.0381\n",
            "Epoch 1 Batch 1285 Loss 7.7753 Accuracy 0.0381\n",
            "Epoch 1 Batch 1286 Loss 7.7742 Accuracy 0.0381\n",
            "Epoch 1 Batch 1287 Loss 7.7728 Accuracy 0.0381\n",
            "Epoch 1 Batch 1288 Loss 7.7717 Accuracy 0.0381\n",
            "Epoch 1 Batch 1289 Loss 7.7706 Accuracy 0.0381\n",
            "Epoch 1 Batch 1290 Loss 7.7693 Accuracy 0.0381\n",
            "Epoch 1 Batch 1291 Loss 7.7683 Accuracy 0.0382\n",
            "Epoch 1 Batch 1292 Loss 7.7673 Accuracy 0.0382\n",
            "Epoch 1 Batch 1293 Loss 7.7662 Accuracy 0.0382\n",
            "Epoch 1 Batch 1294 Loss 7.7649 Accuracy 0.0382\n",
            "Epoch 1 Batch 1295 Loss 7.7636 Accuracy 0.0382\n",
            "Epoch 1 Batch 1296 Loss 7.7623 Accuracy 0.0382\n",
            "Epoch 1 Batch 1297 Loss 7.7609 Accuracy 0.0382\n",
            "Epoch 1 Batch 1298 Loss 7.7595 Accuracy 0.0383\n",
            "Epoch 1 Batch 1299 Loss 7.7581 Accuracy 0.0383\n",
            "Epoch 1 Batch 1300 Loss 7.7568 Accuracy 0.0383\n",
            "Epoch 1 Batch 1301 Loss 7.7558 Accuracy 0.0383\n",
            "Epoch 1 Batch 1302 Loss 7.7548 Accuracy 0.0383\n",
            "Epoch 1 Batch 1303 Loss 7.7536 Accuracy 0.0383\n",
            "Epoch 1 Batch 1304 Loss 7.7525 Accuracy 0.0383\n",
            "Epoch 1 Batch 1305 Loss 7.7513 Accuracy 0.0384\n",
            "Epoch 1 Batch 1306 Loss 7.7501 Accuracy 0.0384\n",
            "Epoch 1 Batch 1307 Loss 7.7488 Accuracy 0.0384\n",
            "Epoch 1 Batch 1308 Loss 7.7476 Accuracy 0.0384\n",
            "Epoch 1 Batch 1309 Loss 7.7462 Accuracy 0.0384\n",
            "Epoch 1 Batch 1310 Loss 7.7451 Accuracy 0.0384\n",
            "Epoch 1 Batch 1311 Loss 7.7440 Accuracy 0.0384\n",
            "Epoch 1 Batch 1312 Loss 7.7428 Accuracy 0.0385\n",
            "Epoch 1 Batch 1313 Loss 7.7415 Accuracy 0.0385\n",
            "Epoch 1 Batch 1314 Loss 7.7404 Accuracy 0.0385\n",
            "Epoch 1 Batch 1315 Loss 7.7390 Accuracy 0.0385\n",
            "Epoch 1 Batch 1316 Loss 7.7379 Accuracy 0.0385\n",
            "Epoch 1 Batch 1317 Loss 7.7368 Accuracy 0.0385\n",
            "Epoch 1 Batch 1318 Loss 7.7356 Accuracy 0.0385\n",
            "Epoch 1 Batch 1319 Loss 7.7345 Accuracy 0.0385\n",
            "Epoch 1 Batch 1320 Loss 7.7335 Accuracy 0.0386\n",
            "Epoch 1 Batch 1321 Loss 7.7323 Accuracy 0.0386\n",
            "Epoch 1 Batch 1322 Loss 7.7313 Accuracy 0.0386\n",
            "Epoch 1 Batch 1323 Loss 7.7302 Accuracy 0.0386\n",
            "Epoch 1 Batch 1324 Loss 7.7290 Accuracy 0.0386\n",
            "Epoch 1 Batch 1325 Loss 7.7278 Accuracy 0.0386\n",
            "Epoch 1 Batch 1326 Loss 7.7268 Accuracy 0.0387\n",
            "Epoch 1 Batch 1327 Loss 7.7253 Accuracy 0.0387\n",
            "Epoch 1 Batch 1328 Loss 7.7244 Accuracy 0.0387\n",
            "Epoch 1 Batch 1329 Loss 7.7233 Accuracy 0.0387\n",
            "Epoch 1 Batch 1330 Loss 7.7222 Accuracy 0.0387\n",
            "Epoch 1 Batch 1331 Loss 7.7210 Accuracy 0.0387\n",
            "Epoch 1 Batch 1332 Loss 7.7197 Accuracy 0.0387\n",
            "Epoch 1 Batch 1333 Loss 7.7186 Accuracy 0.0388\n",
            "Epoch 1 Batch 1334 Loss 7.7175 Accuracy 0.0388\n",
            "Epoch 1 Batch 1335 Loss 7.7163 Accuracy 0.0388\n",
            "Epoch 1 Batch 1336 Loss 7.7150 Accuracy 0.0388\n",
            "Epoch 1 Batch 1337 Loss 7.7138 Accuracy 0.0388\n",
            "Epoch 1 Batch 1338 Loss 7.7130 Accuracy 0.0388\n",
            "Epoch 1 Batch 1339 Loss 7.7116 Accuracy 0.0388\n",
            "Epoch 1 Batch 1340 Loss 7.7105 Accuracy 0.0389\n",
            "Epoch 1 Batch 1341 Loss 7.7093 Accuracy 0.0389\n",
            "Epoch 1 Batch 1342 Loss 7.7082 Accuracy 0.0389\n",
            "Epoch 1 Batch 1343 Loss 7.7070 Accuracy 0.0389\n",
            "Epoch 1 Batch 1344 Loss 7.7060 Accuracy 0.0389\n",
            "Epoch 1 Batch 1345 Loss 7.7047 Accuracy 0.0389\n",
            "Epoch 1 Batch 1346 Loss 7.7034 Accuracy 0.0389\n",
            "Epoch 1 Batch 1347 Loss 7.7023 Accuracy 0.0390\n",
            "Epoch 1 Batch 1348 Loss 7.7011 Accuracy 0.0390\n",
            "Epoch 1 Batch 1349 Loss 7.6999 Accuracy 0.0390\n",
            "Epoch 1 Batch 1350 Loss 7.6988 Accuracy 0.0390\n",
            "Epoch 1 Batch 1351 Loss 7.6978 Accuracy 0.0390\n",
            "Epoch 1 Batch 1352 Loss 7.6967 Accuracy 0.0390\n",
            "Epoch 1 Batch 1353 Loss 7.6956 Accuracy 0.0390\n",
            "Epoch 1 Batch 1354 Loss 7.6944 Accuracy 0.0390\n",
            "Epoch 1 Batch 1355 Loss 7.6931 Accuracy 0.0391\n",
            "Epoch 1 Batch 1356 Loss 7.6917 Accuracy 0.0391\n",
            "Epoch 1 Batch 1357 Loss 7.6904 Accuracy 0.0391\n",
            "Epoch 1 Batch 1358 Loss 7.6892 Accuracy 0.0391\n",
            "Epoch 1 Batch 1359 Loss 7.6881 Accuracy 0.0391\n",
            "Epoch 1 Batch 1360 Loss 7.6870 Accuracy 0.0391\n",
            "Epoch 1 Batch 1361 Loss 7.6859 Accuracy 0.0391\n",
            "Epoch 1 Batch 1362 Loss 7.6848 Accuracy 0.0391\n",
            "Epoch 1 Batch 1363 Loss 7.6835 Accuracy 0.0392\n",
            "Epoch 1 Batch 1364 Loss 7.6823 Accuracy 0.0392\n",
            "Epoch 1 Batch 1365 Loss 7.6810 Accuracy 0.0392\n",
            "Epoch 1 Batch 1366 Loss 7.6799 Accuracy 0.0392\n",
            "Epoch 1 Batch 1367 Loss 7.6786 Accuracy 0.0392\n",
            "Epoch 1 Batch 1368 Loss 7.6777 Accuracy 0.0392\n",
            "Epoch 1 Batch 1369 Loss 7.6765 Accuracy 0.0392\n",
            "Epoch 1 Batch 1370 Loss 7.6752 Accuracy 0.0392\n",
            "Epoch 1 Batch 1371 Loss 7.6741 Accuracy 0.0393\n",
            "Epoch 1 Batch 1372 Loss 7.6731 Accuracy 0.0393\n",
            "Epoch 1 Batch 1373 Loss 7.6720 Accuracy 0.0393\n",
            "Epoch 1 Batch 1374 Loss 7.6711 Accuracy 0.0393\n",
            "Epoch 1 Batch 1375 Loss 7.6697 Accuracy 0.0393\n",
            "Epoch 1 Batch 1376 Loss 7.6686 Accuracy 0.0393\n",
            "Epoch 1 Batch 1377 Loss 7.6676 Accuracy 0.0393\n",
            "Epoch 1 Batch 1378 Loss 7.6665 Accuracy 0.0393\n",
            "Epoch 1 Batch 1379 Loss 7.6655 Accuracy 0.0394\n",
            "Epoch 1 Batch 1380 Loss 7.6643 Accuracy 0.0394\n",
            "Epoch 1 Batch 1381 Loss 7.6632 Accuracy 0.0394\n",
            "Epoch 1 Batch 1382 Loss 7.6621 Accuracy 0.0394\n",
            "Epoch 1 Batch 1383 Loss 7.6610 Accuracy 0.0394\n",
            "Epoch 1 Batch 1384 Loss 7.6599 Accuracy 0.0394\n",
            "Epoch 1 Batch 1385 Loss 7.6588 Accuracy 0.0394\n",
            "Epoch 1 Batch 1386 Loss 7.6576 Accuracy 0.0395\n",
            "Epoch 1 Batch 1387 Loss 7.6566 Accuracy 0.0395\n",
            "Epoch 1 Batch 1388 Loss 7.6556 Accuracy 0.0395\n",
            "Epoch 1 Batch 1389 Loss 7.6544 Accuracy 0.0395\n",
            "Epoch 1 Batch 1390 Loss 7.6532 Accuracy 0.0395\n",
            "Epoch 1 Batch 1391 Loss 7.6523 Accuracy 0.0395\n",
            "Epoch 1 Batch 1392 Loss 7.6512 Accuracy 0.0395\n",
            "Epoch 1 Batch 1393 Loss 7.6499 Accuracy 0.0396\n",
            "Epoch 1 Batch 1394 Loss 7.6488 Accuracy 0.0396\n",
            "Epoch 1 Batch 1395 Loss 7.6477 Accuracy 0.0396\n",
            "Epoch 1 Batch 1396 Loss 7.6466 Accuracy 0.0396\n",
            "Epoch 1 Batch 1397 Loss 7.6455 Accuracy 0.0396\n",
            "Epoch 1 Batch 1398 Loss 7.6443 Accuracy 0.0396\n",
            "Epoch 1 Batch 1399 Loss 7.6430 Accuracy 0.0396\n",
            "Epoch 1 Batch 1400 Loss 7.6423 Accuracy 0.0396\n",
            "Epoch 1 Batch 1401 Loss 7.6410 Accuracy 0.0396\n",
            "Epoch 1 Batch 1402 Loss 7.6397 Accuracy 0.0397\n",
            "Epoch 1 Batch 1403 Loss 7.6387 Accuracy 0.0397\n",
            "Epoch 1 Batch 1404 Loss 7.6374 Accuracy 0.0397\n",
            "Epoch 1 Batch 1405 Loss 7.6363 Accuracy 0.0397\n",
            "Epoch 1 Batch 1406 Loss 7.6354 Accuracy 0.0397\n",
            "Epoch 1 Batch 1407 Loss 7.6343 Accuracy 0.0397\n",
            "Epoch 1 Batch 1408 Loss 7.6332 Accuracy 0.0397\n",
            "Epoch 1 Batch 1409 Loss 7.6321 Accuracy 0.0397\n",
            "Epoch 1 Batch 1410 Loss 7.6308 Accuracy 0.0398\n",
            "Epoch 1 Batch 1411 Loss 7.6297 Accuracy 0.0398\n",
            "Epoch 1 Batch 1412 Loss 7.6284 Accuracy 0.0398\n",
            "Epoch 1 Batch 1413 Loss 7.6272 Accuracy 0.0398\n",
            "Epoch 1 Batch 1414 Loss 7.6262 Accuracy 0.0398\n",
            "Epoch 1 Batch 1415 Loss 7.6253 Accuracy 0.0398\n",
            "Epoch 1 Batch 1416 Loss 7.6244 Accuracy 0.0398\n",
            "Epoch 1 Batch 1417 Loss 7.6234 Accuracy 0.0398\n",
            "Epoch 1 Batch 1418 Loss 7.6224 Accuracy 0.0398\n",
            "Epoch 1 Batch 1419 Loss 7.6215 Accuracy 0.0399\n",
            "Epoch 1 Batch 1420 Loss 7.6202 Accuracy 0.0399\n",
            "Epoch 1 Batch 1421 Loss 7.6191 Accuracy 0.0399\n",
            "Epoch 1 Batch 1422 Loss 7.6181 Accuracy 0.0399\n",
            "Epoch 1 Batch 1423 Loss 7.6172 Accuracy 0.0399\n",
            "Epoch 1 Batch 1424 Loss 7.6160 Accuracy 0.0399\n",
            "Epoch 1 Batch 1425 Loss 7.6150 Accuracy 0.0400\n",
            "Epoch 1 Batch 1426 Loss 7.6137 Accuracy 0.0400\n",
            "Epoch 1 Batch 1427 Loss 7.6127 Accuracy 0.0400\n",
            "Epoch 1 Batch 1428 Loss 7.6117 Accuracy 0.0400\n",
            "Epoch 1 Batch 1429 Loss 7.6108 Accuracy 0.0400\n",
            "Epoch 1 Batch 1430 Loss 7.6096 Accuracy 0.0400\n",
            "Epoch 1 Batch 1431 Loss 7.6083 Accuracy 0.0400\n",
            "Epoch 1 Batch 1432 Loss 7.6072 Accuracy 0.0400\n",
            "Epoch 1 Batch 1433 Loss 7.6063 Accuracy 0.0400\n",
            "Epoch 1 Batch 1434 Loss 7.6049 Accuracy 0.0401\n",
            "Epoch 1 Batch 1435 Loss 7.6038 Accuracy 0.0401\n",
            "Epoch 1 Batch 1436 Loss 7.6027 Accuracy 0.0401\n",
            "Epoch 1 Batch 1437 Loss 7.6014 Accuracy 0.0401\n",
            "Epoch 1 Batch 1438 Loss 7.6003 Accuracy 0.0401\n",
            "Epoch 1 Batch 1439 Loss 7.5990 Accuracy 0.0401\n",
            "Epoch 1 Batch 1440 Loss 7.5980 Accuracy 0.0402\n",
            "Epoch 1 Batch 1441 Loss 7.5970 Accuracy 0.0402\n",
            "Epoch 1 Batch 1442 Loss 7.5960 Accuracy 0.0402\n",
            "Epoch 1 Batch 1443 Loss 7.5949 Accuracy 0.0402\n",
            "Epoch 1 Batch 1444 Loss 7.5938 Accuracy 0.0402\n",
            "Epoch 1 Batch 1445 Loss 7.5928 Accuracy 0.0402\n",
            "Epoch 1 Batch 1446 Loss 7.5916 Accuracy 0.0402\n",
            "Epoch 1 Batch 1447 Loss 7.5905 Accuracy 0.0402\n",
            "Epoch 1 Batch 1448 Loss 7.5895 Accuracy 0.0402\n",
            "Epoch 1 Batch 1449 Loss 7.5884 Accuracy 0.0403\n",
            "Epoch 1 Batch 1450 Loss 7.5875 Accuracy 0.0403\n",
            "Epoch 1 Batch 1451 Loss 7.5866 Accuracy 0.0403\n",
            "Epoch 1 Batch 1452 Loss 7.5856 Accuracy 0.0403\n",
            "Epoch 1 Batch 1453 Loss 7.5846 Accuracy 0.0403\n",
            "Epoch 1 Batch 1454 Loss 7.5834 Accuracy 0.0403\n",
            "Epoch 1 Batch 1455 Loss 7.5825 Accuracy 0.0403\n",
            "Epoch 1 Batch 1456 Loss 7.5815 Accuracy 0.0403\n",
            "Epoch 1 Batch 1457 Loss 7.5807 Accuracy 0.0403\n",
            "Epoch 1 Batch 1458 Loss 7.5795 Accuracy 0.0404\n",
            "Epoch 1 Batch 1459 Loss 7.5784 Accuracy 0.0404\n",
            "Epoch 1 Batch 1460 Loss 7.5776 Accuracy 0.0404\n",
            "Epoch 1 Batch 1461 Loss 7.5766 Accuracy 0.0404\n",
            "Epoch 1 Batch 1462 Loss 7.5757 Accuracy 0.0404\n",
            "Epoch 1 Batch 1463 Loss 7.5747 Accuracy 0.0404\n",
            "Epoch 1 Batch 1464 Loss 7.5738 Accuracy 0.0404\n",
            "Epoch 1 Batch 1465 Loss 7.5729 Accuracy 0.0404\n",
            "Epoch 1 Batch 1466 Loss 7.5719 Accuracy 0.0404\n",
            "Epoch 1 Batch 1467 Loss 7.5708 Accuracy 0.0404\n",
            "Epoch 1 Batch 1468 Loss 7.5697 Accuracy 0.0405\n",
            "Epoch 1 Batch 1469 Loss 7.5689 Accuracy 0.0405\n",
            "Epoch 1 Batch 1470 Loss 7.5678 Accuracy 0.0405\n",
            "Epoch 1 Batch 1471 Loss 7.5669 Accuracy 0.0405\n",
            "Epoch 1 Batch 1472 Loss 7.5660 Accuracy 0.0405\n",
            "Epoch 1 Batch 1473 Loss 7.5650 Accuracy 0.0405\n",
            "Epoch 1 Batch 1474 Loss 7.5640 Accuracy 0.0405\n",
            "Epoch 1 Batch 1475 Loss 7.5629 Accuracy 0.0405\n",
            "Epoch 1 Batch 1476 Loss 7.5619 Accuracy 0.0405\n",
            "Epoch 1 Batch 1477 Loss 7.5609 Accuracy 0.0406\n",
            "Epoch 1 Batch 1478 Loss 7.5601 Accuracy 0.0406\n",
            "Epoch 1 Batch 1479 Loss 7.5589 Accuracy 0.0406\n",
            "Epoch 1 Batch 1480 Loss 7.5580 Accuracy 0.0406\n",
            "Epoch 1 Batch 1481 Loss 7.5570 Accuracy 0.0406\n",
            "Epoch 1 Batch 1482 Loss 7.5560 Accuracy 0.0406\n",
            "Epoch 1 Batch 1483 Loss 7.5551 Accuracy 0.0406\n",
            "Epoch 1 Batch 1484 Loss 7.5541 Accuracy 0.0406\n",
            "Epoch 1 Batch 1485 Loss 7.5531 Accuracy 0.0406\n",
            "Epoch 1 Batch 1486 Loss 7.5521 Accuracy 0.0406\n",
            "Epoch 1 Batch 1487 Loss 7.5510 Accuracy 0.0406\n",
            "Epoch 1 Batch 1488 Loss 7.5498 Accuracy 0.0407\n",
            "Epoch 1 Batch 1489 Loss 7.5490 Accuracy 0.0407\n",
            "Epoch 1 Batch 1490 Loss 7.5481 Accuracy 0.0407\n",
            "Epoch 1 Batch 1491 Loss 7.5471 Accuracy 0.0407\n",
            "Epoch 1 Batch 1492 Loss 7.5464 Accuracy 0.0407\n",
            "Epoch 1 Batch 1493 Loss 7.5454 Accuracy 0.0407\n",
            "Epoch 1 Batch 1494 Loss 7.5446 Accuracy 0.0407\n",
            "Epoch 1 Batch 1495 Loss 7.5436 Accuracy 0.0407\n",
            "Epoch 1 Batch 1496 Loss 7.5426 Accuracy 0.0407\n",
            "Epoch 1 Batch 1497 Loss 7.5416 Accuracy 0.0408\n",
            "Epoch 1 Batch 1498 Loss 7.5408 Accuracy 0.0408\n",
            "Epoch 1 Batch 1499 Loss 7.5399 Accuracy 0.0408\n",
            "Epoch 1 Batch 1500 Loss 7.5390 Accuracy 0.0408\n",
            "Epoch 1 Batch 1501 Loss 7.5380 Accuracy 0.0408\n",
            "Epoch 1 Batch 1502 Loss 7.5369 Accuracy 0.0408\n",
            "Epoch 1 Batch 1503 Loss 7.5360 Accuracy 0.0408\n",
            "Epoch 1 Batch 1504 Loss 7.5350 Accuracy 0.0408\n",
            "Epoch 1 Batch 1505 Loss 7.5343 Accuracy 0.0409\n",
            "Epoch 1 Batch 1506 Loss 7.5333 Accuracy 0.0409\n",
            "Epoch 1 Batch 1507 Loss 7.5321 Accuracy 0.0409\n",
            "Epoch 1 Batch 1508 Loss 7.5314 Accuracy 0.0409\n",
            "Epoch 1 Batch 1509 Loss 7.5306 Accuracy 0.0409\n",
            "Epoch 1 Batch 1510 Loss 7.5296 Accuracy 0.0409\n",
            "Epoch 1 Batch 1511 Loss 7.5287 Accuracy 0.0409\n",
            "Epoch 1 Batch 1512 Loss 7.5278 Accuracy 0.0409\n",
            "Epoch 1 Batch 1513 Loss 7.5268 Accuracy 0.0409\n",
            "Epoch 1 Batch 1514 Loss 7.5258 Accuracy 0.0410\n",
            "Epoch 1 Batch 1515 Loss 7.5248 Accuracy 0.0410\n",
            "Epoch 1 Batch 1516 Loss 7.5238 Accuracy 0.0410\n",
            "Epoch 1 Batch 1517 Loss 7.5229 Accuracy 0.0410\n",
            "Epoch 1 Batch 1518 Loss 7.5219 Accuracy 0.0410\n",
            "Epoch 1 Batch 1519 Loss 7.5210 Accuracy 0.0410\n",
            "Epoch 1 Batch 1520 Loss 7.5200 Accuracy 0.0410\n",
            "Epoch 1 Batch 1521 Loss 7.5190 Accuracy 0.0410\n",
            "Epoch 1 Batch 1522 Loss 7.5178 Accuracy 0.0411\n",
            "Epoch 1 Batch 1523 Loss 7.5168 Accuracy 0.0411\n",
            "Epoch 1 Batch 1524 Loss 7.5158 Accuracy 0.0411\n",
            "Epoch 1 Batch 1525 Loss 7.5147 Accuracy 0.0411\n",
            "Epoch 1 Batch 1526 Loss 7.5139 Accuracy 0.0411\n",
            "Epoch 1 Batch 1527 Loss 7.5131 Accuracy 0.0411\n",
            "Epoch 1 Batch 1528 Loss 7.5121 Accuracy 0.0411\n",
            "Epoch 1 Batch 1529 Loss 7.5111 Accuracy 0.0412\n",
            "Epoch 1 Batch 1530 Loss 7.5102 Accuracy 0.0412\n",
            "Epoch 1 Batch 1531 Loss 7.5094 Accuracy 0.0412\n",
            "Epoch 1 Batch 1532 Loss 7.5085 Accuracy 0.0412\n",
            "Epoch 1 Batch 1533 Loss 7.5075 Accuracy 0.0412\n",
            "Epoch 1 Batch 1534 Loss 7.5064 Accuracy 0.0412\n",
            "Epoch 1 Batch 1535 Loss 7.5053 Accuracy 0.0412\n",
            "Epoch 1 Batch 1536 Loss 7.5043 Accuracy 0.0412\n",
            "Epoch 1 Batch 1537 Loss 7.5035 Accuracy 0.0413\n",
            "Epoch 1 Batch 1538 Loss 7.5026 Accuracy 0.0413\n",
            "Epoch 1 Batch 1539 Loss 7.5018 Accuracy 0.0413\n",
            "Epoch 1 Batch 1540 Loss 7.5008 Accuracy 0.0413\n",
            "Epoch 1 Batch 1541 Loss 7.4999 Accuracy 0.0413\n",
            "Epoch 1 Batch 1542 Loss 7.4990 Accuracy 0.0413\n",
            "Epoch 1 Batch 1543 Loss 7.4981 Accuracy 0.0413\n",
            "Epoch 1 Batch 1544 Loss 7.4971 Accuracy 0.0413\n",
            "Epoch 1 Batch 1545 Loss 7.4964 Accuracy 0.0413\n",
            "Epoch 1 Batch 1546 Loss 7.4953 Accuracy 0.0413\n",
            "Epoch 1 Batch 1547 Loss 7.4945 Accuracy 0.0414\n",
            "Epoch 1 Batch 1548 Loss 7.4935 Accuracy 0.0414\n",
            "Epoch 1 Batch 1549 Loss 7.4923 Accuracy 0.0414\n",
            "Epoch 1 Batch 1550 Loss 7.4911 Accuracy 0.0414\n",
            "Epoch 1 Batch 1551 Loss 7.4903 Accuracy 0.0414\n",
            "Epoch 1 Batch 1552 Loss 7.4892 Accuracy 0.0414\n",
            "Epoch 1 Batch 1553 Loss 7.4884 Accuracy 0.0414\n",
            "Epoch 1 Batch 1554 Loss 7.4874 Accuracy 0.0414\n",
            "Epoch 1 Batch 1555 Loss 7.4864 Accuracy 0.0414\n",
            "Epoch 1 Batch 1556 Loss 7.4855 Accuracy 0.0415\n",
            "Epoch 1 Batch 1557 Loss 7.4846 Accuracy 0.0415\n",
            "Epoch 1 Batch 1558 Loss 7.4837 Accuracy 0.0415\n",
            "Epoch 1 Batch 1559 Loss 7.4827 Accuracy 0.0415\n",
            "Epoch 1 Batch 1560 Loss 7.4814 Accuracy 0.0415\n",
            "Epoch 1 Batch 1561 Loss 7.4804 Accuracy 0.0415\n",
            "Epoch 1 Batch 1562 Loss 7.4795 Accuracy 0.0415\n",
            "Epoch 1 Batch 1563 Loss 7.4787 Accuracy 0.0415\n",
            "Epoch 1 Batch 1564 Loss 7.4778 Accuracy 0.0415\n",
            "Epoch 1 Batch 1565 Loss 7.4769 Accuracy 0.0416\n",
            "Epoch 1 Batch 1566 Loss 7.4760 Accuracy 0.0416\n",
            "Epoch 1 Batch 1567 Loss 7.4750 Accuracy 0.0416\n",
            "Epoch 1 Batch 1568 Loss 7.4741 Accuracy 0.0416\n",
            "Epoch 1 Batch 1569 Loss 7.4732 Accuracy 0.0416\n",
            "Epoch 1 Batch 1570 Loss 7.4723 Accuracy 0.0416\n",
            "Epoch 1 Batch 1571 Loss 7.4712 Accuracy 0.0416\n",
            "Epoch 1 Batch 1572 Loss 7.4702 Accuracy 0.0416\n",
            "Epoch 1 Batch 1573 Loss 7.4692 Accuracy 0.0416\n",
            "Epoch 1 Batch 1574 Loss 7.4681 Accuracy 0.0416\n",
            "Epoch 1 Batch 1575 Loss 7.4674 Accuracy 0.0416\n",
            "Epoch 1 Batch 1576 Loss 7.4665 Accuracy 0.0417\n",
            "Epoch 1 Batch 1577 Loss 7.4657 Accuracy 0.0417\n",
            "Epoch 1 Batch 1578 Loss 7.4648 Accuracy 0.0417\n",
            "Epoch 1 Batch 1579 Loss 7.4639 Accuracy 0.0417\n",
            "Epoch 1 Batch 1580 Loss 7.4630 Accuracy 0.0417\n",
            "Epoch 1 Batch 1581 Loss 7.4621 Accuracy 0.0417\n",
            "Epoch 1 Batch 1582 Loss 7.4613 Accuracy 0.0417\n",
            "Epoch 1 Batch 1583 Loss 7.4604 Accuracy 0.0417\n",
            "Epoch 1 Batch 1584 Loss 7.4596 Accuracy 0.0417\n",
            "Epoch 1 Batch 1585 Loss 7.4586 Accuracy 0.0417\n",
            "Epoch 1 Batch 1586 Loss 7.4577 Accuracy 0.0417\n",
            "Epoch 1 Batch 1587 Loss 7.4568 Accuracy 0.0418\n",
            "Epoch 1 Batch 1588 Loss 7.4558 Accuracy 0.0418\n",
            "Epoch 1 Batch 1589 Loss 7.4550 Accuracy 0.0418\n",
            "Epoch 1 Batch 1590 Loss 7.4541 Accuracy 0.0418\n",
            "Epoch 1 Batch 1591 Loss 7.4533 Accuracy 0.0418\n",
            "Epoch 1 Batch 1592 Loss 7.4524 Accuracy 0.0418\n",
            "Epoch 1 Batch 1593 Loss 7.4514 Accuracy 0.0418\n",
            "Epoch 1 Batch 1594 Loss 7.4507 Accuracy 0.0418\n",
            "Epoch 1 Batch 1595 Loss 7.4497 Accuracy 0.0419\n",
            "Epoch 1 Batch 1596 Loss 7.4486 Accuracy 0.0419\n",
            "Epoch 1 Batch 1597 Loss 7.4476 Accuracy 0.0419\n",
            "Epoch 1 Batch 1598 Loss 7.4467 Accuracy 0.0419\n",
            "Epoch 1 Batch 1599 Loss 7.4458 Accuracy 0.0419\n",
            "Epoch 1 Batch 1600 Loss 7.4449 Accuracy 0.0419\n",
            "Epoch 1 Batch 1601 Loss 7.4438 Accuracy 0.0419\n",
            "Epoch 1 Batch 1602 Loss 7.4430 Accuracy 0.0419\n",
            "Epoch 1 Batch 1603 Loss 7.4421 Accuracy 0.0419\n",
            "Epoch 1 Batch 1604 Loss 7.4412 Accuracy 0.0419\n",
            "Epoch 1 Batch 1605 Loss 7.4402 Accuracy 0.0420\n",
            "Epoch 1 Batch 1606 Loss 7.4393 Accuracy 0.0420\n",
            "Epoch 1 Batch 1607 Loss 7.4383 Accuracy 0.0420\n",
            "Epoch 1 Batch 1608 Loss 7.4375 Accuracy 0.0420\n",
            "Epoch 1 Batch 1609 Loss 7.4365 Accuracy 0.0420\n",
            "Epoch 1 Batch 1610 Loss 7.4355 Accuracy 0.0420\n",
            "Epoch 1 Batch 1611 Loss 7.4344 Accuracy 0.0420\n",
            "Epoch 1 Batch 1612 Loss 7.4335 Accuracy 0.0421\n",
            "Epoch 1 Batch 1613 Loss 7.4326 Accuracy 0.0421\n",
            "Epoch 1 Batch 1614 Loss 7.4317 Accuracy 0.0421\n",
            "Epoch 1 Batch 1615 Loss 7.4309 Accuracy 0.0421\n",
            "Epoch 1 Batch 1616 Loss 7.4300 Accuracy 0.0421\n",
            "Epoch 1 Batch 1617 Loss 7.4290 Accuracy 0.0421\n",
            "Epoch 1 Batch 1618 Loss 7.4282 Accuracy 0.0421\n",
            "Epoch 1 Batch 1619 Loss 7.4274 Accuracy 0.0421\n",
            "Epoch 1 Batch 1620 Loss 7.4264 Accuracy 0.0421\n",
            "Epoch 1 Batch 1621 Loss 7.4255 Accuracy 0.0422\n",
            "Epoch 1 Batch 1622 Loss 7.4249 Accuracy 0.0422\n",
            "Epoch 1 Batch 1623 Loss 7.4241 Accuracy 0.0422\n",
            "Epoch 1 Batch 1624 Loss 7.4232 Accuracy 0.0422\n",
            "Epoch 1 Batch 1625 Loss 7.4223 Accuracy 0.0422\n",
            "Epoch 1 Batch 1626 Loss 7.4212 Accuracy 0.0422\n",
            "Epoch 1 Batch 1627 Loss 7.4203 Accuracy 0.0422\n",
            "Epoch 1 Batch 1628 Loss 7.4194 Accuracy 0.0422\n",
            "Epoch 1 Batch 1629 Loss 7.4186 Accuracy 0.0422\n",
            "Epoch 1 Batch 1630 Loss 7.4177 Accuracy 0.0423\n",
            "Epoch 1 Batch 1631 Loss 7.4168 Accuracy 0.0423\n",
            "Epoch 1 Batch 1632 Loss 7.4160 Accuracy 0.0423\n",
            "Epoch 1 Batch 1633 Loss 7.4153 Accuracy 0.0423\n",
            "Epoch 1 Batch 1634 Loss 7.4144 Accuracy 0.0423\n",
            "Epoch 1 Batch 1635 Loss 7.4135 Accuracy 0.0423\n",
            "Epoch 1 Batch 1636 Loss 7.4123 Accuracy 0.0423\n",
            "Epoch 1 Batch 1637 Loss 7.4115 Accuracy 0.0423\n",
            "Epoch 1 Batch 1638 Loss 7.4107 Accuracy 0.0423\n",
            "Epoch 1 Batch 1639 Loss 7.4098 Accuracy 0.0423\n",
            "Epoch 1 Batch 1640 Loss 7.4088 Accuracy 0.0423\n",
            "Epoch 1 Batch 1641 Loss 7.4079 Accuracy 0.0423\n",
            "Epoch 1 Batch 1642 Loss 7.4070 Accuracy 0.0424\n",
            "Epoch 1 Batch 1643 Loss 7.4063 Accuracy 0.0424\n",
            "Epoch 1 Batch 1644 Loss 7.4055 Accuracy 0.0424\n",
            "Epoch 1 Batch 1645 Loss 7.4046 Accuracy 0.0424\n",
            "Epoch 1 Batch 1646 Loss 7.4037 Accuracy 0.0424\n",
            "Epoch 1 Batch 1647 Loss 7.4030 Accuracy 0.0424\n",
            "Epoch 1 Batch 1648 Loss 7.4023 Accuracy 0.0424\n",
            "Epoch 1 Batch 1649 Loss 7.4014 Accuracy 0.0424\n",
            "Epoch 1 Batch 1650 Loss 7.4006 Accuracy 0.0424\n",
            "Epoch 1 Batch 1651 Loss 7.3997 Accuracy 0.0425\n",
            "Epoch 1 Batch 1652 Loss 7.3988 Accuracy 0.0425\n",
            "Epoch 1 Batch 1653 Loss 7.3980 Accuracy 0.0425\n",
            "Epoch 1 Batch 1654 Loss 7.3972 Accuracy 0.0425\n",
            "Epoch 1 Batch 1655 Loss 7.3964 Accuracy 0.0425\n",
            "Epoch 1 Batch 1656 Loss 7.3957 Accuracy 0.0425\n",
            "Epoch 1 Batch 1657 Loss 7.3949 Accuracy 0.0425\n",
            "Epoch 1 Batch 1658 Loss 7.3942 Accuracy 0.0425\n",
            "Epoch 1 Batch 1659 Loss 7.3933 Accuracy 0.0425\n",
            "Epoch 1 Batch 1660 Loss 7.3925 Accuracy 0.0425\n",
            "Epoch 1 Batch 1661 Loss 7.3917 Accuracy 0.0425\n",
            "Epoch 1 Batch 1662 Loss 7.3910 Accuracy 0.0426\n",
            "Epoch 1 Batch 1663 Loss 7.3900 Accuracy 0.0426\n",
            "Epoch 1 Batch 1664 Loss 7.3892 Accuracy 0.0426\n",
            "Epoch 1 Batch 1665 Loss 7.3884 Accuracy 0.0426\n",
            "Epoch 1 Batch 1666 Loss 7.3876 Accuracy 0.0426\n",
            "Epoch 1 Batch 1667 Loss 7.3868 Accuracy 0.0426\n",
            "Epoch 1 Batch 1668 Loss 7.3860 Accuracy 0.0426\n",
            "Epoch 1 Batch 1669 Loss 7.3852 Accuracy 0.0426\n",
            "Epoch 1 Batch 1670 Loss 7.3844 Accuracy 0.0427\n",
            "Epoch 1 Batch 1671 Loss 7.3835 Accuracy 0.0427\n",
            "Epoch 1 Batch 1672 Loss 7.3827 Accuracy 0.0427\n",
            "Epoch 1 Batch 1673 Loss 7.3817 Accuracy 0.0427\n",
            "Epoch 1 Batch 1674 Loss 7.3810 Accuracy 0.0427\n",
            "Epoch 1 Batch 1675 Loss 7.3802 Accuracy 0.0427\n",
            "Epoch 1 Batch 1676 Loss 7.3794 Accuracy 0.0427\n",
            "Epoch 1 Batch 1677 Loss 7.3785 Accuracy 0.0427\n",
            "Epoch 1 Batch 1678 Loss 7.3779 Accuracy 0.0427\n",
            "Epoch 1 Batch 1679 Loss 7.3771 Accuracy 0.0427\n",
            "Epoch 1 Batch 1680 Loss 7.3761 Accuracy 0.0427\n",
            "Epoch 1 Batch 1681 Loss 7.3754 Accuracy 0.0428\n",
            "Epoch 1 Batch 1682 Loss 7.3746 Accuracy 0.0428\n",
            "Epoch 1 Batch 1683 Loss 7.3739 Accuracy 0.0428\n",
            "Epoch 1 Batch 1684 Loss 7.3731 Accuracy 0.0428\n",
            "Epoch 1 Batch 1685 Loss 7.3722 Accuracy 0.0428\n",
            "Epoch 1 Batch 1686 Loss 7.3714 Accuracy 0.0428\n",
            "Epoch 1 Batch 1687 Loss 7.3705 Accuracy 0.0428\n",
            "Epoch 1 Batch 1688 Loss 7.3699 Accuracy 0.0428\n",
            "Epoch 1 Batch 1689 Loss 7.3691 Accuracy 0.0428\n",
            "Epoch 1 Batch 1690 Loss 7.3683 Accuracy 0.0428\n",
            "Epoch 1 Batch 1691 Loss 7.3674 Accuracy 0.0428\n",
            "Epoch 1 Batch 1692 Loss 7.3667 Accuracy 0.0429\n",
            "Epoch 1 Batch 1693 Loss 7.3657 Accuracy 0.0429\n",
            "Epoch 1 Batch 1694 Loss 7.3649 Accuracy 0.0429\n",
            "Epoch 1 Batch 1695 Loss 7.3643 Accuracy 0.0429\n",
            "Epoch 1 Batch 1696 Loss 7.3635 Accuracy 0.0429\n",
            "Epoch 1 Batch 1697 Loss 7.3627 Accuracy 0.0429\n",
            "Epoch 1 Batch 1698 Loss 7.3620 Accuracy 0.0429\n",
            "Epoch 1 Batch 1699 Loss 7.3613 Accuracy 0.0429\n",
            "Epoch 1 Batch 1700 Loss 7.3606 Accuracy 0.0429\n",
            "Epoch 1 Batch 1701 Loss 7.3600 Accuracy 0.0429\n",
            "Epoch 1 Batch 1702 Loss 7.3590 Accuracy 0.0429\n",
            "Epoch 1 Batch 1703 Loss 7.3582 Accuracy 0.0430\n",
            "Epoch 1 Batch 1704 Loss 7.3576 Accuracy 0.0430\n",
            "Epoch 1 Batch 1705 Loss 7.3569 Accuracy 0.0430\n",
            "Epoch 1 Batch 1706 Loss 7.3562 Accuracy 0.0430\n",
            "Epoch 1 Batch 1707 Loss 7.3552 Accuracy 0.0430\n",
            "Epoch 1 Batch 1708 Loss 7.3544 Accuracy 0.0430\n",
            "Epoch 1 Batch 1709 Loss 7.3536 Accuracy 0.0430\n",
            "Epoch 1 Batch 1710 Loss 7.3530 Accuracy 0.0430\n",
            "Epoch 1 Batch 1711 Loss 7.3525 Accuracy 0.0430\n",
            "Epoch 1 Batch 1712 Loss 7.3516 Accuracy 0.0431\n",
            "Epoch 1 Batch 1713 Loss 7.3510 Accuracy 0.0431\n",
            "Epoch 1 Batch 1714 Loss 7.3505 Accuracy 0.0431\n",
            "Epoch 1 Batch 1715 Loss 7.3497 Accuracy 0.0431\n",
            "Epoch 1 Batch 1716 Loss 7.3489 Accuracy 0.0431\n",
            "Epoch 1 Batch 1717 Loss 7.3483 Accuracy 0.0431\n",
            "Epoch 1 Batch 1718 Loss 7.3475 Accuracy 0.0431\n",
            "Epoch 1 Batch 1719 Loss 7.3466 Accuracy 0.0431\n",
            "Epoch 1 Batch 1720 Loss 7.3458 Accuracy 0.0432\n",
            "Epoch 1 Batch 1721 Loss 7.3450 Accuracy 0.0432\n",
            "Epoch 1 Batch 1722 Loss 7.3442 Accuracy 0.0432\n",
            "Epoch 1 Batch 1723 Loss 7.3434 Accuracy 0.0432\n",
            "Epoch 1 Batch 1724 Loss 7.3427 Accuracy 0.0432\n",
            "Epoch 1 Batch 1725 Loss 7.3419 Accuracy 0.0432\n",
            "Epoch 1 Batch 1726 Loss 7.3412 Accuracy 0.0432\n",
            "Epoch 1 Batch 1727 Loss 7.3403 Accuracy 0.0432\n",
            "Epoch 1 Batch 1728 Loss 7.3398 Accuracy 0.0432\n",
            "Epoch 1 Batch 1729 Loss 7.3392 Accuracy 0.0432\n",
            "Epoch 1 Batch 1730 Loss 7.3384 Accuracy 0.0433\n",
            "Epoch 1 Batch 1731 Loss 7.3378 Accuracy 0.0433\n",
            "Epoch 1 Batch 1732 Loss 7.3372 Accuracy 0.0433\n",
            "Epoch 1 Batch 1733 Loss 7.3364 Accuracy 0.0433\n",
            "Epoch 1 Batch 1734 Loss 7.3354 Accuracy 0.0433\n",
            "Epoch 1 Batch 1735 Loss 7.3345 Accuracy 0.0433\n",
            "Epoch 1 Batch 1736 Loss 7.3337 Accuracy 0.0433\n",
            "Epoch 1 Batch 1737 Loss 7.3327 Accuracy 0.0433\n",
            "Epoch 1 Batch 1738 Loss 7.3320 Accuracy 0.0433\n",
            "Epoch 1 Batch 1739 Loss 7.3313 Accuracy 0.0433\n",
            "Epoch 1 Batch 1740 Loss 7.3307 Accuracy 0.0433\n",
            "Epoch 1 Batch 1741 Loss 7.3301 Accuracy 0.0433\n",
            "Epoch 1 Batch 1742 Loss 7.3294 Accuracy 0.0434\n",
            "Epoch 1 Batch 1743 Loss 7.3285 Accuracy 0.0434\n",
            "Epoch 1 Batch 1744 Loss 7.3277 Accuracy 0.0434\n",
            "Epoch 1 Batch 1745 Loss 7.3269 Accuracy 0.0434\n",
            "Epoch 1 Batch 1746 Loss 7.3262 Accuracy 0.0434\n",
            "Epoch 1 Batch 1747 Loss 7.3255 Accuracy 0.0434\n",
            "Epoch 1 Batch 1748 Loss 7.3247 Accuracy 0.0434\n",
            "Epoch 1 Batch 1749 Loss 7.3239 Accuracy 0.0434\n",
            "Epoch 1 Batch 1750 Loss 7.3232 Accuracy 0.0434\n",
            "Epoch 1 Batch 1751 Loss 7.3224 Accuracy 0.0434\n",
            "Epoch 1 Batch 1752 Loss 7.3218 Accuracy 0.0435\n",
            "Epoch 1 Batch 1753 Loss 7.3211 Accuracy 0.0435\n",
            "Epoch 1 Batch 1754 Loss 7.3205 Accuracy 0.0435\n",
            "Epoch 1 Batch 1755 Loss 7.3194 Accuracy 0.0435\n",
            "Epoch 1 Batch 1756 Loss 7.3187 Accuracy 0.0435\n",
            "Epoch 1 Batch 1757 Loss 7.3181 Accuracy 0.0435\n",
            "Epoch 1 Batch 1758 Loss 7.3173 Accuracy 0.0435\n",
            "Epoch 1 Batch 1759 Loss 7.3167 Accuracy 0.0435\n",
            "Epoch 1 Batch 1760 Loss 7.3159 Accuracy 0.0435\n",
            "Epoch 1 Batch 1761 Loss 7.3152 Accuracy 0.0435\n",
            "Epoch 1 Batch 1762 Loss 7.3146 Accuracy 0.0435\n",
            "Epoch 1 Batch 1763 Loss 7.3140 Accuracy 0.0435\n",
            "Epoch 1 Batch 1764 Loss 7.3133 Accuracy 0.0436\n",
            "Epoch 1 Batch 1765 Loss 7.3126 Accuracy 0.0436\n",
            "Epoch 1 Batch 1766 Loss 7.3119 Accuracy 0.0436\n",
            "Epoch 1 Batch 1767 Loss 7.3110 Accuracy 0.0436\n",
            "Epoch 1 Batch 1768 Loss 7.3103 Accuracy 0.0436\n",
            "Epoch 1 Batch 1769 Loss 7.3095 Accuracy 0.0436\n",
            "Epoch 1 Batch 1770 Loss 7.3087 Accuracy 0.0436\n",
            "Epoch 1 Batch 1771 Loss 7.3080 Accuracy 0.0436\n",
            "Epoch 1 Batch 1772 Loss 7.3071 Accuracy 0.0436\n",
            "Epoch 1 Batch 1773 Loss 7.3064 Accuracy 0.0436\n",
            "Epoch 1 Batch 1774 Loss 7.3058 Accuracy 0.0437\n",
            "Epoch 1 Batch 1775 Loss 7.3051 Accuracy 0.0437\n",
            "Epoch 1 Batch 1776 Loss 7.3044 Accuracy 0.0437\n",
            "Epoch 1 Batch 1777 Loss 7.3038 Accuracy 0.0437\n",
            "Epoch 1 Batch 1778 Loss 7.3031 Accuracy 0.0437\n",
            "Epoch 1 Batch 1779 Loss 7.3023 Accuracy 0.0437\n",
            "Epoch 1 Batch 1780 Loss 7.3015 Accuracy 0.0437\n",
            "Epoch 1 Batch 1781 Loss 7.3009 Accuracy 0.0437\n",
            "Epoch 1 Batch 1782 Loss 7.3003 Accuracy 0.0437\n",
            "Epoch 1 Batch 1783 Loss 7.2995 Accuracy 0.0437\n",
            "Epoch 1 Batch 1784 Loss 7.2987 Accuracy 0.0437\n",
            "Epoch 1 Batch 1785 Loss 7.2980 Accuracy 0.0437\n",
            "Epoch 1 Batch 1786 Loss 7.2971 Accuracy 0.0438\n",
            "Epoch 1 Batch 1787 Loss 7.2965 Accuracy 0.0438\n",
            "Epoch 1 Batch 1788 Loss 7.2958 Accuracy 0.0438\n",
            "Epoch 1 Batch 1789 Loss 7.2952 Accuracy 0.0438\n",
            "Epoch 1 Batch 1790 Loss 7.2946 Accuracy 0.0438\n",
            "Epoch 1 Batch 1791 Loss 7.2938 Accuracy 0.0438\n",
            "Epoch 1 Batch 1792 Loss 7.2931 Accuracy 0.0438\n",
            "Epoch 1 Batch 1793 Loss 7.2924 Accuracy 0.0438\n",
            "Epoch 1 Batch 1794 Loss 7.2917 Accuracy 0.0438\n",
            "Epoch 1 Batch 1795 Loss 7.2911 Accuracy 0.0438\n",
            "Epoch 1 Batch 1796 Loss 7.2903 Accuracy 0.0439\n",
            "Epoch 1 Batch 1797 Loss 7.2895 Accuracy 0.0439\n",
            "Epoch 1 Batch 1798 Loss 7.2889 Accuracy 0.0439\n",
            "Epoch 1 Batch 1799 Loss 7.2881 Accuracy 0.0439\n",
            "Epoch 1 Batch 1800 Loss 7.2876 Accuracy 0.0439\n",
            "Epoch 1 Batch 1801 Loss 7.2870 Accuracy 0.0439\n",
            "Epoch 1 Batch 1802 Loss 7.2861 Accuracy 0.0439\n",
            "Epoch 1 Batch 1803 Loss 7.2855 Accuracy 0.0439\n",
            "Epoch 1 Batch 1804 Loss 7.2848 Accuracy 0.0439\n",
            "Epoch 1 Batch 1805 Loss 7.2841 Accuracy 0.0440\n",
            "Epoch 1 Batch 1806 Loss 7.2834 Accuracy 0.0440\n",
            "Epoch 1 Batch 1807 Loss 7.2825 Accuracy 0.0440\n",
            "Epoch 1 Batch 1808 Loss 7.2817 Accuracy 0.0440\n",
            "Epoch 1 Batch 1809 Loss 7.2810 Accuracy 0.0440\n",
            "Epoch 1 Batch 1810 Loss 7.2803 Accuracy 0.0440\n",
            "Epoch 1 Batch 1811 Loss 7.2797 Accuracy 0.0440\n",
            "Epoch 1 Batch 1812 Loss 7.2789 Accuracy 0.0440\n",
            "Epoch 1 Batch 1813 Loss 7.2783 Accuracy 0.0440\n",
            "Epoch 1 Batch 1814 Loss 7.2776 Accuracy 0.0440\n",
            "Epoch 1 Batch 1815 Loss 7.2769 Accuracy 0.0440\n",
            "Epoch 1 Batch 1816 Loss 7.2762 Accuracy 0.0441\n",
            "Epoch 1 Batch 1817 Loss 7.2754 Accuracy 0.0441\n",
            "Epoch 1 Batch 1818 Loss 7.2747 Accuracy 0.0441\n",
            "Epoch 1 Batch 1819 Loss 7.2741 Accuracy 0.0441\n",
            "Epoch 1 Batch 1820 Loss 7.2733 Accuracy 0.0441\n",
            "Epoch 1 Batch 1821 Loss 7.2727 Accuracy 0.0441\n",
            "Epoch 1 Batch 1822 Loss 7.2721 Accuracy 0.0441\n",
            "Epoch 1 Batch 1823 Loss 7.2715 Accuracy 0.0441\n",
            "Epoch 1 Batch 1824 Loss 7.2708 Accuracy 0.0441\n",
            "Epoch 1 Batch 1825 Loss 7.2702 Accuracy 0.0441\n",
            "Epoch 1 Batch 1826 Loss 7.2694 Accuracy 0.0441\n",
            "Epoch 1 Batch 1827 Loss 7.2687 Accuracy 0.0442\n",
            "Epoch 1 Batch 1828 Loss 7.2679 Accuracy 0.0442\n",
            "Epoch 1 Batch 1829 Loss 7.2671 Accuracy 0.0442\n",
            "Epoch 1 Batch 1830 Loss 7.2663 Accuracy 0.0442\n",
            "Epoch 1 Batch 1831 Loss 7.2660 Accuracy 0.0442\n",
            "Epoch 1 Batch 1832 Loss 7.2653 Accuracy 0.0442\n",
            "Epoch 1 Batch 1833 Loss 7.2645 Accuracy 0.0442\n",
            "Epoch 1 Batch 1834 Loss 7.2638 Accuracy 0.0442\n",
            "Epoch 1 Batch 1835 Loss 7.2631 Accuracy 0.0442\n",
            "Epoch 1 Batch 1836 Loss 7.2625 Accuracy 0.0442\n",
            "Epoch 1 Batch 1837 Loss 7.2620 Accuracy 0.0442\n",
            "Epoch 1 Batch 1838 Loss 7.2613 Accuracy 0.0442\n",
            "Epoch 1 Batch 1839 Loss 7.2606 Accuracy 0.0443\n",
            "Epoch 1 Batch 1840 Loss 7.2599 Accuracy 0.0443\n",
            "Epoch 1 Batch 1841 Loss 7.2594 Accuracy 0.0443\n",
            "Epoch 1 Batch 1842 Loss 7.2587 Accuracy 0.0443\n",
            "Epoch 1 Batch 1843 Loss 7.2581 Accuracy 0.0443\n",
            "Epoch 1 Batch 1844 Loss 7.2576 Accuracy 0.0443\n",
            "Epoch 1 Batch 1845 Loss 7.2568 Accuracy 0.0443\n",
            "Epoch 1 Batch 1846 Loss 7.2561 Accuracy 0.0443\n",
            "Epoch 1 Batch 1847 Loss 7.2553 Accuracy 0.0443\n",
            "Epoch 1 Batch 1848 Loss 7.2547 Accuracy 0.0443\n",
            "Epoch 1 Batch 1849 Loss 7.2540 Accuracy 0.0443\n",
            "Epoch 1 Batch 1850 Loss 7.2533 Accuracy 0.0443\n",
            "Epoch 1 Batch 1851 Loss 7.2527 Accuracy 0.0444\n",
            "Epoch 1 Batch 1852 Loss 7.2520 Accuracy 0.0444\n",
            "Epoch 1 Batch 1853 Loss 7.2514 Accuracy 0.0444\n",
            "Epoch 1 Batch 1854 Loss 7.2506 Accuracy 0.0444\n",
            "Epoch 1 Batch 1855 Loss 7.2501 Accuracy 0.0444\n",
            "Epoch 1 Batch 1856 Loss 7.2493 Accuracy 0.0444\n",
            "Epoch 1 Batch 1857 Loss 7.2486 Accuracy 0.0444\n",
            "Epoch 1 Batch 1858 Loss 7.2480 Accuracy 0.0444\n",
            "Epoch 1 Batch 1859 Loss 7.2472 Accuracy 0.0444\n",
            "Epoch 1 Batch 1860 Loss 7.2466 Accuracy 0.0444\n",
            "Epoch 1 Batch 1861 Loss 7.2460 Accuracy 0.0445\n",
            "Epoch 1 Batch 1862 Loss 7.2453 Accuracy 0.0445\n",
            "Epoch 1 Batch 1863 Loss 7.2447 Accuracy 0.0445\n",
            "Epoch 1 Batch 1864 Loss 7.2441 Accuracy 0.0445\n",
            "Epoch 1 Batch 1865 Loss 7.2432 Accuracy 0.0445\n",
            "Epoch 1 Batch 1866 Loss 7.2426 Accuracy 0.0445\n",
            "Epoch 1 Batch 1867 Loss 7.2419 Accuracy 0.0445\n",
            "Epoch 1 Batch 1868 Loss 7.2413 Accuracy 0.0445\n",
            "Epoch 1 Batch 1869 Loss 7.2407 Accuracy 0.0445\n",
            "Epoch 1 Batch 1870 Loss 7.2400 Accuracy 0.0445\n",
            "Epoch 1 Batch 1871 Loss 7.2394 Accuracy 0.0445\n",
            "Epoch 1 Batch 1872 Loss 7.2388 Accuracy 0.0446\n",
            "Epoch 1 Batch 1873 Loss 7.2382 Accuracy 0.0446\n",
            "Epoch 1 Batch 1874 Loss 7.2373 Accuracy 0.0446\n",
            "Epoch 1 Batch 1875 Loss 7.2365 Accuracy 0.0446\n",
            "Epoch 1 Batch 1876 Loss 7.2360 Accuracy 0.0446\n",
            "Epoch 1 Batch 1877 Loss 7.2352 Accuracy 0.0446\n",
            "Epoch 1 Batch 1878 Loss 7.2348 Accuracy 0.0446\n",
            "Epoch 1 Batch 1879 Loss 7.2340 Accuracy 0.0446\n",
            "Epoch 1 Batch 1880 Loss 7.2333 Accuracy 0.0446\n",
            "Epoch 1 Batch 1881 Loss 7.2324 Accuracy 0.0446\n",
            "Epoch 1 Batch 1882 Loss 7.2319 Accuracy 0.0446\n",
            "Epoch 1 Batch 1883 Loss 7.2313 Accuracy 0.0446\n",
            "Epoch 1 Batch 1884 Loss 7.2307 Accuracy 0.0447\n",
            "Epoch 1 Batch 1885 Loss 7.2302 Accuracy 0.0447\n",
            "Epoch 1 Batch 1886 Loss 7.2293 Accuracy 0.0447\n",
            "Epoch 1 Batch 1887 Loss 7.2285 Accuracy 0.0447\n",
            "Epoch 1 Batch 1888 Loss 7.2280 Accuracy 0.0447\n",
            "Epoch 1 Batch 1889 Loss 7.2272 Accuracy 0.0447\n",
            "Epoch 1 Batch 1890 Loss 7.2265 Accuracy 0.0447\n",
            "Epoch 1 Batch 1891 Loss 7.2258 Accuracy 0.0447\n",
            "Epoch 1 Batch 1892 Loss 7.2250 Accuracy 0.0447\n",
            "Epoch 1 Batch 1893 Loss 7.2243 Accuracy 0.0447\n",
            "Epoch 1 Batch 1894 Loss 7.2235 Accuracy 0.0447\n",
            "Epoch 1 Batch 1895 Loss 7.2230 Accuracy 0.0448\n",
            "Epoch 1 Batch 1896 Loss 7.2223 Accuracy 0.0448\n",
            "Epoch 1 Batch 1897 Loss 7.2217 Accuracy 0.0448\n",
            "Epoch 1 Batch 1898 Loss 7.2210 Accuracy 0.0448\n",
            "Epoch 1 Batch 1899 Loss 7.2204 Accuracy 0.0448\n",
            "Epoch 1 Batch 1900 Loss 7.2196 Accuracy 0.0448\n",
            "Epoch 1 Batch 1901 Loss 7.2189 Accuracy 0.0448\n",
            "Epoch 1 Batch 1902 Loss 7.2182 Accuracy 0.0448\n",
            "Epoch 1 Batch 1903 Loss 7.2175 Accuracy 0.0448\n",
            "Epoch 1 Batch 1904 Loss 7.2170 Accuracy 0.0448\n",
            "Epoch 1 Batch 1905 Loss 7.2163 Accuracy 0.0448\n",
            "Epoch 1 Batch 1906 Loss 7.2156 Accuracy 0.0448\n",
            "Epoch 1 Batch 1907 Loss 7.2149 Accuracy 0.0448\n",
            "Epoch 1 Batch 1908 Loss 7.2143 Accuracy 0.0449\n",
            "Epoch 1 Batch 1909 Loss 7.2135 Accuracy 0.0449\n",
            "Epoch 1 Batch 1910 Loss 7.2127 Accuracy 0.0449\n",
            "Epoch 1 Batch 1911 Loss 7.2119 Accuracy 0.0449\n",
            "Epoch 1 Batch 1912 Loss 7.2111 Accuracy 0.0449\n",
            "Epoch 1 Batch 1913 Loss 7.2105 Accuracy 0.0449\n",
            "Epoch 1 Batch 1914 Loss 7.2099 Accuracy 0.0449\n",
            "Epoch 1 Batch 1915 Loss 7.2093 Accuracy 0.0449\n",
            "Epoch 1 Batch 1916 Loss 7.2088 Accuracy 0.0449\n",
            "Epoch 1 Batch 1917 Loss 7.2080 Accuracy 0.0449\n",
            "Epoch 1 Batch 1918 Loss 7.2073 Accuracy 0.0449\n",
            "Epoch 1 Batch 1919 Loss 7.2065 Accuracy 0.0449\n",
            "Epoch 1 Batch 1920 Loss 7.2058 Accuracy 0.0449\n",
            "Epoch 1 Batch 1921 Loss 7.2052 Accuracy 0.0449\n",
            "Epoch 1 Batch 1922 Loss 7.2046 Accuracy 0.0450\n",
            "Epoch 1 Batch 1923 Loss 7.2039 Accuracy 0.0450\n",
            "Epoch 1 Batch 1924 Loss 7.2030 Accuracy 0.0450\n",
            "Epoch 1 Batch 1925 Loss 7.2024 Accuracy 0.0450\n",
            "Epoch 1 Batch 1926 Loss 7.2017 Accuracy 0.0450\n",
            "Epoch 1 Batch 1927 Loss 7.2010 Accuracy 0.0450\n",
            "Epoch 1 Batch 1928 Loss 7.2004 Accuracy 0.0450\n",
            "Epoch 1 Batch 1929 Loss 7.1998 Accuracy 0.0450\n",
            "Epoch 1 Batch 1930 Loss 7.1991 Accuracy 0.0450\n",
            "Epoch 1 Batch 1931 Loss 7.1985 Accuracy 0.0450\n",
            "Epoch 1 Batch 1932 Loss 7.1979 Accuracy 0.0450\n",
            "Epoch 1 Batch 1933 Loss 7.1973 Accuracy 0.0450\n",
            "Epoch 1 Batch 1934 Loss 7.1966 Accuracy 0.0450\n",
            "Epoch 1 Batch 1935 Loss 7.1960 Accuracy 0.0451\n",
            "Epoch 1 Batch 1936 Loss 7.1954 Accuracy 0.0451\n",
            "Epoch 1 Batch 1937 Loss 7.1948 Accuracy 0.0451\n",
            "Epoch 1 Batch 1938 Loss 7.1939 Accuracy 0.0451\n",
            "Epoch 1 Batch 1939 Loss 7.1933 Accuracy 0.0451\n",
            "Epoch 1 Batch 1940 Loss 7.1927 Accuracy 0.0451\n",
            "Epoch 1 Batch 1941 Loss 7.1919 Accuracy 0.0451\n",
            "Epoch 1 Batch 1942 Loss 7.1912 Accuracy 0.0451\n",
            "Epoch 1 Batch 1943 Loss 7.1905 Accuracy 0.0451\n",
            "Epoch 1 Batch 1944 Loss 7.1898 Accuracy 0.0451\n",
            "Epoch 1 Batch 1945 Loss 7.1891 Accuracy 0.0451\n",
            "Epoch 1 Batch 1946 Loss 7.1886 Accuracy 0.0452\n",
            "Epoch 1 Batch 1947 Loss 7.1879 Accuracy 0.0452\n",
            "Epoch 1 Batch 1948 Loss 7.1872 Accuracy 0.0452\n",
            "Epoch 1 Batch 1949 Loss 7.1865 Accuracy 0.0452\n",
            "Epoch 1 Batch 1950 Loss 7.1861 Accuracy 0.0452\n",
            "Epoch 1 Batch 1951 Loss 7.1854 Accuracy 0.0452\n",
            "Epoch 1 Batch 1952 Loss 7.1847 Accuracy 0.0452\n",
            "Epoch 1 Batch 1953 Loss 7.1842 Accuracy 0.0452\n",
            "Epoch 1 Batch 1954 Loss 7.1833 Accuracy 0.0452\n",
            "Epoch 1 Batch 1955 Loss 7.1827 Accuracy 0.0452\n",
            "Epoch 1 Batch 1956 Loss 7.1820 Accuracy 0.0452\n",
            "Epoch 1 Batch 1957 Loss 7.1813 Accuracy 0.0452\n",
            "Epoch 1 Batch 1958 Loss 7.1806 Accuracy 0.0453\n",
            "Epoch 1 Batch 1959 Loss 7.1800 Accuracy 0.0453\n",
            "Epoch 1 Batch 1960 Loss 7.1794 Accuracy 0.0453\n",
            "Epoch 1 Batch 1961 Loss 7.1788 Accuracy 0.0453\n",
            "Epoch 1 Batch 1962 Loss 7.1782 Accuracy 0.0453\n",
            "Epoch 1 Batch 1963 Loss 7.1775 Accuracy 0.0453\n",
            "Epoch 1 Batch 1964 Loss 7.1768 Accuracy 0.0453\n",
            "Epoch 1 Batch 1965 Loss 7.1761 Accuracy 0.0453\n",
            "Epoch 1 Batch 1966 Loss 7.1754 Accuracy 0.0453\n",
            "Epoch 1 Batch 1967 Loss 7.1747 Accuracy 0.0453\n",
            "Epoch 1 Batch 1968 Loss 7.1742 Accuracy 0.0453\n",
            "Epoch 1 Batch 1969 Loss 7.1736 Accuracy 0.0454\n",
            "Epoch 1 Batch 1970 Loss 7.1729 Accuracy 0.0454\n",
            "Epoch 1 Batch 1971 Loss 7.1722 Accuracy 0.0454\n",
            "Epoch 1 Batch 1972 Loss 7.1717 Accuracy 0.0454\n",
            "Epoch 1 Batch 1973 Loss 7.1709 Accuracy 0.0454\n",
            "Epoch 1 Batch 1974 Loss 7.1704 Accuracy 0.0454\n",
            "Epoch 1 Batch 1975 Loss 7.1697 Accuracy 0.0454\n",
            "Epoch 1 Batch 1976 Loss 7.1691 Accuracy 0.0454\n",
            "Epoch 1 Batch 1977 Loss 7.1683 Accuracy 0.0454\n",
            "Epoch 1 Batch 1978 Loss 7.1677 Accuracy 0.0454\n",
            "Epoch 1 Batch 1979 Loss 7.1670 Accuracy 0.0454\n",
            "Epoch 1 Batch 1980 Loss 7.1664 Accuracy 0.0454\n",
            "Epoch 1 Batch 1981 Loss 7.1658 Accuracy 0.0454\n",
            "Epoch 1 Batch 1982 Loss 7.1653 Accuracy 0.0454\n",
            "Epoch 1 Batch 1983 Loss 7.1646 Accuracy 0.0455\n",
            "Epoch 1 Batch 1984 Loss 7.1640 Accuracy 0.0455\n",
            "Epoch 1 Batch 1985 Loss 7.1635 Accuracy 0.0455\n",
            "Epoch 1 Batch 1986 Loss 7.1629 Accuracy 0.0455\n",
            "Epoch 1 Batch 1987 Loss 7.1622 Accuracy 0.0455\n",
            "Epoch 1 Batch 1988 Loss 7.1618 Accuracy 0.0455\n",
            "Epoch 1 Batch 1989 Loss 7.1612 Accuracy 0.0455\n",
            "Epoch 1 Batch 1990 Loss 7.1606 Accuracy 0.0455\n",
            "Epoch 1 Batch 1991 Loss 7.1601 Accuracy 0.0455\n",
            "Epoch 1 Batch 1992 Loss 7.1596 Accuracy 0.0455\n",
            "Epoch 1 Batch 1993 Loss 7.1592 Accuracy 0.0455\n",
            "Epoch 1 Batch 1994 Loss 7.1587 Accuracy 0.0455\n",
            "Epoch 1 Batch 1995 Loss 7.1581 Accuracy 0.0455\n",
            "Epoch 1 Batch 1996 Loss 7.1576 Accuracy 0.0455\n",
            "Epoch 1 Batch 1997 Loss 7.1568 Accuracy 0.0456\n",
            "Epoch 1 Batch 1998 Loss 7.1562 Accuracy 0.0456\n",
            "Epoch 1 Batch 1999 Loss 7.1556 Accuracy 0.0456\n",
            "Epoch 1 Batch 2000 Loss 7.1550 Accuracy 0.0456\n",
            "Epoch 1 Batch 2001 Loss 7.1546 Accuracy 0.0456\n",
            "Epoch 1 Batch 2002 Loss 7.1540 Accuracy 0.0456\n",
            "Epoch 1 Batch 2003 Loss 7.1534 Accuracy 0.0456\n",
            "Epoch 1 Batch 2004 Loss 7.1527 Accuracy 0.0456\n",
            "Epoch 1 Batch 2005 Loss 7.1521 Accuracy 0.0456\n",
            "Epoch 1 Batch 2006 Loss 7.1514 Accuracy 0.0456\n",
            "Epoch 1 Batch 2007 Loss 7.1508 Accuracy 0.0456\n",
            "Epoch 1 Batch 2008 Loss 7.1505 Accuracy 0.0456\n",
            "Epoch 1 Batch 2009 Loss 7.1500 Accuracy 0.0457\n",
            "Epoch 1 Batch 2010 Loss 7.1495 Accuracy 0.0457\n",
            "Epoch 1 Batch 2011 Loss 7.1488 Accuracy 0.0457\n",
            "Epoch 1 Batch 2012 Loss 7.1482 Accuracy 0.0457\n",
            "Epoch 1 Batch 2013 Loss 7.1477 Accuracy 0.0457\n",
            "Epoch 1 Batch 2014 Loss 7.1470 Accuracy 0.0457\n",
            "Epoch 1 Batch 2015 Loss 7.1464 Accuracy 0.0457\n",
            "Epoch 1 Batch 2016 Loss 7.1460 Accuracy 0.0457\n",
            "Epoch 1 Batch 2017 Loss 7.1455 Accuracy 0.0457\n",
            "Epoch 1 Batch 2018 Loss 7.1449 Accuracy 0.0457\n",
            "Epoch 1 Batch 2019 Loss 7.1442 Accuracy 0.0457\n",
            "Epoch 1 Batch 2020 Loss 7.1435 Accuracy 0.0457\n",
            "Epoch 1 Batch 2021 Loss 7.1429 Accuracy 0.0457\n",
            "Epoch 1 Batch 2022 Loss 7.1422 Accuracy 0.0458\n",
            "Epoch 1 Batch 2023 Loss 7.1415 Accuracy 0.0458\n",
            "Epoch 1 Batch 2024 Loss 7.1409 Accuracy 0.0458\n",
            "Epoch 1 Batch 2025 Loss 7.1404 Accuracy 0.0458\n",
            "Epoch 1 Batch 2026 Loss 7.1399 Accuracy 0.0458\n",
            "Epoch 1 Batch 2027 Loss 7.1394 Accuracy 0.0458\n",
            "Epoch 1 Batch 2028 Loss 7.1388 Accuracy 0.0458\n",
            "Epoch 1 Batch 2029 Loss 7.1381 Accuracy 0.0458\n",
            "Epoch 1 Batch 2030 Loss 7.1374 Accuracy 0.0458\n",
            "Epoch 1 Batch 2031 Loss 7.1370 Accuracy 0.0458\n",
            "Epoch 1 Batch 2032 Loss 7.1364 Accuracy 0.0458\n",
            "Epoch 1 Batch 2033 Loss 7.1359 Accuracy 0.0458\n",
            "Epoch 1 Batch 2034 Loss 7.1354 Accuracy 0.0458\n",
            "Epoch 1 Batch 2035 Loss 7.1348 Accuracy 0.0458\n",
            "Epoch 1 Batch 2036 Loss 7.1344 Accuracy 0.0458\n",
            "Epoch 1 Batch 2037 Loss 7.1337 Accuracy 0.0458\n",
            "Epoch 1 Batch 2038 Loss 7.1330 Accuracy 0.0459\n",
            "Epoch 1 Batch 2039 Loss 7.1324 Accuracy 0.0459\n",
            "Epoch 1 Batch 2040 Loss 7.1318 Accuracy 0.0459\n",
            "Epoch 1 Batch 2041 Loss 7.1313 Accuracy 0.0459\n",
            "Epoch 1 Batch 2042 Loss 7.1307 Accuracy 0.0459\n",
            "Epoch 1 Batch 2043 Loss 7.1302 Accuracy 0.0459\n",
            "Epoch 1 Batch 2044 Loss 7.1295 Accuracy 0.0459\n",
            "Epoch 1 Batch 2045 Loss 7.1290 Accuracy 0.0459\n",
            "Epoch 1 Batch 2046 Loss 7.1286 Accuracy 0.0459\n",
            "Epoch 1 Batch 2047 Loss 7.1280 Accuracy 0.0459\n",
            "Epoch 1 Batch 2048 Loss 7.1275 Accuracy 0.0459\n",
            "Epoch 1 Batch 2049 Loss 7.1271 Accuracy 0.0459\n",
            "Epoch 1 Batch 2050 Loss 7.1265 Accuracy 0.0459\n",
            "Epoch 1 Batch 2051 Loss 7.1260 Accuracy 0.0459\n",
            "Epoch 1 Batch 2052 Loss 7.1254 Accuracy 0.0459\n",
            "Epoch 1 Batch 2053 Loss 7.1248 Accuracy 0.0459\n",
            "Epoch 1 Batch 2054 Loss 7.1243 Accuracy 0.0460\n",
            "Epoch 1 Batch 2055 Loss 7.1237 Accuracy 0.0460\n",
            "Epoch 1 Batch 2056 Loss 7.1233 Accuracy 0.0460\n",
            "Epoch 1 Batch 2057 Loss 7.1229 Accuracy 0.0460\n",
            "Epoch 1 Batch 2058 Loss 7.1222 Accuracy 0.0460\n",
            "Epoch 1 Batch 2059 Loss 7.1217 Accuracy 0.0460\n",
            "Epoch 1 Batch 2060 Loss 7.1211 Accuracy 0.0460\n",
            "Epoch 1 Batch 2061 Loss 7.1204 Accuracy 0.0460\n",
            "Epoch 1 Batch 2062 Loss 7.1197 Accuracy 0.0460\n",
            "Epoch 1 Batch 2063 Loss 7.1191 Accuracy 0.0460\n",
            "Epoch 1 Batch 2064 Loss 7.1185 Accuracy 0.0460\n",
            "Epoch 1 Batch 2065 Loss 7.1180 Accuracy 0.0460\n",
            "Epoch 1 Batch 2066 Loss 7.1174 Accuracy 0.0460\n",
            "Epoch 1 Batch 2067 Loss 7.1169 Accuracy 0.0460\n",
            "Epoch 1 Batch 2068 Loss 7.1163 Accuracy 0.0461\n",
            "Epoch 1 Batch 2069 Loss 7.1159 Accuracy 0.0461\n",
            "Epoch 1 Batch 2070 Loss 7.1154 Accuracy 0.0461\n",
            "Epoch 1 Batch 2071 Loss 7.1148 Accuracy 0.0461\n",
            "Epoch 1 Batch 2072 Loss 7.1142 Accuracy 0.0461\n",
            "Epoch 1 Batch 2073 Loss 7.1136 Accuracy 0.0461\n",
            "Epoch 1 Batch 2074 Loss 7.1129 Accuracy 0.0461\n",
            "Epoch 1 Batch 2075 Loss 7.1124 Accuracy 0.0461\n",
            "Epoch 1 Batch 2076 Loss 7.1118 Accuracy 0.0461\n",
            "Epoch 1 Batch 2077 Loss 7.1114 Accuracy 0.0461\n",
            "Epoch 1 Batch 2078 Loss 7.1108 Accuracy 0.0461\n",
            "Epoch 1 Batch 2079 Loss 7.1101 Accuracy 0.0461\n",
            "Epoch 1 Batch 2080 Loss 7.1097 Accuracy 0.0461\n",
            "Epoch 1 Batch 2081 Loss 7.1092 Accuracy 0.0462\n",
            "Epoch 1 Batch 2082 Loss 7.1086 Accuracy 0.0462\n",
            "Epoch 1 Batch 2083 Loss 7.1080 Accuracy 0.0462\n",
            "Epoch 1 Batch 2084 Loss 7.1075 Accuracy 0.0462\n",
            "Epoch 1 Batch 2085 Loss 7.1069 Accuracy 0.0462\n",
            "Epoch 1 Batch 2086 Loss 7.1063 Accuracy 0.0462\n",
            "Epoch 1 Batch 2087 Loss 7.1059 Accuracy 0.0462\n",
            "Epoch 1 Batch 2088 Loss 7.1054 Accuracy 0.0462\n",
            "Epoch 1 Batch 2089 Loss 7.1049 Accuracy 0.0462\n",
            "Epoch 1 Batch 2090 Loss 7.1045 Accuracy 0.0462\n",
            "Epoch 1 Batch 2091 Loss 7.1039 Accuracy 0.0462\n",
            "Epoch 1 Batch 2092 Loss 7.1033 Accuracy 0.0462\n",
            "Epoch 1 Batch 2093 Loss 7.1028 Accuracy 0.0462\n",
            "Epoch 1 Batch 2094 Loss 7.1024 Accuracy 0.0462\n",
            "Epoch 1 Batch 2095 Loss 7.1019 Accuracy 0.0462\n",
            "Epoch 1 Batch 2096 Loss 7.1013 Accuracy 0.0463\n",
            "Epoch 1 Batch 2097 Loss 7.1008 Accuracy 0.0463\n",
            "Epoch 1 Batch 2098 Loss 7.1001 Accuracy 0.0463\n",
            "Epoch 1 Batch 2099 Loss 7.0999 Accuracy 0.0463\n",
            "Epoch 1 Batch 2100 Loss 7.0993 Accuracy 0.0463\n",
            "Epoch 1 Batch 2101 Loss 7.0988 Accuracy 0.0463\n",
            "Epoch 1 Batch 2102 Loss 7.0982 Accuracy 0.0463\n",
            "Epoch 1 Batch 2103 Loss 7.0977 Accuracy 0.0463\n",
            "Epoch 1 Batch 2104 Loss 7.0972 Accuracy 0.0463\n",
            "Epoch 1 Batch 2105 Loss 7.0965 Accuracy 0.0463\n",
            "Epoch 1 Batch 2106 Loss 7.0959 Accuracy 0.0463\n",
            "Epoch 1 Batch 2107 Loss 7.0954 Accuracy 0.0463\n",
            "Epoch 1 Batch 2108 Loss 7.0949 Accuracy 0.0463\n",
            "Epoch 1 Batch 2109 Loss 7.0945 Accuracy 0.0463\n",
            "Epoch 1 Batch 2110 Loss 7.0939 Accuracy 0.0463\n",
            "Epoch 1 Batch 2111 Loss 7.0933 Accuracy 0.0464\n",
            "Epoch 1 Batch 2112 Loss 7.0929 Accuracy 0.0464\n",
            "Epoch 1 Batch 2113 Loss 7.0925 Accuracy 0.0464\n",
            "Epoch 1 Batch 2114 Loss 7.0920 Accuracy 0.0464\n",
            "Epoch 1 Batch 2115 Loss 7.0915 Accuracy 0.0464\n",
            "Epoch 1 Batch 2116 Loss 7.0911 Accuracy 0.0464\n",
            "Epoch 1 Batch 2117 Loss 7.0906 Accuracy 0.0464\n",
            "Epoch 1 Batch 2118 Loss 7.0901 Accuracy 0.0464\n",
            "Epoch 1 Batch 2119 Loss 7.0896 Accuracy 0.0464\n",
            "Epoch 1 Batch 2120 Loss 7.0890 Accuracy 0.0464\n",
            "Epoch 1 Batch 2121 Loss 7.0883 Accuracy 0.0464\n",
            "Epoch 1 Batch 2122 Loss 7.0878 Accuracy 0.0464\n",
            "Epoch 1 Batch 2123 Loss 7.0872 Accuracy 0.0464\n",
            "Epoch 1 Batch 2124 Loss 7.0866 Accuracy 0.0464\n",
            "Epoch 1 Batch 2125 Loss 7.0861 Accuracy 0.0465\n",
            "Epoch 1 Batch 2126 Loss 7.0854 Accuracy 0.0465\n",
            "Epoch 1 Batch 2127 Loss 7.0849 Accuracy 0.0465\n",
            "Epoch 1 Batch 2128 Loss 7.0844 Accuracy 0.0465\n",
            "Epoch 1 Batch 2129 Loss 7.0839 Accuracy 0.0465\n",
            "Epoch 1 Batch 2130 Loss 7.0834 Accuracy 0.0465\n",
            "Epoch 1 Batch 2131 Loss 7.0828 Accuracy 0.0465\n",
            "Epoch 1 Batch 2132 Loss 7.0824 Accuracy 0.0465\n",
            "Epoch 1 Batch 2133 Loss 7.0819 Accuracy 0.0465\n",
            "Epoch 1 Batch 2134 Loss 7.0814 Accuracy 0.0465\n",
            "Epoch 1 Batch 2135 Loss 7.0807 Accuracy 0.0465\n",
            "Epoch 1 Batch 2136 Loss 7.0801 Accuracy 0.0465\n",
            "Epoch 1 Batch 2137 Loss 7.0795 Accuracy 0.0465\n",
            "Epoch 1 Batch 2138 Loss 7.0789 Accuracy 0.0466\n",
            "Epoch 1 Batch 2139 Loss 7.0784 Accuracy 0.0466\n",
            "Epoch 1 Batch 2140 Loss 7.0779 Accuracy 0.0466\n",
            "Epoch 1 Batch 2141 Loss 7.0776 Accuracy 0.0466\n",
            "Epoch 1 Batch 2142 Loss 7.0770 Accuracy 0.0466\n",
            "Epoch 1 Batch 2143 Loss 7.0763 Accuracy 0.0466\n",
            "Epoch 1 Batch 2144 Loss 7.0759 Accuracy 0.0466\n",
            "Epoch 1 Batch 2145 Loss 7.0753 Accuracy 0.0466\n",
            "Epoch 1 Batch 2146 Loss 7.0747 Accuracy 0.0466\n",
            "Epoch 1 Batch 2147 Loss 7.0742 Accuracy 0.0466\n",
            "Epoch 1 Batch 2148 Loss 7.0737 Accuracy 0.0466\n",
            "Epoch 1 Batch 2149 Loss 7.0731 Accuracy 0.0466\n",
            "Epoch 1 Batch 2150 Loss 7.0725 Accuracy 0.0466\n",
            "Epoch 1 Batch 2151 Loss 7.0720 Accuracy 0.0466\n",
            "Epoch 1 Batch 2152 Loss 7.0714 Accuracy 0.0466\n",
            "Epoch 1 Batch 2153 Loss 7.0709 Accuracy 0.0467\n",
            "Epoch 1 Batch 2154 Loss 7.0704 Accuracy 0.0467\n",
            "Epoch 1 Batch 2155 Loss 7.0698 Accuracy 0.0467\n",
            "Epoch 1 Batch 2156 Loss 7.0693 Accuracy 0.0467\n",
            "Epoch 1 Batch 2157 Loss 7.0687 Accuracy 0.0467\n",
            "Epoch 1 Batch 2158 Loss 7.0684 Accuracy 0.0467\n",
            "Epoch 1 Batch 2159 Loss 7.0678 Accuracy 0.0467\n",
            "Epoch 1 Batch 2160 Loss 7.0672 Accuracy 0.0467\n",
            "Epoch 1 Batch 2161 Loss 7.0665 Accuracy 0.0467\n",
            "Epoch 1 Batch 2162 Loss 7.0659 Accuracy 0.0467\n",
            "Epoch 1 Batch 2163 Loss 7.0654 Accuracy 0.0467\n",
            "Epoch 1 Batch 2164 Loss 7.0649 Accuracy 0.0467\n",
            "Epoch 1 Batch 2165 Loss 7.0644 Accuracy 0.0467\n",
            "Epoch 1 Batch 2166 Loss 7.0639 Accuracy 0.0467\n",
            "Epoch 1 Batch 2167 Loss 7.0633 Accuracy 0.0467\n",
            "Epoch 1 Batch 2168 Loss 7.0628 Accuracy 0.0468\n",
            "Epoch 1 Batch 2169 Loss 7.0623 Accuracy 0.0468\n",
            "Epoch 1 Batch 2170 Loss 7.0619 Accuracy 0.0468\n",
            "Epoch 1 Batch 2171 Loss 7.0613 Accuracy 0.0468\n",
            "Epoch 1 Batch 2172 Loss 7.0607 Accuracy 0.0468\n",
            "Epoch 1 Batch 2173 Loss 7.0602 Accuracy 0.0468\n",
            "Epoch 1 Batch 2174 Loss 7.0598 Accuracy 0.0468\n",
            "Epoch 1 Batch 2175 Loss 7.0594 Accuracy 0.0468\n",
            "Epoch 1 Batch 2176 Loss 7.0588 Accuracy 0.0468\n",
            "Epoch 1 Batch 2177 Loss 7.0583 Accuracy 0.0468\n",
            "Epoch 1 Batch 2178 Loss 7.0576 Accuracy 0.0468\n",
            "Epoch 1 Batch 2179 Loss 7.0571 Accuracy 0.0468\n",
            "Epoch 1 Batch 2180 Loss 7.0567 Accuracy 0.0468\n",
            "Epoch 1 Batch 2181 Loss 7.0563 Accuracy 0.0468\n",
            "Epoch 1 Batch 2182 Loss 7.0558 Accuracy 0.0469\n",
            "Epoch 1 Batch 2183 Loss 7.0552 Accuracy 0.0469\n",
            "Epoch 1 Batch 2184 Loss 7.0546 Accuracy 0.0469\n",
            "Epoch 1 Batch 2185 Loss 7.0541 Accuracy 0.0469\n",
            "Epoch 1 Batch 2186 Loss 7.0536 Accuracy 0.0469\n",
            "Epoch 1 Batch 2187 Loss 7.0531 Accuracy 0.0469\n",
            "Epoch 1 Batch 2188 Loss 7.0527 Accuracy 0.0469\n",
            "Epoch 1 Batch 2189 Loss 7.0523 Accuracy 0.0469\n",
            "Epoch 1 Batch 2190 Loss 7.0519 Accuracy 0.0469\n",
            "Epoch 1 Batch 2191 Loss 7.0513 Accuracy 0.0469\n",
            "Epoch 1 Batch 2192 Loss 7.0509 Accuracy 0.0469\n",
            "Epoch 1 Batch 2193 Loss 7.0502 Accuracy 0.0469\n",
            "Epoch 1 Batch 2194 Loss 7.0497 Accuracy 0.0469\n",
            "Epoch 1 Batch 2195 Loss 7.0492 Accuracy 0.0469\n",
            "Epoch 1 Batch 2196 Loss 7.0488 Accuracy 0.0469\n",
            "Epoch 1 Batch 2197 Loss 7.0483 Accuracy 0.0469\n",
            "Epoch 1 Batch 2198 Loss 7.0478 Accuracy 0.0470\n",
            "Epoch 1 Batch 2199 Loss 7.0474 Accuracy 0.0470\n",
            "Epoch 1 Batch 2200 Loss 7.0469 Accuracy 0.0470\n",
            "Epoch 1 Batch 2201 Loss 7.0463 Accuracy 0.0470\n",
            "Epoch 1 Batch 2202 Loss 7.0458 Accuracy 0.0470\n",
            "Epoch 1 Batch 2203 Loss 7.0451 Accuracy 0.0470\n",
            "Epoch 1 Batch 2204 Loss 7.0447 Accuracy 0.0470\n",
            "Epoch 1 Batch 2205 Loss 7.0442 Accuracy 0.0470\n",
            "Epoch 1 Batch 2206 Loss 7.0438 Accuracy 0.0470\n",
            "Epoch 1 Batch 2207 Loss 7.0432 Accuracy 0.0470\n",
            "Epoch 1 Batch 2208 Loss 7.0428 Accuracy 0.0470\n",
            "Epoch 1 Batch 2209 Loss 7.0423 Accuracy 0.0470\n",
            "Epoch 1 Batch 2210 Loss 7.0417 Accuracy 0.0470\n",
            "Epoch 1 Batch 2211 Loss 7.0411 Accuracy 0.0470\n",
            "Epoch 1 Batch 2212 Loss 7.0407 Accuracy 0.0470\n",
            "Epoch 1 Batch 2213 Loss 7.0400 Accuracy 0.0470\n",
            "Epoch 1 Batch 2214 Loss 7.0395 Accuracy 0.0471\n",
            "Epoch 1 Batch 2215 Loss 7.0390 Accuracy 0.0471\n",
            "Epoch 1 Batch 2216 Loss 7.0384 Accuracy 0.0471\n",
            "Epoch 1 Batch 2217 Loss 7.0380 Accuracy 0.0471\n",
            "Epoch 1 Batch 2218 Loss 7.0375 Accuracy 0.0471\n",
            "Epoch 1 Batch 2219 Loss 7.0370 Accuracy 0.0471\n",
            "Epoch 1 Batch 2220 Loss 7.0366 Accuracy 0.0471\n",
            "Epoch 1 Batch 2221 Loss 7.0362 Accuracy 0.0471\n",
            "Epoch 1 Batch 2222 Loss 7.0357 Accuracy 0.0471\n",
            "Epoch 1 Batch 2223 Loss 7.0353 Accuracy 0.0471\n",
            "Epoch 1 Batch 2224 Loss 7.0348 Accuracy 0.0471\n",
            "Epoch 1 Batch 2225 Loss 7.0345 Accuracy 0.0471\n",
            "Epoch 1 Batch 2226 Loss 7.0340 Accuracy 0.0471\n",
            "Epoch 1 Batch 2227 Loss 7.0335 Accuracy 0.0471\n",
            "Epoch 1 Batch 2228 Loss 7.0330 Accuracy 0.0471\n",
            "Epoch 1 Batch 2229 Loss 7.0325 Accuracy 0.0471\n",
            "Epoch 1 Batch 2230 Loss 7.0320 Accuracy 0.0471\n",
            "Epoch 1 Batch 2231 Loss 7.0315 Accuracy 0.0471\n",
            "Epoch 1 Batch 2232 Loss 7.0310 Accuracy 0.0472\n",
            "Epoch 1 Batch 2233 Loss 7.0304 Accuracy 0.0472\n",
            "Epoch 1 Batch 2234 Loss 7.0299 Accuracy 0.0472\n",
            "Epoch 1 Batch 2235 Loss 7.0297 Accuracy 0.0472\n",
            "Epoch 1 Batch 2236 Loss 7.0292 Accuracy 0.0472\n",
            "Epoch 1 Batch 2237 Loss 7.0287 Accuracy 0.0472\n",
            "Epoch 1 Batch 2238 Loss 7.0282 Accuracy 0.0472\n",
            "Epoch 1 Batch 2239 Loss 7.0277 Accuracy 0.0472\n",
            "Epoch 1 Batch 2240 Loss 7.0272 Accuracy 0.0472\n",
            "Epoch 1 Batch 2241 Loss 7.0266 Accuracy 0.0472\n",
            "Epoch 1 Batch 2242 Loss 7.0262 Accuracy 0.0472\n",
            "Epoch 1 Batch 2243 Loss 7.0258 Accuracy 0.0472\n",
            "Epoch 1 Batch 2244 Loss 7.0254 Accuracy 0.0472\n",
            "Epoch 1 Batch 2245 Loss 7.0248 Accuracy 0.0472\n",
            "Epoch 1 Batch 2246 Loss 7.0243 Accuracy 0.0472\n",
            "Epoch 1 Batch 2247 Loss 7.0239 Accuracy 0.0473\n",
            "Epoch 1 Batch 2248 Loss 7.0234 Accuracy 0.0473\n",
            "Epoch 1 Batch 2249 Loss 7.0227 Accuracy 0.0473\n",
            "Epoch 1 Batch 2250 Loss 7.0223 Accuracy 0.0473\n",
            "Epoch 1 Batch 2251 Loss 7.0219 Accuracy 0.0473\n",
            "Epoch 1 Batch 2252 Loss 7.0214 Accuracy 0.0473\n",
            "Epoch 1 Batch 2253 Loss 7.0209 Accuracy 0.0473\n",
            "Epoch 1 Batch 2254 Loss 7.0203 Accuracy 0.0473\n",
            "Epoch 1 Batch 2255 Loss 7.0199 Accuracy 0.0473\n",
            "Epoch 1 Batch 2256 Loss 7.0194 Accuracy 0.0473\n",
            "Epoch 1 Batch 2257 Loss 7.0188 Accuracy 0.0473\n",
            "Epoch 1 Batch 2258 Loss 7.0182 Accuracy 0.0473\n",
            "Epoch 1 Batch 2259 Loss 7.0177 Accuracy 0.0473\n",
            "Epoch 1 Batch 2260 Loss 7.0172 Accuracy 0.0473\n",
            "Epoch 1 Batch 2261 Loss 7.0167 Accuracy 0.0473\n",
            "Epoch 1 Batch 2262 Loss 7.0161 Accuracy 0.0473\n",
            "Epoch 1 Batch 2263 Loss 7.0155 Accuracy 0.0473\n",
            "Epoch 1 Batch 2264 Loss 7.0151 Accuracy 0.0474\n",
            "Epoch 1 Batch 2265 Loss 7.0146 Accuracy 0.0474\n",
            "Epoch 1 Batch 2266 Loss 7.0140 Accuracy 0.0474\n",
            "Epoch 1 Batch 2267 Loss 7.0135 Accuracy 0.0474\n",
            "Epoch 1 Batch 2268 Loss 7.0129 Accuracy 0.0474\n",
            "Epoch 1 Batch 2269 Loss 7.0124 Accuracy 0.0474\n",
            "Epoch 1 Batch 2270 Loss 7.0120 Accuracy 0.0474\n",
            "Epoch 1 Batch 2271 Loss 7.0115 Accuracy 0.0474\n",
            "Epoch 1 Batch 2272 Loss 7.0110 Accuracy 0.0474\n",
            "Epoch 1 Batch 2273 Loss 7.0105 Accuracy 0.0474\n",
            "Epoch 1 Batch 2274 Loss 7.0101 Accuracy 0.0474\n",
            "Epoch 1 Batch 2275 Loss 7.0098 Accuracy 0.0474\n",
            "Epoch 1 Batch 2276 Loss 7.0093 Accuracy 0.0474\n",
            "Epoch 1 Batch 2277 Loss 7.0089 Accuracy 0.0474\n",
            "Epoch 1 Batch 2278 Loss 7.0085 Accuracy 0.0474\n",
            "Epoch 1 Batch 2279 Loss 7.0081 Accuracy 0.0475\n",
            "Epoch 1 Batch 2280 Loss 7.0075 Accuracy 0.0475\n",
            "Epoch 1 Batch 2281 Loss 7.0070 Accuracy 0.0475\n",
            "Epoch 1 Batch 2282 Loss 7.0068 Accuracy 0.0475\n",
            "Epoch 1 Batch 2283 Loss 7.0064 Accuracy 0.0475\n",
            "Epoch 1 Batch 2284 Loss 7.0059 Accuracy 0.0475\n",
            "Epoch 1 Batch 2285 Loss 7.0053 Accuracy 0.0475\n",
            "Epoch 1 Batch 2286 Loss 7.0050 Accuracy 0.0475\n",
            "Epoch 1 Batch 2287 Loss 7.0045 Accuracy 0.0475\n",
            "Epoch 1 Batch 2288 Loss 7.0039 Accuracy 0.0475\n",
            "Epoch 1 Batch 2289 Loss 7.0036 Accuracy 0.0475\n",
            "Epoch 1 Batch 2290 Loss 7.0030 Accuracy 0.0475\n",
            "Epoch 1 Batch 2291 Loss 7.0025 Accuracy 0.0475\n",
            "Epoch 1 Batch 2292 Loss 7.0021 Accuracy 0.0475\n",
            "Epoch 1 Batch 2293 Loss 7.0018 Accuracy 0.0475\n",
            "Epoch 1 Batch 2294 Loss 7.0012 Accuracy 0.0476\n",
            "Epoch 1 Batch 2295 Loss 7.0007 Accuracy 0.0476\n",
            "Epoch 1 Batch 2296 Loss 7.0001 Accuracy 0.0476\n",
            "Epoch 1 Batch 2297 Loss 6.9996 Accuracy 0.0476\n",
            "Epoch 1 Batch 2298 Loss 6.9990 Accuracy 0.0476\n",
            "Epoch 1 Batch 2299 Loss 6.9984 Accuracy 0.0476\n",
            "Epoch 1 Batch 2300 Loss 6.9981 Accuracy 0.0476\n",
            "Epoch 1 Batch 2301 Loss 6.9977 Accuracy 0.0476\n",
            "Epoch 1 Batch 2302 Loss 6.9971 Accuracy 0.0476\n",
            "Epoch 1 Batch 2303 Loss 6.9967 Accuracy 0.0476\n",
            "Epoch 1 Batch 2304 Loss 6.9962 Accuracy 0.0476\n",
            "Epoch 1 Batch 2305 Loss 6.9956 Accuracy 0.0476\n",
            "Epoch 1 Batch 2306 Loss 6.9952 Accuracy 0.0476\n",
            "Epoch 1 Batch 2307 Loss 6.9948 Accuracy 0.0476\n",
            "Epoch 1 Batch 2308 Loss 6.9942 Accuracy 0.0476\n",
            "Epoch 1 Batch 2309 Loss 6.9937 Accuracy 0.0476\n",
            "Epoch 1 Batch 2310 Loss 6.9932 Accuracy 0.0476\n",
            "Epoch 1 Batch 2311 Loss 6.9928 Accuracy 0.0476\n",
            "Epoch 1 Batch 2312 Loss 6.9924 Accuracy 0.0476\n",
            "Epoch 1 Batch 2313 Loss 6.9920 Accuracy 0.0477\n",
            "Epoch 1 Batch 2314 Loss 6.9916 Accuracy 0.0477\n",
            "Epoch 1 Batch 2315 Loss 6.9911 Accuracy 0.0477\n",
            "Epoch 1 Batch 2316 Loss 6.9906 Accuracy 0.0477\n",
            "Epoch 1 Batch 2317 Loss 6.9900 Accuracy 0.0477\n",
            "Epoch 1 Batch 2318 Loss 6.9895 Accuracy 0.0477\n",
            "Epoch 1 Batch 2319 Loss 6.9890 Accuracy 0.0477\n",
            "Epoch 1 Batch 2320 Loss 6.9886 Accuracy 0.0477\n",
            "Epoch 1 Batch 2321 Loss 6.9881 Accuracy 0.0477\n",
            "Epoch 1 Batch 2322 Loss 6.9877 Accuracy 0.0477\n",
            "Epoch 1 Batch 2323 Loss 6.9873 Accuracy 0.0477\n",
            "Epoch 1 Batch 2324 Loss 6.9869 Accuracy 0.0477\n",
            "Epoch 1 Batch 2325 Loss 6.9864 Accuracy 0.0477\n",
            "Epoch 1 Batch 2326 Loss 6.9860 Accuracy 0.0477\n",
            "Epoch 1 Batch 2327 Loss 6.9855 Accuracy 0.0477\n",
            "Epoch 1 Batch 2328 Loss 6.9851 Accuracy 0.0477\n",
            "Epoch 1 Batch 2329 Loss 6.9847 Accuracy 0.0477\n",
            "Epoch 1 Batch 2330 Loss 6.9842 Accuracy 0.0477\n",
            "Epoch 1 Batch 2331 Loss 6.9839 Accuracy 0.0478\n",
            "Epoch 1 Batch 2332 Loss 6.9834 Accuracy 0.0478\n",
            "Epoch 1 Batch 2333 Loss 6.9829 Accuracy 0.0478\n",
            "Epoch 1 Batch 2334 Loss 6.9825 Accuracy 0.0478\n",
            "Epoch 1 Batch 2335 Loss 6.9820 Accuracy 0.0478\n",
            "Epoch 1 Batch 2336 Loss 6.9814 Accuracy 0.0478\n",
            "Epoch 1 Batch 2337 Loss 6.9811 Accuracy 0.0478\n",
            "Epoch 1 Batch 2338 Loss 6.9806 Accuracy 0.0478\n",
            "Epoch 1 Batch 2339 Loss 6.9802 Accuracy 0.0478\n",
            "Epoch 1 Batch 2340 Loss 6.9798 Accuracy 0.0478\n",
            "Epoch 1 Batch 2341 Loss 6.9793 Accuracy 0.0478\n",
            "Epoch 1 Batch 2342 Loss 6.9788 Accuracy 0.0478\n",
            "Epoch 1 Batch 2343 Loss 6.9784 Accuracy 0.0478\n",
            "Epoch 1 Batch 2344 Loss 6.9779 Accuracy 0.0478\n",
            "Epoch 1 Batch 2345 Loss 6.9774 Accuracy 0.0478\n",
            "Epoch 1 Batch 2346 Loss 6.9768 Accuracy 0.0478\n",
            "Epoch 1 Batch 2347 Loss 6.9763 Accuracy 0.0478\n",
            "Epoch 1 Batch 2348 Loss 6.9759 Accuracy 0.0478\n",
            "Epoch 1 Batch 2349 Loss 6.9755 Accuracy 0.0479\n",
            "Epoch 1 Batch 2350 Loss 6.9751 Accuracy 0.0479\n",
            "Epoch 1 Batch 2351 Loss 6.9747 Accuracy 0.0479\n",
            "Epoch 1 Batch 2352 Loss 6.9743 Accuracy 0.0479\n",
            "Epoch 1 Batch 2353 Loss 6.9738 Accuracy 0.0479\n",
            "Epoch 1 Batch 2354 Loss 6.9733 Accuracy 0.0479\n",
            "Epoch 1 Batch 2355 Loss 6.9728 Accuracy 0.0479\n",
            "Epoch 1 Batch 2356 Loss 6.9724 Accuracy 0.0479\n",
            "Epoch 1 Batch 2357 Loss 6.9719 Accuracy 0.0479\n",
            "Epoch 1 Batch 2358 Loss 6.9715 Accuracy 0.0479\n",
            "Epoch 1 Batch 2359 Loss 6.9710 Accuracy 0.0479\n",
            "Epoch 1 Batch 2360 Loss 6.9706 Accuracy 0.0479\n",
            "Epoch 1 Batch 2361 Loss 6.9703 Accuracy 0.0479\n",
            "Epoch 1 Batch 2362 Loss 6.9698 Accuracy 0.0479\n",
            "Epoch 1 Batch 2363 Loss 6.9692 Accuracy 0.0479\n",
            "Epoch 1 Batch 2364 Loss 6.9688 Accuracy 0.0479\n",
            "Epoch 1 Batch 2365 Loss 6.9684 Accuracy 0.0479\n",
            "Epoch 1 Batch 2366 Loss 6.9679 Accuracy 0.0480\n",
            "Epoch 1 Batch 2367 Loss 6.9675 Accuracy 0.0480\n",
            "Epoch 1 Batch 2368 Loss 6.9670 Accuracy 0.0480\n",
            "Epoch 1 Batch 2369 Loss 6.9665 Accuracy 0.0480\n",
            "Epoch 1 Batch 2370 Loss 6.9660 Accuracy 0.0480\n",
            "Epoch 1 Batch 2371 Loss 6.9655 Accuracy 0.0480\n",
            "Epoch 1 Batch 2372 Loss 6.9652 Accuracy 0.0480\n",
            "Epoch 1 Batch 2373 Loss 6.9647 Accuracy 0.0480\n",
            "Epoch 1 Batch 2374 Loss 6.9643 Accuracy 0.0480\n",
            "Epoch 1 Batch 2375 Loss 6.9639 Accuracy 0.0480\n",
            "Epoch 1 Batch 2376 Loss 6.9636 Accuracy 0.0480\n",
            "Epoch 1 Batch 2377 Loss 6.9630 Accuracy 0.0480\n",
            "Epoch 1 Batch 2378 Loss 6.9626 Accuracy 0.0480\n",
            "Epoch 1 Batch 2379 Loss 6.9621 Accuracy 0.0480\n",
            "Epoch 1 Batch 2380 Loss 6.9616 Accuracy 0.0480\n",
            "Epoch 1 Batch 2381 Loss 6.9612 Accuracy 0.0480\n",
            "Epoch 1 Batch 2382 Loss 6.9607 Accuracy 0.0480\n",
            "Epoch 1 Batch 2383 Loss 6.9603 Accuracy 0.0481\n",
            "Epoch 1 Batch 2384 Loss 6.9599 Accuracy 0.0481\n",
            "Epoch 1 Batch 2385 Loss 6.9594 Accuracy 0.0481\n",
            "Epoch 1 Batch 2386 Loss 6.9592 Accuracy 0.0481\n",
            "Epoch 1 Batch 2387 Loss 6.9588 Accuracy 0.0481\n",
            "Epoch 1 Batch 2388 Loss 6.9582 Accuracy 0.0481\n",
            "Epoch 1 Batch 2389 Loss 6.9577 Accuracy 0.0481\n",
            "Epoch 1 Batch 2390 Loss 6.9573 Accuracy 0.0481\n",
            "Epoch 1 Batch 2391 Loss 6.9567 Accuracy 0.0481\n",
            "Epoch 1 Batch 2392 Loss 6.9564 Accuracy 0.0481\n",
            "Epoch 1 Batch 2393 Loss 6.9559 Accuracy 0.0481\n",
            "Epoch 1 Batch 2394 Loss 6.9555 Accuracy 0.0481\n",
            "Epoch 1 Batch 2395 Loss 6.9551 Accuracy 0.0481\n",
            "Epoch 1 Batch 2396 Loss 6.9546 Accuracy 0.0481\n",
            "Epoch 1 Batch 2397 Loss 6.9541 Accuracy 0.0481\n",
            "Epoch 1 Batch 2398 Loss 6.9537 Accuracy 0.0481\n",
            "Epoch 1 Batch 2399 Loss 6.9532 Accuracy 0.0481\n",
            "Epoch 1 Batch 2400 Loss 6.9529 Accuracy 0.0482\n",
            "Epoch 1 Batch 2401 Loss 6.9524 Accuracy 0.0482\n",
            "Epoch 1 Batch 2402 Loss 6.9519 Accuracy 0.0482\n",
            "Epoch 1 Batch 2403 Loss 6.9515 Accuracy 0.0482\n",
            "Epoch 1 Batch 2404 Loss 6.9512 Accuracy 0.0482\n",
            "Epoch 1 Batch 2405 Loss 6.9507 Accuracy 0.0482\n",
            "Epoch 1 Batch 2406 Loss 6.9502 Accuracy 0.0482\n",
            "Epoch 1 Batch 2407 Loss 6.9498 Accuracy 0.0482\n",
            "Epoch 1 Batch 2408 Loss 6.9494 Accuracy 0.0482\n",
            "Epoch 1 Batch 2409 Loss 6.9489 Accuracy 0.0482\n",
            "Epoch 1 Batch 2410 Loss 6.9484 Accuracy 0.0482\n",
            "Epoch 1 Batch 2411 Loss 6.9480 Accuracy 0.0482\n",
            "Epoch 1 Batch 2412 Loss 6.9475 Accuracy 0.0482\n",
            "Epoch 1 Batch 2413 Loss 6.9471 Accuracy 0.0482\n",
            "Epoch 1 Batch 2414 Loss 6.9467 Accuracy 0.0482\n",
            "Epoch 1 Batch 2415 Loss 6.9463 Accuracy 0.0482\n",
            "Epoch 1 Batch 2416 Loss 6.9459 Accuracy 0.0482\n",
            "Epoch 1 Batch 2417 Loss 6.9456 Accuracy 0.0482\n",
            "Epoch 1 Batch 2418 Loss 6.9450 Accuracy 0.0483\n",
            "Epoch 1 Batch 2419 Loss 6.9447 Accuracy 0.0483\n",
            "Epoch 1 Batch 2420 Loss 6.9443 Accuracy 0.0483\n",
            "Epoch 1 Batch 2421 Loss 6.9438 Accuracy 0.0483\n",
            "Epoch 1 Batch 2422 Loss 6.9433 Accuracy 0.0483\n",
            "Epoch 1 Batch 2423 Loss 6.9430 Accuracy 0.0483\n",
            "Epoch 1 Batch 2424 Loss 6.9426 Accuracy 0.0483\n",
            "Epoch 1 Batch 2425 Loss 6.9422 Accuracy 0.0483\n",
            "Epoch 1 Batch 2426 Loss 6.9417 Accuracy 0.0483\n",
            "Epoch 1 Batch 2427 Loss 6.9413 Accuracy 0.0483\n",
            "Epoch 1 Batch 2428 Loss 6.9410 Accuracy 0.0483\n",
            "Epoch 1 Batch 2429 Loss 6.9405 Accuracy 0.0483\n",
            "Epoch 1 Batch 2430 Loss 6.9401 Accuracy 0.0483\n",
            "Epoch 1 Batch 2431 Loss 6.9397 Accuracy 0.0483\n",
            "Epoch 1 Batch 2432 Loss 6.9393 Accuracy 0.0483\n",
            "Epoch 1 Batch 2433 Loss 6.9391 Accuracy 0.0483\n",
            "Epoch 1 Batch 2434 Loss 6.9387 Accuracy 0.0483\n",
            "Epoch 1 Batch 2435 Loss 6.9383 Accuracy 0.0483\n",
            "Epoch 1 Batch 2436 Loss 6.9379 Accuracy 0.0484\n",
            "Epoch 1 Batch 2437 Loss 6.9375 Accuracy 0.0484\n",
            "Epoch 1 Batch 2438 Loss 6.9370 Accuracy 0.0484\n",
            "Epoch 1 Batch 2439 Loss 6.9365 Accuracy 0.0484\n",
            "Epoch 1 Batch 2440 Loss 6.9362 Accuracy 0.0484\n",
            "Epoch 1 Batch 2441 Loss 6.9357 Accuracy 0.0484\n",
            "Epoch 1 Batch 2442 Loss 6.9353 Accuracy 0.0484\n",
            "Epoch 1 Batch 2443 Loss 6.9349 Accuracy 0.0484\n",
            "Epoch 1 Batch 2444 Loss 6.9346 Accuracy 0.0484\n",
            "Epoch 1 Batch 2445 Loss 6.9342 Accuracy 0.0484\n",
            "Epoch 1 Batch 2446 Loss 6.9337 Accuracy 0.0484\n",
            "Epoch 1 Batch 2447 Loss 6.9334 Accuracy 0.0484\n",
            "Epoch 1 Batch 2448 Loss 6.9330 Accuracy 0.0484\n",
            "Epoch 1 Batch 2449 Loss 6.9326 Accuracy 0.0484\n",
            "Epoch 1 Batch 2450 Loss 6.9323 Accuracy 0.0484\n",
            "Epoch 1 Batch 2451 Loss 6.9318 Accuracy 0.0484\n",
            "Epoch 1 Batch 2452 Loss 6.9314 Accuracy 0.0484\n",
            "Epoch 1 Batch 2453 Loss 6.9309 Accuracy 0.0484\n",
            "Epoch 1 Batch 2454 Loss 6.9307 Accuracy 0.0484\n",
            "Epoch 1 Batch 2455 Loss 6.9303 Accuracy 0.0484\n",
            "Epoch 1 Batch 2456 Loss 6.9298 Accuracy 0.0484\n",
            "Epoch 1 Batch 2457 Loss 6.9294 Accuracy 0.0485\n",
            "Epoch 1 Batch 2458 Loss 6.9290 Accuracy 0.0485\n",
            "Epoch 1 Batch 2459 Loss 6.9284 Accuracy 0.0485\n",
            "Epoch 1 Batch 2460 Loss 6.9282 Accuracy 0.0485\n",
            "Epoch 1 Batch 2461 Loss 6.9278 Accuracy 0.0485\n",
            "Epoch 1 Batch 2462 Loss 6.9275 Accuracy 0.0485\n",
            "Epoch 1 Batch 2463 Loss 6.9271 Accuracy 0.0485\n",
            "Epoch 1 Batch 2464 Loss 6.9266 Accuracy 0.0485\n",
            "Epoch 1 Batch 2465 Loss 6.9262 Accuracy 0.0485\n",
            "Epoch 1 Batch 2466 Loss 6.9258 Accuracy 0.0485\n",
            "Epoch 1 Batch 2467 Loss 6.9253 Accuracy 0.0485\n",
            "Epoch 1 Batch 2468 Loss 6.9249 Accuracy 0.0485\n",
            "Epoch 1 Batch 2469 Loss 6.9245 Accuracy 0.0485\n",
            "Epoch 1 Batch 2470 Loss 6.9242 Accuracy 0.0485\n",
            "Epoch 1 Batch 2471 Loss 6.9239 Accuracy 0.0485\n",
            "Epoch 1 Batch 2472 Loss 6.9235 Accuracy 0.0485\n",
            "Epoch 1 Batch 2473 Loss 6.9231 Accuracy 0.0486\n",
            "Epoch 1 Batch 2474 Loss 6.9226 Accuracy 0.0486\n",
            "Epoch 1 Batch 2475 Loss 6.9223 Accuracy 0.0486\n",
            "Epoch 1 Batch 2476 Loss 6.9221 Accuracy 0.0486\n",
            "Epoch 1 Batch 2477 Loss 6.9217 Accuracy 0.0486\n",
            "Epoch 1 Batch 2478 Loss 6.9213 Accuracy 0.0486\n",
            "Epoch 1 Batch 2479 Loss 6.9209 Accuracy 0.0486\n",
            "Epoch 1 Batch 2480 Loss 6.9206 Accuracy 0.0486\n",
            "Epoch 1 Batch 2481 Loss 6.9202 Accuracy 0.0486\n",
            "Epoch 1 Batch 2482 Loss 6.9199 Accuracy 0.0486\n",
            "Epoch 1 Batch 2483 Loss 6.9195 Accuracy 0.0486\n",
            "Epoch 1 Batch 2484 Loss 6.9193 Accuracy 0.0486\n",
            "Epoch 1 Batch 2485 Loss 6.9189 Accuracy 0.0486\n",
            "Epoch 1 Batch 2486 Loss 6.9185 Accuracy 0.0486\n",
            "Epoch 1 Batch 2487 Loss 6.9181 Accuracy 0.0486\n",
            "Epoch 1 Batch 2488 Loss 6.9177 Accuracy 0.0486\n",
            "Epoch 1 Batch 2489 Loss 6.9172 Accuracy 0.0486\n",
            "Epoch 1 Batch 2490 Loss 6.9168 Accuracy 0.0486\n",
            "Epoch 1 Batch 2491 Loss 6.9164 Accuracy 0.0486\n",
            "Epoch 1 Batch 2492 Loss 6.9160 Accuracy 0.0487\n",
            "Epoch 1 Batch 2493 Loss 6.9156 Accuracy 0.0487\n",
            "Epoch 1 Batch 2494 Loss 6.9152 Accuracy 0.0487\n",
            "Epoch 1 Batch 2495 Loss 6.9149 Accuracy 0.0487\n",
            "Epoch 1 Batch 2496 Loss 6.9144 Accuracy 0.0487\n",
            "Epoch 1 Batch 2497 Loss 6.9140 Accuracy 0.0487\n",
            "Epoch 1 Batch 2498 Loss 6.9136 Accuracy 0.0487\n",
            "Epoch 1 Batch 2499 Loss 6.9132 Accuracy 0.0487\n",
            "Epoch 1 Batch 2500 Loss 6.9129 Accuracy 0.0487\n",
            "Epoch 1 Batch 2501 Loss 6.9125 Accuracy 0.0487\n",
            "Epoch 1 Batch 2502 Loss 6.9120 Accuracy 0.0487\n",
            "Epoch 1 Batch 2503 Loss 6.9116 Accuracy 0.0487\n",
            "Epoch 1 Batch 2504 Loss 6.9112 Accuracy 0.0487\n",
            "Epoch 1 Batch 2505 Loss 6.9107 Accuracy 0.0487\n",
            "Epoch 1 Batch 2506 Loss 6.9103 Accuracy 0.0487\n",
            "Epoch 1 Batch 2507 Loss 6.9100 Accuracy 0.0487\n",
            "Epoch 1 Batch 2508 Loss 6.9097 Accuracy 0.0488\n",
            "Epoch 1 Batch 2509 Loss 6.9094 Accuracy 0.0488\n",
            "Epoch 1 Batch 2510 Loss 6.9090 Accuracy 0.0488\n",
            "Epoch 1 Batch 2511 Loss 6.9087 Accuracy 0.0488\n",
            "Epoch 1 Batch 2512 Loss 6.9083 Accuracy 0.0488\n",
            "Epoch 1 Batch 2513 Loss 6.9078 Accuracy 0.0488\n",
            "Epoch 1 Batch 2514 Loss 6.9074 Accuracy 0.0488\n",
            "Epoch 1 Batch 2515 Loss 6.9071 Accuracy 0.0488\n",
            "Epoch 1 Batch 2516 Loss 6.9065 Accuracy 0.0488\n",
            "Epoch 1 Batch 2517 Loss 6.9061 Accuracy 0.0488\n",
            "Epoch 1 Batch 2518 Loss 6.9056 Accuracy 0.0488\n",
            "Epoch 1 Batch 2519 Loss 6.9052 Accuracy 0.0488\n",
            "Epoch 1 Batch 2520 Loss 6.9049 Accuracy 0.0488\n",
            "Epoch 1 Batch 2521 Loss 6.9045 Accuracy 0.0488\n",
            "Epoch 1 Batch 2522 Loss 6.9040 Accuracy 0.0488\n",
            "Epoch 1 Batch 2523 Loss 6.9035 Accuracy 0.0488\n",
            "Epoch 1 Batch 2524 Loss 6.9032 Accuracy 0.0488\n",
            "Epoch 1 Batch 2525 Loss 6.9028 Accuracy 0.0489\n",
            "Epoch 1 Batch 2526 Loss 6.9024 Accuracy 0.0489\n",
            "Epoch 1 Batch 2527 Loss 6.9021 Accuracy 0.0489\n",
            "Epoch 1 Batch 2528 Loss 6.9017 Accuracy 0.0489\n",
            "Epoch 1 Batch 2529 Loss 6.9013 Accuracy 0.0489\n",
            "Epoch 1 Batch 2530 Loss 6.9009 Accuracy 0.0489\n",
            "Epoch 1 Batch 2531 Loss 6.9005 Accuracy 0.0489\n",
            "Epoch 1 Batch 2532 Loss 6.9000 Accuracy 0.0489\n",
            "Epoch 1 Batch 2533 Loss 6.8998 Accuracy 0.0489\n",
            "Epoch 1 Batch 2534 Loss 6.8993 Accuracy 0.0489\n",
            "Epoch 1 Batch 2535 Loss 6.8989 Accuracy 0.0489\n",
            "Epoch 1 Batch 2536 Loss 6.8985 Accuracy 0.0489\n",
            "Epoch 1 Batch 2537 Loss 6.8981 Accuracy 0.0489\n",
            "Epoch 1 Batch 2538 Loss 6.8977 Accuracy 0.0489\n",
            "Epoch 1 Batch 2539 Loss 6.8973 Accuracy 0.0489\n",
            "Epoch 1 Batch 2540 Loss 6.8968 Accuracy 0.0489\n",
            "Epoch 1 Batch 2541 Loss 6.8965 Accuracy 0.0489\n",
            "Epoch 1 Batch 2542 Loss 6.8962 Accuracy 0.0489\n",
            "Epoch 1 Batch 2543 Loss 6.8959 Accuracy 0.0489\n",
            "Epoch 1 Batch 2544 Loss 6.8955 Accuracy 0.0489\n",
            "Epoch 1 Batch 2545 Loss 6.8952 Accuracy 0.0490\n",
            "Epoch 1 Batch 2546 Loss 6.8947 Accuracy 0.0490\n",
            "Epoch 1 Batch 2547 Loss 6.8942 Accuracy 0.0490\n",
            "Epoch 1 Batch 2548 Loss 6.8940 Accuracy 0.0490\n",
            "Epoch 1 Batch 2549 Loss 6.8936 Accuracy 0.0490\n",
            "Epoch 1 Batch 2550 Loss 6.8933 Accuracy 0.0490\n",
            "Epoch 1 Batch 2551 Loss 6.8929 Accuracy 0.0490\n",
            "Epoch 1 Batch 2552 Loss 6.8925 Accuracy 0.0490\n",
            "Epoch 1 Batch 2553 Loss 6.8922 Accuracy 0.0490\n",
            "Epoch 1 Batch 2554 Loss 6.8919 Accuracy 0.0490\n",
            "Epoch 1 Batch 2555 Loss 6.8914 Accuracy 0.0490\n",
            "Epoch 1 Batch 2556 Loss 6.8911 Accuracy 0.0490\n",
            "Epoch 1 Batch 2557 Loss 6.8908 Accuracy 0.0490\n",
            "Epoch 1 Batch 2558 Loss 6.8905 Accuracy 0.0490\n",
            "Epoch 1 Batch 2559 Loss 6.8900 Accuracy 0.0490\n",
            "Epoch 1 Batch 2560 Loss 6.8897 Accuracy 0.0490\n",
            "Epoch 1 Batch 2561 Loss 6.8894 Accuracy 0.0490\n",
            "Epoch 1 Batch 2562 Loss 6.8890 Accuracy 0.0490\n",
            "Epoch 1 Batch 2563 Loss 6.8886 Accuracy 0.0490\n",
            "Epoch 1 Batch 2564 Loss 6.8883 Accuracy 0.0490\n",
            "Epoch 1 Batch 2565 Loss 6.8878 Accuracy 0.0491\n",
            "Epoch 1 Batch 2566 Loss 6.8876 Accuracy 0.0491\n",
            "Epoch 1 Batch 2567 Loss 6.8871 Accuracy 0.0491\n",
            "Epoch 1 Batch 2568 Loss 6.8867 Accuracy 0.0491\n",
            "Epoch 1 Batch 2569 Loss 6.8864 Accuracy 0.0491\n",
            "Epoch 1 Batch 2570 Loss 6.8860 Accuracy 0.0491\n",
            "Epoch 1 Batch 2571 Loss 6.8856 Accuracy 0.0491\n",
            "Epoch 1 Batch 2572 Loss 6.8853 Accuracy 0.0491\n",
            "Epoch 1 Batch 2573 Loss 6.8849 Accuracy 0.0491\n",
            "Epoch 1 Batch 2574 Loss 6.8845 Accuracy 0.0491\n",
            "Epoch 1 Batch 2575 Loss 6.8843 Accuracy 0.0491\n",
            "Epoch 1 Batch 2576 Loss 6.8839 Accuracy 0.0491\n",
            "Epoch 1 Batch 2577 Loss 6.8836 Accuracy 0.0491\n",
            "Epoch 1 Batch 2578 Loss 6.8832 Accuracy 0.0491\n",
            "Epoch 1 Batch 2579 Loss 6.8829 Accuracy 0.0491\n",
            "Epoch 1 Batch 2580 Loss 6.8825 Accuracy 0.0492\n",
            "Epoch 1 Batch 2581 Loss 6.8823 Accuracy 0.0492\n",
            "Epoch 1 Batch 2582 Loss 6.8817 Accuracy 0.0492\n",
            "Epoch 1 Batch 2583 Loss 6.8813 Accuracy 0.0492\n",
            "Epoch 1 Batch 2584 Loss 6.8809 Accuracy 0.0492\n",
            "Epoch 1 Batch 2585 Loss 6.8805 Accuracy 0.0492\n",
            "Epoch 1 Batch 2586 Loss 6.8801 Accuracy 0.0492\n",
            "Epoch 1 Batch 2587 Loss 6.8796 Accuracy 0.0492\n",
            "Epoch 1 Batch 2588 Loss 6.8792 Accuracy 0.0492\n",
            "Epoch 1 Batch 2589 Loss 6.8787 Accuracy 0.0492\n",
            "Epoch 1 Batch 2590 Loss 6.8782 Accuracy 0.0492\n",
            "Epoch 1 Batch 2591 Loss 6.8778 Accuracy 0.0492\n",
            "Epoch 1 Batch 2592 Loss 6.8775 Accuracy 0.0492\n",
            "Epoch 1 Batch 2593 Loss 6.8771 Accuracy 0.0492\n",
            "Epoch 1 Batch 2594 Loss 6.8766 Accuracy 0.0492\n",
            "Epoch 1 Batch 2595 Loss 6.8763 Accuracy 0.0492\n",
            "Epoch 1 Batch 2596 Loss 6.8760 Accuracy 0.0492\n",
            "Epoch 1 Batch 2597 Loss 6.8756 Accuracy 0.0492\n",
            "Epoch 1 Batch 2598 Loss 6.8751 Accuracy 0.0493\n",
            "Epoch 1 Batch 2599 Loss 6.8747 Accuracy 0.0493\n",
            "Epoch 1 Batch 2600 Loss 6.8743 Accuracy 0.0493\n",
            "Epoch 1 Batch 2601 Loss 6.8739 Accuracy 0.0493\n",
            "Epoch 1 Batch 2602 Loss 6.8735 Accuracy 0.0493\n",
            "Epoch 1 Batch 2603 Loss 6.8732 Accuracy 0.0493\n",
            "Epoch 1 Batch 2604 Loss 6.8728 Accuracy 0.0493\n",
            "Epoch 1 Batch 2605 Loss 6.8724 Accuracy 0.0493\n",
            "Epoch 1 Batch 2606 Loss 6.8721 Accuracy 0.0493\n",
            "Epoch 1 Batch 2607 Loss 6.8718 Accuracy 0.0493\n",
            "Epoch 1 Batch 2608 Loss 6.8713 Accuracy 0.0493\n",
            "Epoch 1 Batch 2609 Loss 6.8710 Accuracy 0.0493\n",
            "Epoch 1 Batch 2610 Loss 6.8706 Accuracy 0.0493\n",
            "Epoch 1 Batch 2611 Loss 6.8703 Accuracy 0.0493\n",
            "Epoch 1 Batch 2612 Loss 6.8698 Accuracy 0.0493\n",
            "Epoch 1 Batch 2613 Loss 6.8694 Accuracy 0.0493\n",
            "Epoch 1 Batch 2614 Loss 6.8691 Accuracy 0.0493\n",
            "Epoch 1 Batch 2615 Loss 6.8688 Accuracy 0.0493\n",
            "Epoch 1 Batch 2616 Loss 6.8684 Accuracy 0.0493\n",
            "Epoch 1 Batch 2617 Loss 6.8680 Accuracy 0.0494\n",
            "Epoch 1 Batch 2618 Loss 6.8675 Accuracy 0.0494\n",
            "Epoch 1 Batch 2619 Loss 6.8671 Accuracy 0.0494\n",
            "Epoch 1 Batch 2620 Loss 6.8668 Accuracy 0.0494\n",
            "Epoch 1 Batch 2621 Loss 6.8664 Accuracy 0.0494\n",
            "Epoch 1 Batch 2622 Loss 6.8661 Accuracy 0.0494\n",
            "Epoch 1 Batch 2623 Loss 6.8657 Accuracy 0.0494\n",
            "Epoch 1 Batch 2624 Loss 6.8652 Accuracy 0.0494\n",
            "Epoch 1 Batch 2625 Loss 6.8649 Accuracy 0.0494\n",
            "Epoch 1 Batch 2626 Loss 6.8645 Accuracy 0.0494\n",
            "Epoch 1 Batch 2627 Loss 6.8642 Accuracy 0.0494\n",
            "Epoch 1 Batch 2628 Loss 6.8638 Accuracy 0.0494\n",
            "Epoch 1 Batch 2629 Loss 6.8633 Accuracy 0.0494\n",
            "Epoch 1 Batch 2630 Loss 6.8629 Accuracy 0.0494\n",
            "Epoch 1 Batch 2631 Loss 6.8624 Accuracy 0.0494\n",
            "Epoch 1 Batch 2632 Loss 6.8621 Accuracy 0.0494\n",
            "Epoch 1 Batch 2633 Loss 6.8617 Accuracy 0.0494\n",
            "Epoch 1 Batch 2634 Loss 6.8612 Accuracy 0.0495\n",
            "Epoch 1 Batch 2635 Loss 6.8608 Accuracy 0.0495\n",
            "Epoch 1 Batch 2636 Loss 6.8606 Accuracy 0.0495\n",
            "Epoch 1 Batch 2637 Loss 6.8601 Accuracy 0.0495\n",
            "Epoch 1 Batch 2638 Loss 6.8598 Accuracy 0.0495\n",
            "Epoch 1 Batch 2639 Loss 6.8594 Accuracy 0.0495\n",
            "Epoch 1 Batch 2640 Loss 6.8591 Accuracy 0.0495\n",
            "Epoch 1 Batch 2641 Loss 6.8588 Accuracy 0.0495\n",
            "Epoch 1 Batch 2642 Loss 6.8584 Accuracy 0.0495\n",
            "Epoch 1 Batch 2643 Loss 6.8579 Accuracy 0.0495\n",
            "Epoch 1 Batch 2644 Loss 6.8576 Accuracy 0.0495\n",
            "Epoch 1 Batch 2645 Loss 6.8571 Accuracy 0.0495\n",
            "Epoch 1 Batch 2646 Loss 6.8568 Accuracy 0.0495\n",
            "Epoch 1 Batch 2647 Loss 6.8565 Accuracy 0.0495\n",
            "Epoch 1 Batch 2648 Loss 6.8561 Accuracy 0.0495\n",
            "Epoch 1 Batch 2649 Loss 6.8558 Accuracy 0.0495\n",
            "Epoch 1 Batch 2650 Loss 6.8553 Accuracy 0.0495\n",
            "Epoch 1 Batch 2651 Loss 6.8548 Accuracy 0.0495\n",
            "Epoch 1 Batch 2652 Loss 6.8545 Accuracy 0.0495\n",
            "Epoch 1 Batch 2653 Loss 6.8540 Accuracy 0.0495\n",
            "Epoch 1 Batch 2654 Loss 6.8536 Accuracy 0.0496\n",
            "Epoch 1 Batch 2655 Loss 6.8532 Accuracy 0.0496\n",
            "Epoch 1 Batch 2656 Loss 6.8528 Accuracy 0.0496\n",
            "Epoch 1 Batch 2657 Loss 6.8526 Accuracy 0.0496\n",
            "Epoch 1 Batch 2658 Loss 6.8522 Accuracy 0.0496\n",
            "Epoch 1 Batch 2659 Loss 6.8518 Accuracy 0.0496\n",
            "Epoch 1 Batch 2660 Loss 6.8514 Accuracy 0.0496\n",
            "Epoch 1 Batch 2661 Loss 6.8510 Accuracy 0.0496\n",
            "Epoch 1 Batch 2662 Loss 6.8507 Accuracy 0.0496\n",
            "Epoch 1 Batch 2663 Loss 6.8503 Accuracy 0.0496\n",
            "Epoch 1 Batch 2664 Loss 6.8500 Accuracy 0.0496\n",
            "Epoch 1 Batch 2665 Loss 6.8496 Accuracy 0.0496\n",
            "Epoch 1 Batch 2666 Loss 6.8493 Accuracy 0.0496\n",
            "Epoch 1 Batch 2667 Loss 6.8489 Accuracy 0.0496\n",
            "Epoch 1 Batch 2668 Loss 6.8485 Accuracy 0.0496\n",
            "Epoch 1 Batch 2669 Loss 6.8482 Accuracy 0.0496\n",
            "Epoch 1 Batch 2670 Loss 6.8477 Accuracy 0.0496\n",
            "Epoch 1 Batch 2671 Loss 6.8473 Accuracy 0.0496\n",
            "Epoch 1 Batch 2672 Loss 6.8469 Accuracy 0.0497\n",
            "Epoch 1 Batch 2673 Loss 6.8465 Accuracy 0.0497\n",
            "Epoch 1 Batch 2674 Loss 6.8462 Accuracy 0.0497\n",
            "Epoch 1 Batch 2675 Loss 6.8459 Accuracy 0.0497\n",
            "Epoch 1 Batch 2676 Loss 6.8454 Accuracy 0.0497\n",
            "Epoch 1 Batch 2677 Loss 6.8452 Accuracy 0.0497\n",
            "Epoch 1 Batch 2678 Loss 6.8448 Accuracy 0.0497\n",
            "Epoch 1 Batch 2679 Loss 6.8446 Accuracy 0.0497\n",
            "Epoch 1 Batch 2680 Loss 6.8442 Accuracy 0.0497\n",
            "Epoch 1 Batch 2681 Loss 6.8440 Accuracy 0.0497\n",
            "Epoch 1 Batch 2682 Loss 6.8436 Accuracy 0.0497\n",
            "Epoch 1 Batch 2683 Loss 6.8432 Accuracy 0.0497\n",
            "Epoch 1 Batch 2684 Loss 6.8429 Accuracy 0.0497\n",
            "Epoch 1 Batch 2685 Loss 6.8425 Accuracy 0.0497\n",
            "Epoch 1 Batch 2686 Loss 6.8423 Accuracy 0.0497\n",
            "Epoch 1 Batch 2687 Loss 6.8419 Accuracy 0.0497\n",
            "Epoch 1 Batch 2688 Loss 6.8415 Accuracy 0.0497\n",
            "Epoch 1 Batch 2689 Loss 6.8411 Accuracy 0.0497\n",
            "Epoch 1 Batch 2690 Loss 6.8407 Accuracy 0.0498\n",
            "Epoch 1 Batch 2691 Loss 6.8404 Accuracy 0.0498\n",
            "Epoch 1 Batch 2692 Loss 6.8400 Accuracy 0.0498\n",
            "Epoch 1 Batch 2693 Loss 6.8397 Accuracy 0.0498\n",
            "Epoch 1 Batch 2694 Loss 6.8393 Accuracy 0.0498\n",
            "Epoch 1 Batch 2695 Loss 6.8390 Accuracy 0.0498\n",
            "Epoch 1 Batch 2696 Loss 6.8386 Accuracy 0.0498\n",
            "Epoch 1 Batch 2697 Loss 6.8382 Accuracy 0.0498\n",
            "Epoch 1 Batch 2698 Loss 6.8380 Accuracy 0.0498\n",
            "Epoch 1 Batch 2699 Loss 6.8376 Accuracy 0.0498\n",
            "Epoch 1 Batch 2700 Loss 6.8371 Accuracy 0.0498\n",
            "Epoch 1 Batch 2701 Loss 6.8368 Accuracy 0.0498\n",
            "Epoch 1 Batch 2702 Loss 6.8365 Accuracy 0.0498\n",
            "Epoch 1 Batch 2703 Loss 6.8363 Accuracy 0.0498\n",
            "Epoch 1 Batch 2704 Loss 6.8358 Accuracy 0.0498\n",
            "Epoch 1 Batch 2705 Loss 6.8354 Accuracy 0.0498\n",
            "Epoch 1 Batch 2706 Loss 6.8351 Accuracy 0.0498\n",
            "Epoch 1 Batch 2707 Loss 6.8347 Accuracy 0.0498\n",
            "Epoch 1 Batch 2708 Loss 6.8343 Accuracy 0.0498\n",
            "Epoch 1 Batch 2709 Loss 6.8340 Accuracy 0.0498\n",
            "Epoch 1 Batch 2710 Loss 6.8336 Accuracy 0.0498\n",
            "Epoch 1 Batch 2711 Loss 6.8333 Accuracy 0.0499\n",
            "Epoch 1 Batch 2712 Loss 6.8330 Accuracy 0.0499\n",
            "Epoch 1 Batch 2713 Loss 6.8327 Accuracy 0.0499\n",
            "Epoch 1 Batch 2714 Loss 6.8322 Accuracy 0.0499\n",
            "Epoch 1 Batch 2715 Loss 6.8318 Accuracy 0.0499\n",
            "Epoch 1 Batch 2716 Loss 6.8315 Accuracy 0.0499\n",
            "Epoch 1 Batch 2717 Loss 6.8311 Accuracy 0.0499\n",
            "Epoch 1 Batch 2718 Loss 6.8307 Accuracy 0.0499\n",
            "Epoch 1 Batch 2719 Loss 6.8303 Accuracy 0.0499\n",
            "Epoch 1 Batch 2720 Loss 6.8300 Accuracy 0.0499\n",
            "Epoch 1 Batch 2721 Loss 6.8295 Accuracy 0.0499\n",
            "Epoch 1 Batch 2722 Loss 6.8291 Accuracy 0.0499\n",
            "Epoch 1 Batch 2723 Loss 6.8288 Accuracy 0.0499\n",
            "Epoch 1 Batch 2724 Loss 6.8285 Accuracy 0.0499\n",
            "Epoch 1 Batch 2725 Loss 6.8282 Accuracy 0.0499\n",
            "Epoch 1 Batch 2726 Loss 6.8278 Accuracy 0.0499\n",
            "Epoch 1 Batch 2727 Loss 6.8274 Accuracy 0.0499\n",
            "Epoch 1 Batch 2728 Loss 6.8270 Accuracy 0.0500\n",
            "Epoch 1 Batch 2729 Loss 6.8268 Accuracy 0.0500\n",
            "Epoch 1 Batch 2730 Loss 6.8265 Accuracy 0.0500\n",
            "Epoch 1 Batch 2731 Loss 6.8261 Accuracy 0.0500\n",
            "Epoch 1 Batch 2732 Loss 6.8258 Accuracy 0.0500\n",
            "Epoch 1 Batch 2733 Loss 6.8255 Accuracy 0.0500\n",
            "Epoch 1 Batch 2734 Loss 6.8251 Accuracy 0.0500\n",
            "Epoch 1 Batch 2735 Loss 6.8248 Accuracy 0.0500\n",
            "Epoch 1 Batch 2736 Loss 6.8245 Accuracy 0.0500\n",
            "Epoch 1 Batch 2737 Loss 6.8240 Accuracy 0.0500\n",
            "Epoch 1 Batch 2738 Loss 6.8236 Accuracy 0.0500\n",
            "Epoch 1 Batch 2739 Loss 6.8234 Accuracy 0.0500\n",
            "Epoch 1 Batch 2740 Loss 6.8231 Accuracy 0.0500\n",
            "Epoch 1 Batch 2741 Loss 6.8228 Accuracy 0.0500\n",
            "Epoch 1 Batch 2742 Loss 6.8225 Accuracy 0.0500\n",
            "Epoch 1 Batch 2743 Loss 6.8221 Accuracy 0.0500\n",
            "Epoch 1 Batch 2744 Loss 6.8217 Accuracy 0.0500\n",
            "Epoch 1 Batch 2745 Loss 6.8213 Accuracy 0.0500\n",
            "Epoch 1 Batch 2746 Loss 6.8210 Accuracy 0.0500\n",
            "Epoch 1 Batch 2747 Loss 6.8207 Accuracy 0.0501\n",
            "Epoch 1 Batch 2748 Loss 6.8204 Accuracy 0.0501\n",
            "Epoch 1 Batch 2749 Loss 6.8201 Accuracy 0.0501\n",
            "Epoch 1 Batch 2750 Loss 6.8198 Accuracy 0.0501\n",
            "Epoch 1 Batch 2751 Loss 6.8194 Accuracy 0.0501\n",
            "Epoch 1 Batch 2752 Loss 6.8191 Accuracy 0.0501\n",
            "Epoch 1 Batch 2753 Loss 6.8187 Accuracy 0.0501\n",
            "Epoch 1 Batch 2754 Loss 6.8183 Accuracy 0.0501\n",
            "Epoch 1 Batch 2755 Loss 6.8178 Accuracy 0.0501\n",
            "Epoch 1 Batch 2756 Loss 6.8175 Accuracy 0.0501\n",
            "Epoch 1 Batch 2757 Loss 6.8171 Accuracy 0.0501\n",
            "Epoch 1 Batch 2758 Loss 6.8168 Accuracy 0.0501\n",
            "Epoch 1 Batch 2759 Loss 6.8165 Accuracy 0.0501\n",
            "Epoch 1 Batch 2760 Loss 6.8162 Accuracy 0.0501\n",
            "Epoch 1 Batch 2761 Loss 6.8158 Accuracy 0.0501\n",
            "Epoch 1 Batch 2762 Loss 6.8154 Accuracy 0.0501\n",
            "Epoch 1 Batch 2763 Loss 6.8149 Accuracy 0.0501\n",
            "Epoch 1 Batch 2764 Loss 6.8145 Accuracy 0.0501\n",
            "Epoch 1 Batch 2765 Loss 6.8141 Accuracy 0.0501\n",
            "Epoch 1 Batch 2766 Loss 6.8139 Accuracy 0.0502\n",
            "Epoch 1 Batch 2767 Loss 6.8134 Accuracy 0.0502\n",
            "Epoch 1 Batch 2768 Loss 6.8132 Accuracy 0.0502\n",
            "Epoch 1 Batch 2769 Loss 6.8128 Accuracy 0.0502\n",
            "Epoch 1 Batch 2770 Loss 6.8123 Accuracy 0.0502\n",
            "Epoch 1 Batch 2771 Loss 6.8119 Accuracy 0.0502\n",
            "Epoch 1 Batch 2772 Loss 6.8116 Accuracy 0.0502\n",
            "Epoch 1 Batch 2773 Loss 6.8112 Accuracy 0.0502\n",
            "Epoch 1 Batch 2774 Loss 6.8108 Accuracy 0.0502\n",
            "Epoch 1 Batch 2775 Loss 6.8104 Accuracy 0.0502\n",
            "Epoch 1 Batch 2776 Loss 6.8102 Accuracy 0.0502\n",
            "Epoch 1 Batch 2777 Loss 6.8097 Accuracy 0.0502\n",
            "Epoch 1 Batch 2778 Loss 6.8094 Accuracy 0.0502\n",
            "Epoch 1 Batch 2779 Loss 6.8089 Accuracy 0.0502\n",
            "Epoch 1 Batch 2780 Loss 6.8085 Accuracy 0.0502\n",
            "Epoch 1 Batch 2781 Loss 6.8082 Accuracy 0.0502\n",
            "Epoch 1 Batch 2782 Loss 6.8078 Accuracy 0.0502\n",
            "Epoch 1 Batch 2783 Loss 6.8075 Accuracy 0.0502\n",
            "Epoch 1 Batch 2784 Loss 6.8072 Accuracy 0.0502\n",
            "Epoch 1 Batch 2785 Loss 6.8068 Accuracy 0.0502\n",
            "Epoch 1 Batch 2786 Loss 6.8064 Accuracy 0.0503\n",
            "Epoch 1 Batch 2787 Loss 6.8061 Accuracy 0.0503\n",
            "Epoch 1 Batch 2788 Loss 6.8058 Accuracy 0.0503\n",
            "Epoch 1 Batch 2789 Loss 6.8055 Accuracy 0.0503\n",
            "Epoch 1 Batch 2790 Loss 6.8051 Accuracy 0.0503\n",
            "Epoch 1 Batch 2791 Loss 6.8049 Accuracy 0.0503\n",
            "Epoch 1 Batch 2792 Loss 6.8045 Accuracy 0.0503\n",
            "Epoch 1 Batch 2793 Loss 6.8041 Accuracy 0.0503\n",
            "Epoch 1 Batch 2794 Loss 6.8038 Accuracy 0.0503\n",
            "Epoch 1 Batch 2795 Loss 6.8035 Accuracy 0.0503\n",
            "Epoch 1 Batch 2796 Loss 6.8032 Accuracy 0.0503\n",
            "Epoch 1 Batch 2797 Loss 6.8029 Accuracy 0.0503\n",
            "Epoch 1 Batch 2798 Loss 6.8026 Accuracy 0.0503\n",
            "Epoch 1 Batch 2799 Loss 6.8022 Accuracy 0.0503\n",
            "Epoch 1 Batch 2800 Loss 6.8018 Accuracy 0.0503\n",
            "Epoch 1 Batch 2801 Loss 6.8016 Accuracy 0.0503\n",
            "Epoch 1 Batch 2802 Loss 6.8011 Accuracy 0.0503\n",
            "Epoch 1 Batch 2803 Loss 6.8008 Accuracy 0.0503\n",
            "Epoch 1 Batch 2804 Loss 6.8006 Accuracy 0.0504\n",
            "Epoch 1 Batch 2805 Loss 6.8003 Accuracy 0.0504\n",
            "Epoch 1 Batch 2806 Loss 6.7999 Accuracy 0.0504\n",
            "Epoch 1 Batch 2807 Loss 6.7995 Accuracy 0.0504\n",
            "Epoch 1 Batch 2808 Loss 6.7992 Accuracy 0.0504\n",
            "Epoch 1 Batch 2809 Loss 6.7989 Accuracy 0.0504\n",
            "Epoch 1 Batch 2810 Loss 6.7986 Accuracy 0.0504\n",
            "Epoch 1 Batch 2811 Loss 6.7983 Accuracy 0.0504\n",
            "Epoch 1 Batch 2812 Loss 6.7979 Accuracy 0.0504\n",
            "Epoch 1 Batch 2813 Loss 6.7975 Accuracy 0.0504\n",
            "Epoch 1 Batch 2814 Loss 6.7972 Accuracy 0.0504\n",
            "Epoch 1 Batch 2815 Loss 6.7969 Accuracy 0.0504\n",
            "Epoch 1 Batch 2816 Loss 6.7966 Accuracy 0.0504\n",
            "Epoch 1 Batch 2817 Loss 6.7962 Accuracy 0.0504\n",
            "Epoch 1 Batch 2818 Loss 6.7959 Accuracy 0.0504\n",
            "Epoch 1 Batch 2819 Loss 6.7954 Accuracy 0.0504\n",
            "Epoch 1 Batch 2820 Loss 6.7949 Accuracy 0.0504\n",
            "Epoch 1 Batch 2821 Loss 6.7945 Accuracy 0.0504\n",
            "Epoch 1 Batch 2822 Loss 6.7942 Accuracy 0.0504\n",
            "Epoch 1 Batch 2823 Loss 6.7940 Accuracy 0.0504\n",
            "Epoch 1 Batch 2824 Loss 6.7936 Accuracy 0.0505\n",
            "Epoch 1 Batch 2825 Loss 6.7934 Accuracy 0.0505\n",
            "Epoch 1 Batch 2826 Loss 6.7931 Accuracy 0.0505\n",
            "Epoch 1 Batch 2827 Loss 6.7927 Accuracy 0.0505\n",
            "Epoch 1 Batch 2828 Loss 6.7924 Accuracy 0.0505\n",
            "Epoch 1 Batch 2829 Loss 6.7921 Accuracy 0.0505\n",
            "Epoch 1 Batch 2830 Loss 6.7919 Accuracy 0.0505\n",
            "Epoch 1 Batch 2831 Loss 6.7916 Accuracy 0.0505\n",
            "Epoch 1 Batch 2832 Loss 6.7913 Accuracy 0.0505\n",
            "Epoch 1 Batch 2833 Loss 6.7911 Accuracy 0.0505\n",
            "Epoch 1 Batch 2834 Loss 6.7908 Accuracy 0.0505\n",
            "Epoch 1 Batch 2835 Loss 6.7905 Accuracy 0.0505\n",
            "Epoch 1 Batch 2836 Loss 6.7901 Accuracy 0.0505\n",
            "Epoch 1 Batch 2837 Loss 6.7898 Accuracy 0.0505\n",
            "Epoch 1 Batch 2838 Loss 6.7894 Accuracy 0.0505\n",
            "Epoch 1 Batch 2839 Loss 6.7891 Accuracy 0.0505\n",
            "Epoch 1 Batch 2840 Loss 6.7888 Accuracy 0.0505\n",
            "Epoch 1 Batch 2841 Loss 6.7885 Accuracy 0.0505\n",
            "Epoch 1 Batch 2842 Loss 6.7883 Accuracy 0.0505\n",
            "Epoch 1 Batch 2843 Loss 6.7880 Accuracy 0.0505\n",
            "Epoch 1 Batch 2844 Loss 6.7877 Accuracy 0.0506\n",
            "Epoch 1 Batch 2845 Loss 6.7874 Accuracy 0.0506\n",
            "Epoch 1 Batch 2846 Loss 6.7871 Accuracy 0.0506\n",
            "Epoch 1 Batch 2847 Loss 6.7868 Accuracy 0.0506\n",
            "Epoch 1 Batch 2848 Loss 6.7865 Accuracy 0.0506\n",
            "Epoch 1 Batch 2849 Loss 6.7861 Accuracy 0.0506\n",
            "Epoch 1 Batch 2850 Loss 6.7858 Accuracy 0.0506\n",
            "Epoch 1 Batch 2851 Loss 6.7854 Accuracy 0.0506\n",
            "Epoch 1 Batch 2852 Loss 6.7851 Accuracy 0.0506\n",
            "Epoch 1 Batch 2853 Loss 6.7849 Accuracy 0.0506\n",
            "Epoch 1 Batch 2854 Loss 6.7845 Accuracy 0.0506\n",
            "Epoch 1 Batch 2855 Loss 6.7842 Accuracy 0.0506\n",
            "Epoch 1 Batch 2856 Loss 6.7840 Accuracy 0.0506\n",
            "Epoch 1 Batch 2857 Loss 6.7836 Accuracy 0.0506\n",
            "Epoch 1 Batch 2858 Loss 6.7832 Accuracy 0.0506\n",
            "Epoch 1 Batch 2859 Loss 6.7829 Accuracy 0.0506\n",
            "Epoch 1 Batch 2860 Loss 6.7826 Accuracy 0.0506\n",
            "Epoch 1 Batch 2861 Loss 6.7823 Accuracy 0.0506\n",
            "Epoch 1 Batch 2862 Loss 6.7820 Accuracy 0.0506\n",
            "Epoch 1 Batch 2863 Loss 6.7816 Accuracy 0.0506\n",
            "Epoch 1 Batch 2864 Loss 6.7814 Accuracy 0.0506\n",
            "Epoch 1 Batch 2865 Loss 6.7812 Accuracy 0.0506\n",
            "Epoch 1 Batch 2866 Loss 6.7809 Accuracy 0.0506\n",
            "Epoch 1 Batch 2867 Loss 6.7806 Accuracy 0.0506\n",
            "Epoch 1 Batch 2868 Loss 6.7803 Accuracy 0.0506\n",
            "Epoch 1 Batch 2869 Loss 6.7800 Accuracy 0.0507\n",
            "Epoch 1 Batch 2870 Loss 6.7797 Accuracy 0.0507\n",
            "Epoch 1 Batch 2871 Loss 6.7794 Accuracy 0.0507\n",
            "Epoch 1 Batch 2872 Loss 6.7792 Accuracy 0.0507\n",
            "Epoch 1 Batch 2873 Loss 6.7789 Accuracy 0.0507\n",
            "Epoch 1 Batch 2874 Loss 6.7787 Accuracy 0.0507\n",
            "Epoch 1 Batch 2875 Loss 6.7784 Accuracy 0.0507\n",
            "Epoch 1 Batch 2876 Loss 6.7782 Accuracy 0.0507\n",
            "Epoch 1 Batch 2877 Loss 6.7778 Accuracy 0.0507\n",
            "Epoch 1 Batch 2878 Loss 6.7775 Accuracy 0.0507\n",
            "Epoch 1 Batch 2879 Loss 6.7772 Accuracy 0.0507\n",
            "Epoch 1 Batch 2880 Loss 6.7769 Accuracy 0.0507\n",
            "Epoch 1 Batch 2881 Loss 6.7766 Accuracy 0.0507\n",
            "Epoch 1 Batch 2882 Loss 6.7762 Accuracy 0.0507\n",
            "Epoch 1 Batch 2883 Loss 6.7760 Accuracy 0.0507\n",
            "Epoch 1 Batch 2884 Loss 6.7757 Accuracy 0.0507\n",
            "Epoch 1 Batch 2885 Loss 6.7754 Accuracy 0.0507\n",
            "Epoch 1 Batch 2886 Loss 6.7752 Accuracy 0.0508\n",
            "Epoch 1 Batch 2887 Loss 6.7748 Accuracy 0.0508\n",
            "Epoch 1 Batch 2888 Loss 6.7745 Accuracy 0.0508\n",
            "Epoch 1 Batch 2889 Loss 6.7742 Accuracy 0.0508\n",
            "Epoch 1 Batch 2890 Loss 6.7738 Accuracy 0.0508\n",
            "Epoch 1 Batch 2891 Loss 6.7734 Accuracy 0.0508\n",
            "Epoch 1 Batch 2892 Loss 6.7732 Accuracy 0.0508\n",
            "Epoch 1 Batch 2893 Loss 6.7728 Accuracy 0.0508\n",
            "Epoch 1 Batch 2894 Loss 6.7724 Accuracy 0.0508\n",
            "Epoch 1 Batch 2895 Loss 6.7720 Accuracy 0.0508\n",
            "Epoch 1 Batch 2896 Loss 6.7717 Accuracy 0.0508\n",
            "Epoch 1 Batch 2897 Loss 6.7715 Accuracy 0.0508\n",
            "Epoch 1 Batch 2898 Loss 6.7712 Accuracy 0.0508\n",
            "Epoch 1 Batch 2899 Loss 6.7708 Accuracy 0.0508\n",
            "Epoch 1 Batch 2900 Loss 6.7706 Accuracy 0.0508\n",
            "Epoch 1 Batch 2901 Loss 6.7703 Accuracy 0.0508\n",
            "Epoch 1 Batch 2902 Loss 6.7699 Accuracy 0.0508\n",
            "Epoch 1 Batch 2903 Loss 6.7697 Accuracy 0.0508\n",
            "Epoch 1 Batch 2904 Loss 6.7693 Accuracy 0.0508\n",
            "Epoch 1 Batch 2905 Loss 6.7691 Accuracy 0.0509\n",
            "Epoch 1 Batch 2906 Loss 6.7688 Accuracy 0.0509\n",
            "Epoch 1 Batch 2907 Loss 6.7685 Accuracy 0.0509\n",
            "Epoch 1 Batch 2908 Loss 6.7683 Accuracy 0.0509\n",
            "Epoch 1 Batch 2909 Loss 6.7679 Accuracy 0.0509\n",
            "Epoch 1 Batch 2910 Loss 6.7676 Accuracy 0.0509\n",
            "Epoch 1 Batch 2911 Loss 6.7672 Accuracy 0.0509\n",
            "Epoch 1 Batch 2912 Loss 6.7669 Accuracy 0.0509\n",
            "Epoch 1 Batch 2913 Loss 6.7666 Accuracy 0.0509\n",
            "Epoch 1 Batch 2914 Loss 6.7662 Accuracy 0.0509\n",
            "Epoch 1 Batch 2915 Loss 6.7659 Accuracy 0.0509\n",
            "Epoch 1 Batch 2916 Loss 6.7657 Accuracy 0.0509\n",
            "Epoch 1 Batch 2917 Loss 6.7653 Accuracy 0.0509\n",
            "Epoch 1 Batch 2918 Loss 6.7651 Accuracy 0.0509\n",
            "Epoch 1 Batch 2919 Loss 6.7648 Accuracy 0.0509\n",
            "Epoch 1 Batch 2920 Loss 6.7644 Accuracy 0.0509\n",
            "Epoch 1 Batch 2921 Loss 6.7642 Accuracy 0.0509\n",
            "Epoch 1 Batch 2922 Loss 6.7638 Accuracy 0.0509\n",
            "Epoch 1 Batch 2923 Loss 6.7635 Accuracy 0.0509\n",
            "Epoch 1 Batch 2924 Loss 6.7633 Accuracy 0.0509\n",
            "Epoch 1 Batch 2925 Loss 6.7630 Accuracy 0.0510\n",
            "Epoch 1 Batch 2926 Loss 6.7627 Accuracy 0.0510\n",
            "Epoch 1 Batch 2927 Loss 6.7624 Accuracy 0.0510\n",
            "Epoch 1 Batch 2928 Loss 6.7621 Accuracy 0.0510\n",
            "Epoch 1 Batch 2929 Loss 6.7619 Accuracy 0.0510\n",
            "Epoch 1 Batch 2930 Loss 6.7616 Accuracy 0.0510\n",
            "Epoch 1 Batch 2931 Loss 6.7613 Accuracy 0.0510\n",
            "Epoch 1 Batch 2932 Loss 6.7610 Accuracy 0.0510\n",
            "Epoch 1 Batch 2933 Loss 6.7607 Accuracy 0.0510\n",
            "Epoch 1 Batch 2934 Loss 6.7605 Accuracy 0.0510\n",
            "Epoch 1 Batch 2935 Loss 6.7602 Accuracy 0.0510\n",
            "Epoch 1 Batch 2936 Loss 6.7599 Accuracy 0.0510\n",
            "Epoch 1 Batch 2937 Loss 6.7595 Accuracy 0.0510\n",
            "Epoch 1 Batch 2938 Loss 6.7592 Accuracy 0.0510\n",
            "Epoch 1 Batch 2939 Loss 6.7590 Accuracy 0.0510\n",
            "Epoch 1 Batch 2940 Loss 6.7586 Accuracy 0.0510\n",
            "Epoch 1 Batch 2941 Loss 6.7581 Accuracy 0.0510\n",
            "Epoch 1 Batch 2942 Loss 6.7578 Accuracy 0.0510\n",
            "Epoch 1 Batch 2943 Loss 6.7577 Accuracy 0.0510\n",
            "Epoch 1 Batch 2944 Loss 6.7575 Accuracy 0.0510\n",
            "Epoch 1 Batch 2945 Loss 6.7573 Accuracy 0.0510\n",
            "Epoch 1 Batch 2946 Loss 6.7571 Accuracy 0.0510\n",
            "Epoch 1 Batch 2947 Loss 6.7568 Accuracy 0.0510\n",
            "Epoch 1 Batch 2948 Loss 6.7565 Accuracy 0.0510\n",
            "Epoch 1 Batch 2949 Loss 6.7562 Accuracy 0.0511\n",
            "Epoch 1 Batch 2950 Loss 6.7558 Accuracy 0.0511\n",
            "Epoch 1 Batch 2951 Loss 6.7554 Accuracy 0.0511\n",
            "Epoch 1 Batch 2952 Loss 6.7551 Accuracy 0.0511\n",
            "Epoch 1 Batch 2953 Loss 6.7548 Accuracy 0.0511\n",
            "Epoch 1 Batch 2954 Loss 6.7544 Accuracy 0.0511\n",
            "Epoch 1 Batch 2955 Loss 6.7541 Accuracy 0.0511\n",
            "Epoch 1 Batch 2956 Loss 6.7539 Accuracy 0.0511\n",
            "Epoch 1 Batch 2957 Loss 6.7536 Accuracy 0.0511\n",
            "Epoch 1 Batch 2958 Loss 6.7533 Accuracy 0.0511\n",
            "Epoch 1 Batch 2959 Loss 6.7531 Accuracy 0.0511\n",
            "Epoch 1 Batch 2960 Loss 6.7527 Accuracy 0.0511\n",
            "Epoch 1 Batch 2961 Loss 6.7524 Accuracy 0.0511\n",
            "Epoch 1 Batch 2962 Loss 6.7522 Accuracy 0.0511\n",
            "Epoch 1 Batch 2963 Loss 6.7518 Accuracy 0.0511\n",
            "Epoch 1 Batch 2964 Loss 6.7514 Accuracy 0.0511\n",
            "Epoch 1 Batch 2965 Loss 6.7510 Accuracy 0.0511\n",
            "Epoch 1 Batch 2966 Loss 6.7508 Accuracy 0.0511\n",
            "Epoch 1 Batch 2967 Loss 6.7506 Accuracy 0.0511\n",
            "Epoch 1 Batch 2968 Loss 6.7503 Accuracy 0.0511\n",
            "Epoch 1 Batch 2969 Loss 6.7499 Accuracy 0.0511\n",
            "Epoch 1 Batch 2970 Loss 6.7497 Accuracy 0.0512\n",
            "Epoch 1 Batch 2971 Loss 6.7495 Accuracy 0.0512\n",
            "Epoch 1 Batch 2972 Loss 6.7492 Accuracy 0.0512\n",
            "Epoch 1 Batch 2973 Loss 6.7488 Accuracy 0.0512\n",
            "Epoch 1 Batch 2974 Loss 6.7485 Accuracy 0.0512\n",
            "Epoch 1 Batch 2975 Loss 6.7481 Accuracy 0.0512\n",
            "Epoch 1 Batch 2976 Loss 6.7478 Accuracy 0.0512\n",
            "Epoch 1 Batch 2977 Loss 6.7476 Accuracy 0.0512\n",
            "Epoch 1 Batch 2978 Loss 6.7472 Accuracy 0.0512\n",
            "Epoch 1 Batch 2979 Loss 6.7470 Accuracy 0.0512\n",
            "Epoch 1 Batch 2980 Loss 6.7468 Accuracy 0.0512\n",
            "Epoch 1 Batch 2981 Loss 6.7465 Accuracy 0.0512\n",
            "Epoch 1 Batch 2982 Loss 6.7461 Accuracy 0.0512\n",
            "Epoch 1 Batch 2983 Loss 6.7458 Accuracy 0.0512\n",
            "Epoch 1 Batch 2984 Loss 6.7455 Accuracy 0.0512\n",
            "Epoch 1 Batch 2985 Loss 6.7451 Accuracy 0.0512\n",
            "Epoch 1 Batch 2986 Loss 6.7449 Accuracy 0.0512\n",
            "Epoch 1 Batch 2987 Loss 6.7446 Accuracy 0.0512\n",
            "Epoch 1 Batch 2988 Loss 6.7443 Accuracy 0.0512\n",
            "Epoch 1 Batch 2989 Loss 6.7439 Accuracy 0.0512\n",
            "Epoch 1 Batch 2990 Loss 6.7436 Accuracy 0.0512\n",
            "Epoch 1 Batch 2991 Loss 6.7433 Accuracy 0.0512\n",
            "Epoch 1 Batch 2992 Loss 6.7429 Accuracy 0.0512\n",
            "Epoch 1 Batch 2993 Loss 6.7428 Accuracy 0.0513\n",
            "Epoch 1 Batch 2994 Loss 6.7424 Accuracy 0.0513\n",
            "Epoch 1 Batch 2995 Loss 6.7421 Accuracy 0.0513\n",
            "Epoch 1 Batch 2996 Loss 6.7418 Accuracy 0.0513\n",
            "Epoch 1 Batch 2997 Loss 6.7416 Accuracy 0.0513\n",
            "Epoch 1 Batch 2998 Loss 6.7413 Accuracy 0.0513\n",
            "Epoch 1 Batch 2999 Loss 6.7410 Accuracy 0.0513\n",
            "Epoch 1 Batch 3000 Loss 6.7407 Accuracy 0.0513\n",
            "Epoch 1 Batch 3001 Loss 6.7404 Accuracy 0.0513\n",
            "Epoch 1 Batch 3002 Loss 6.7401 Accuracy 0.0513\n",
            "Epoch 1 Batch 3003 Loss 6.7397 Accuracy 0.0513\n",
            "Epoch 1 Batch 3004 Loss 6.7394 Accuracy 0.0513\n",
            "Epoch 1 Batch 3005 Loss 6.7390 Accuracy 0.0513\n",
            "Epoch 1 Batch 3006 Loss 6.7388 Accuracy 0.0513\n",
            "Epoch 1 Batch 3007 Loss 6.7384 Accuracy 0.0513\n",
            "Epoch 1 Batch 3008 Loss 6.7381 Accuracy 0.0513\n",
            "Epoch 1 Batch 3009 Loss 6.7377 Accuracy 0.0513\n",
            "Epoch 1 Batch 3010 Loss 6.7374 Accuracy 0.0513\n",
            "Epoch 1 Batch 3011 Loss 6.7371 Accuracy 0.0513\n",
            "Epoch 1 Batch 3012 Loss 6.7369 Accuracy 0.0514\n",
            "Epoch 1 Batch 3013 Loss 6.7366 Accuracy 0.0514\n",
            "Epoch 1 Batch 3014 Loss 6.7364 Accuracy 0.0514\n",
            "Epoch 1 Batch 3015 Loss 6.7361 Accuracy 0.0514\n",
            "Epoch 1 Batch 3016 Loss 6.7358 Accuracy 0.0514\n",
            "Epoch 1 Batch 3017 Loss 6.7354 Accuracy 0.0514\n",
            "Epoch 1 Batch 3018 Loss 6.7351 Accuracy 0.0514\n",
            "Epoch 1 Batch 3019 Loss 6.7349 Accuracy 0.0514\n",
            "Epoch 1 Batch 3020 Loss 6.7346 Accuracy 0.0514\n",
            "Epoch 1 Batch 3021 Loss 6.7344 Accuracy 0.0514\n",
            "Epoch 1 Batch 3022 Loss 6.7341 Accuracy 0.0514\n",
            "Epoch 1 Batch 3023 Loss 6.7338 Accuracy 0.0514\n",
            "Epoch 1 Batch 3024 Loss 6.7336 Accuracy 0.0514\n",
            "Epoch 1 Batch 3025 Loss 6.7334 Accuracy 0.0514\n",
            "Epoch 1 Batch 3026 Loss 6.7331 Accuracy 0.0514\n",
            "Epoch 1 Batch 3027 Loss 6.7328 Accuracy 0.0514\n",
            "Epoch 1 Batch 3028 Loss 6.7324 Accuracy 0.0514\n",
            "Epoch 1 Batch 3029 Loss 6.7320 Accuracy 0.0514\n",
            "Epoch 1 Batch 3030 Loss 6.7316 Accuracy 0.0514\n",
            "Epoch 1 Batch 3031 Loss 6.7313 Accuracy 0.0515\n",
            "Epoch 1 Batch 3032 Loss 6.7310 Accuracy 0.0515\n",
            "Epoch 1 Batch 3033 Loss 6.7307 Accuracy 0.0515\n",
            "Epoch 1 Batch 3034 Loss 6.7304 Accuracy 0.0515\n",
            "Epoch 1 Batch 3035 Loss 6.7301 Accuracy 0.0515\n",
            "Epoch 1 Batch 3036 Loss 6.7299 Accuracy 0.0515\n",
            "Epoch 1 Batch 3037 Loss 6.7296 Accuracy 0.0515\n",
            "Epoch 1 Batch 3038 Loss 6.7293 Accuracy 0.0515\n",
            "Epoch 1 Batch 3039 Loss 6.7289 Accuracy 0.0515\n",
            "Epoch 1 Batch 3040 Loss 6.7286 Accuracy 0.0515\n",
            "Epoch 1 Batch 3041 Loss 6.7285 Accuracy 0.0515\n",
            "Epoch 1 Batch 3042 Loss 6.7281 Accuracy 0.0515\n",
            "Epoch 1 Batch 3043 Loss 6.7277 Accuracy 0.0515\n",
            "Epoch 1 Batch 3044 Loss 6.7274 Accuracy 0.0515\n",
            "Epoch 1 Batch 3045 Loss 6.7270 Accuracy 0.0515\n",
            "Epoch 1 Batch 3046 Loss 6.7268 Accuracy 0.0515\n",
            "Epoch 1 Batch 3047 Loss 6.7265 Accuracy 0.0515\n",
            "Epoch 1 Batch 3048 Loss 6.7262 Accuracy 0.0515\n",
            "Epoch 1 Batch 3049 Loss 6.7259 Accuracy 0.0515\n",
            "Epoch 1 Batch 3050 Loss 6.7256 Accuracy 0.0515\n",
            "Epoch 1 Batch 3051 Loss 6.7254 Accuracy 0.0515\n",
            "Epoch 1 Batch 3052 Loss 6.7251 Accuracy 0.0515\n",
            "Epoch 1 Batch 3053 Loss 6.7249 Accuracy 0.0515\n",
            "Epoch 1 Batch 3054 Loss 6.7245 Accuracy 0.0516\n",
            "Epoch 1 Batch 3055 Loss 6.7243 Accuracy 0.0516\n",
            "Epoch 1 Batch 3056 Loss 6.7240 Accuracy 0.0516\n",
            "Epoch 1 Batch 3057 Loss 6.7237 Accuracy 0.0516\n",
            "Epoch 1 Batch 3058 Loss 6.7234 Accuracy 0.0516\n",
            "Epoch 1 Batch 3059 Loss 6.7232 Accuracy 0.0516\n",
            "Epoch 1 Batch 3060 Loss 6.7229 Accuracy 0.0516\n",
            "Epoch 1 Batch 3061 Loss 6.7226 Accuracy 0.0516\n",
            "Epoch 1 Batch 3062 Loss 6.7224 Accuracy 0.0516\n",
            "Epoch 1 Batch 3063 Loss 6.7221 Accuracy 0.0516\n",
            "Epoch 1 Batch 3064 Loss 6.7219 Accuracy 0.0516\n",
            "Epoch 1 Batch 3065 Loss 6.7216 Accuracy 0.0516\n",
            "Epoch 1 Batch 3066 Loss 6.7213 Accuracy 0.0516\n",
            "Epoch 1 Batch 3067 Loss 6.7209 Accuracy 0.0516\n",
            "Epoch 1 Batch 3068 Loss 6.7207 Accuracy 0.0516\n",
            "Epoch 1 Batch 3069 Loss 6.7205 Accuracy 0.0516\n",
            "Epoch 1 Batch 3070 Loss 6.7202 Accuracy 0.0516\n",
            "Epoch 1 Batch 3071 Loss 6.7200 Accuracy 0.0516\n",
            "Epoch 1 Batch 3072 Loss 6.7197 Accuracy 0.0516\n",
            "Epoch 1 Batch 3073 Loss 6.7194 Accuracy 0.0516\n",
            "Epoch 1 Batch 3074 Loss 6.7191 Accuracy 0.0516\n",
            "Epoch 1 Batch 3075 Loss 6.7189 Accuracy 0.0516\n",
            "Epoch 1 Batch 3076 Loss 6.7187 Accuracy 0.0517\n",
            "Epoch 1 Batch 3077 Loss 6.7183 Accuracy 0.0517\n",
            "Epoch 1 Batch 3078 Loss 6.7179 Accuracy 0.0517\n",
            "Epoch 1 Batch 3079 Loss 6.7177 Accuracy 0.0517\n",
            "Epoch 1 Batch 3080 Loss 6.7174 Accuracy 0.0517\n",
            "Epoch 1 Batch 3081 Loss 6.7172 Accuracy 0.0517\n",
            "Epoch 1 Batch 3082 Loss 6.7169 Accuracy 0.0517\n",
            "Epoch 1 Batch 3083 Loss 6.7167 Accuracy 0.0517\n",
            "Epoch 1 Batch 3084 Loss 6.7164 Accuracy 0.0517\n",
            "Epoch 1 Batch 3085 Loss 6.7161 Accuracy 0.0517\n",
            "Epoch 1 Batch 3086 Loss 6.7157 Accuracy 0.0517\n",
            "Epoch 1 Batch 3087 Loss 6.7154 Accuracy 0.0517\n",
            "Epoch 1 Batch 3088 Loss 6.7152 Accuracy 0.0517\n",
            "Epoch 1 Batch 3089 Loss 6.7149 Accuracy 0.0517\n",
            "Epoch 1 Batch 3090 Loss 6.7146 Accuracy 0.0517\n",
            "Epoch 1 Batch 3091 Loss 6.7143 Accuracy 0.0517\n",
            "Epoch 1 Batch 3092 Loss 6.7142 Accuracy 0.0517\n",
            "Epoch 1 Batch 3093 Loss 6.7139 Accuracy 0.0517\n",
            "Epoch 1 Batch 3094 Loss 6.7136 Accuracy 0.0517\n",
            "Epoch 1 Batch 3095 Loss 6.7132 Accuracy 0.0517\n",
            "Epoch 1 Batch 3096 Loss 6.7129 Accuracy 0.0517\n",
            "Epoch 1 Batch 3097 Loss 6.7126 Accuracy 0.0517\n",
            "Epoch 1 Batch 3098 Loss 6.7122 Accuracy 0.0517\n",
            "Epoch 1 Batch 3099 Loss 6.7119 Accuracy 0.0518\n",
            "Epoch 1 Batch 3100 Loss 6.7116 Accuracy 0.0518\n",
            "Epoch 1 Batch 3101 Loss 6.7113 Accuracy 0.0518\n",
            "Epoch 1 Batch 3102 Loss 6.7110 Accuracy 0.0518\n",
            "Epoch 1 Batch 3103 Loss 6.7107 Accuracy 0.0518\n",
            "Epoch 1 Batch 3104 Loss 6.7105 Accuracy 0.0518\n",
            "Epoch 1 Batch 3105 Loss 6.7103 Accuracy 0.0518\n",
            "Epoch 1 Batch 3106 Loss 6.7099 Accuracy 0.0518\n",
            "Epoch 1 Batch 3107 Loss 6.7095 Accuracy 0.0518\n",
            "Epoch 1 Batch 3108 Loss 6.7092 Accuracy 0.0518\n",
            "Epoch 1 Batch 3109 Loss 6.7090 Accuracy 0.0518\n",
            "Epoch 1 Batch 3110 Loss 6.7088 Accuracy 0.0518\n",
            "Epoch 1 Batch 3111 Loss 6.7085 Accuracy 0.0518\n",
            "Epoch 1 Batch 3112 Loss 6.7081 Accuracy 0.0518\n",
            "Epoch 1 Batch 3113 Loss 6.7078 Accuracy 0.0518\n",
            "Epoch 1 Batch 3114 Loss 6.7076 Accuracy 0.0518\n",
            "Epoch 1 Batch 3115 Loss 6.7073 Accuracy 0.0518\n",
            "Epoch 1 Batch 3116 Loss 6.7071 Accuracy 0.0518\n",
            "Epoch 1 Batch 3117 Loss 6.7068 Accuracy 0.0518\n",
            "Epoch 1 Batch 3118 Loss 6.7064 Accuracy 0.0518\n",
            "Epoch 1 Batch 3119 Loss 6.7061 Accuracy 0.0518\n",
            "Epoch 1 Batch 3120 Loss 6.7058 Accuracy 0.0518\n",
            "Epoch 1 Batch 3121 Loss 6.7053 Accuracy 0.0519\n",
            "Epoch 1 Batch 3122 Loss 6.7051 Accuracy 0.0519\n",
            "Epoch 1 Batch 3123 Loss 6.7048 Accuracy 0.0519\n",
            "Epoch 1 Batch 3124 Loss 6.7046 Accuracy 0.0519\n",
            "Epoch 1 Batch 3125 Loss 6.7042 Accuracy 0.0519\n",
            "Epoch 1 Batch 3126 Loss 6.7040 Accuracy 0.0519\n",
            "Epoch 1 Batch 3127 Loss 6.7038 Accuracy 0.0519\n",
            "Epoch 1 Batch 3128 Loss 6.7036 Accuracy 0.0519\n",
            "Epoch 1 Batch 3129 Loss 6.7033 Accuracy 0.0519\n",
            "Epoch 1 Batch 3130 Loss 6.7030 Accuracy 0.0519\n",
            "Epoch 1 Batch 3131 Loss 6.7027 Accuracy 0.0519\n",
            "Epoch 1 Batch 3132 Loss 6.7024 Accuracy 0.0519\n",
            "Epoch 1 Batch 3133 Loss 6.7021 Accuracy 0.0519\n",
            "Epoch 1 Batch 3134 Loss 6.7019 Accuracy 0.0519\n",
            "Epoch 1 Batch 3135 Loss 6.7016 Accuracy 0.0519\n",
            "Epoch 1 Batch 3136 Loss 6.7014 Accuracy 0.0519\n",
            "Epoch 1 Batch 3137 Loss 6.7011 Accuracy 0.0519\n",
            "Epoch 1 Batch 3138 Loss 6.7009 Accuracy 0.0519\n",
            "Epoch 1 Batch 3139 Loss 6.7006 Accuracy 0.0519\n",
            "Epoch 1 Batch 3140 Loss 6.7004 Accuracy 0.0519\n",
            "Epoch 1 Batch 3141 Loss 6.7002 Accuracy 0.0519\n",
            "Epoch 1 Batch 3142 Loss 6.7000 Accuracy 0.0519\n",
            "Epoch 1 Batch 3143 Loss 6.6997 Accuracy 0.0520\n",
            "Epoch 1 Batch 3144 Loss 6.6994 Accuracy 0.0520\n",
            "Epoch 1 Batch 3145 Loss 6.6991 Accuracy 0.0520\n",
            "Epoch 1 Batch 3146 Loss 6.6987 Accuracy 0.0520\n",
            "Epoch 1 Batch 3147 Loss 6.6984 Accuracy 0.0520\n",
            "Epoch 1 Batch 3148 Loss 6.6982 Accuracy 0.0520\n",
            "Epoch 1 Batch 3149 Loss 6.6979 Accuracy 0.0520\n",
            "Epoch 1 Batch 3150 Loss 6.6976 Accuracy 0.0520\n",
            "Epoch 1 Batch 3151 Loss 6.6973 Accuracy 0.0520\n",
            "Epoch 1 Batch 3152 Loss 6.6970 Accuracy 0.0520\n",
            "Epoch 1 Batch 3153 Loss 6.6967 Accuracy 0.0520\n",
            "Epoch 1 Batch 3154 Loss 6.6965 Accuracy 0.0520\n",
            "Epoch 1 Batch 3155 Loss 6.6962 Accuracy 0.0520\n",
            "Epoch 1 Batch 3156 Loss 6.6959 Accuracy 0.0520\n",
            "Epoch 1 Batch 3157 Loss 6.6955 Accuracy 0.0520\n",
            "Epoch 1 Batch 3158 Loss 6.6952 Accuracy 0.0520\n",
            "Epoch 1 Batch 3159 Loss 6.6949 Accuracy 0.0520\n",
            "Epoch 1 Batch 3160 Loss 6.6945 Accuracy 0.0521\n",
            "Epoch 1 Batch 3161 Loss 6.6943 Accuracy 0.0521\n",
            "Epoch 1 Batch 3162 Loss 6.6941 Accuracy 0.0521\n",
            "Epoch 1 Batch 3163 Loss 6.6937 Accuracy 0.0521\n",
            "Epoch 1 Batch 3164 Loss 6.6935 Accuracy 0.0521\n",
            "Epoch 1 Batch 3165 Loss 6.6931 Accuracy 0.0521\n",
            "Epoch 1 Batch 3166 Loss 6.6927 Accuracy 0.0521\n",
            "Epoch 1 Batch 3167 Loss 6.6924 Accuracy 0.0521\n",
            "Epoch 1 Batch 3168 Loss 6.6920 Accuracy 0.0521\n",
            "Epoch 1 Batch 3169 Loss 6.6918 Accuracy 0.0521\n",
            "Epoch 1 Batch 3170 Loss 6.6914 Accuracy 0.0521\n",
            "Epoch 1 Batch 3171 Loss 6.6912 Accuracy 0.0521\n",
            "Epoch 1 Batch 3172 Loss 6.6909 Accuracy 0.0521\n",
            "Epoch 1 Batch 3173 Loss 6.6907 Accuracy 0.0521\n",
            "Epoch 1 Batch 3174 Loss 6.6904 Accuracy 0.0521\n",
            "Epoch 1 Batch 3175 Loss 6.6901 Accuracy 0.0521\n",
            "Epoch 1 Batch 3176 Loss 6.6899 Accuracy 0.0521\n",
            "Epoch 1 Batch 3177 Loss 6.6896 Accuracy 0.0521\n",
            "Epoch 1 Batch 3178 Loss 6.6894 Accuracy 0.0521\n",
            "Epoch 1 Batch 3179 Loss 6.6891 Accuracy 0.0521\n",
            "Epoch 1 Batch 3180 Loss 6.6889 Accuracy 0.0521\n",
            "Epoch 1 Batch 3181 Loss 6.6886 Accuracy 0.0521\n",
            "Epoch 1 Batch 3182 Loss 6.6884 Accuracy 0.0521\n",
            "Epoch 1 Batch 3183 Loss 6.6881 Accuracy 0.0521\n",
            "Epoch 1 Batch 3184 Loss 6.6877 Accuracy 0.0521\n",
            "Epoch 1 Batch 3185 Loss 6.6874 Accuracy 0.0521\n",
            "Epoch 1 Batch 3186 Loss 6.6872 Accuracy 0.0521\n",
            "Epoch 1 Batch 3187 Loss 6.6869 Accuracy 0.0521\n",
            "Epoch 1 Batch 3188 Loss 6.6867 Accuracy 0.0522\n",
            "Epoch 1 Batch 3189 Loss 6.6864 Accuracy 0.0522\n",
            "Epoch 1 Batch 3190 Loss 6.6862 Accuracy 0.0522\n",
            "Epoch 1 Batch 3191 Loss 6.6859 Accuracy 0.0522\n",
            "Epoch 1 Batch 3192 Loss 6.6856 Accuracy 0.0522\n",
            "Epoch 1 Batch 3193 Loss 6.6854 Accuracy 0.0522\n",
            "Epoch 1 Batch 3194 Loss 6.6851 Accuracy 0.0522\n",
            "Epoch 1 Batch 3195 Loss 6.6849 Accuracy 0.0522\n",
            "Epoch 1 Batch 3196 Loss 6.6847 Accuracy 0.0522\n",
            "Epoch 1 Batch 3197 Loss 6.6844 Accuracy 0.0522\n",
            "Epoch 1 Batch 3198 Loss 6.6840 Accuracy 0.0522\n",
            "Epoch 1 Batch 3199 Loss 6.6837 Accuracy 0.0522\n",
            "Epoch 1 Batch 3200 Loss 6.6834 Accuracy 0.0522\n",
            "Epoch 1 Batch 3201 Loss 6.6830 Accuracy 0.0522\n",
            "Epoch 1 Batch 3202 Loss 6.6827 Accuracy 0.0522\n",
            "Epoch 1 Batch 3203 Loss 6.6824 Accuracy 0.0522\n",
            "Epoch 1 Batch 3204 Loss 6.6823 Accuracy 0.0522\n",
            "Epoch 1 Batch 3205 Loss 6.6821 Accuracy 0.0522\n",
            "Epoch 1 Batch 3206 Loss 6.6819 Accuracy 0.0522\n",
            "Epoch 1 Batch 3207 Loss 6.6816 Accuracy 0.0522\n",
            "Epoch 1 Batch 3208 Loss 6.6814 Accuracy 0.0522\n",
            "Epoch 1 Batch 3209 Loss 6.6811 Accuracy 0.0522\n",
            "Epoch 1 Batch 3210 Loss 6.6809 Accuracy 0.0523\n",
            "Epoch 1 Batch 3211 Loss 6.6805 Accuracy 0.0523\n",
            "Epoch 1 Batch 3212 Loss 6.6803 Accuracy 0.0523\n",
            "Epoch 1 Batch 3213 Loss 6.6801 Accuracy 0.0523\n",
            "Epoch 1 Batch 3214 Loss 6.6797 Accuracy 0.0523\n",
            "Epoch 1 Batch 3215 Loss 6.6794 Accuracy 0.0523\n",
            "Epoch 1 Batch 3216 Loss 6.6791 Accuracy 0.0523\n",
            "Epoch 1 Batch 3217 Loss 6.6788 Accuracy 0.0523\n",
            "Epoch 1 Batch 3218 Loss 6.6785 Accuracy 0.0523\n",
            "Epoch 1 Batch 3219 Loss 6.6783 Accuracy 0.0523\n",
            "Epoch 1 Batch 3220 Loss 6.6781 Accuracy 0.0523\n",
            "Epoch 1 Batch 3221 Loss 6.6779 Accuracy 0.0523\n",
            "Epoch 1 Batch 3222 Loss 6.6776 Accuracy 0.0523\n",
            "Epoch 1 Batch 3223 Loss 6.6773 Accuracy 0.0523\n",
            "Epoch 1 Batch 3224 Loss 6.6770 Accuracy 0.0523\n",
            "Epoch 1 Batch 3225 Loss 6.6768 Accuracy 0.0523\n",
            "Epoch 1 Batch 3226 Loss 6.6765 Accuracy 0.0523\n",
            "Epoch 1 Batch 3227 Loss 6.6762 Accuracy 0.0523\n",
            "Epoch 1 Batch 3228 Loss 6.6759 Accuracy 0.0523\n",
            "Epoch 1 Batch 3229 Loss 6.6756 Accuracy 0.0523\n",
            "Epoch 1 Batch 3230 Loss 6.6753 Accuracy 0.0523\n",
            "Epoch 1 Batch 3231 Loss 6.6750 Accuracy 0.0523\n",
            "Epoch 1 Batch 3232 Loss 6.6747 Accuracy 0.0524\n",
            "Epoch 1 Batch 3233 Loss 6.6744 Accuracy 0.0524\n",
            "Epoch 1 Batch 3234 Loss 6.6741 Accuracy 0.0524\n",
            "Epoch 1 Batch 3235 Loss 6.6739 Accuracy 0.0524\n",
            "Epoch 1 Batch 3236 Loss 6.6736 Accuracy 0.0524\n",
            "Epoch 1 Batch 3237 Loss 6.6733 Accuracy 0.0524\n",
            "Epoch 1 Batch 3238 Loss 6.6730 Accuracy 0.0524\n",
            "Epoch 1 Batch 3239 Loss 6.6727 Accuracy 0.0524\n",
            "Epoch 1 Batch 3240 Loss 6.6724 Accuracy 0.0524\n",
            "Epoch 1 Batch 3241 Loss 6.6721 Accuracy 0.0524\n",
            "Epoch 1 Batch 3242 Loss 6.6718 Accuracy 0.0524\n",
            "Epoch 1 Batch 3243 Loss 6.6716 Accuracy 0.0524\n",
            "Epoch 1 Batch 3244 Loss 6.6714 Accuracy 0.0524\n",
            "Epoch 1 Batch 3245 Loss 6.6711 Accuracy 0.0524\n",
            "Epoch 1 Batch 3246 Loss 6.6709 Accuracy 0.0524\n",
            "Epoch 1 Batch 3247 Loss 6.6706 Accuracy 0.0524\n",
            "Epoch 1 Batch 3248 Loss 6.6703 Accuracy 0.0524\n",
            "Epoch 1 Batch 3249 Loss 6.6700 Accuracy 0.0524\n",
            "Epoch 1 Batch 3250 Loss 6.6697 Accuracy 0.0524\n",
            "Epoch 1 Batch 3251 Loss 6.6694 Accuracy 0.0524\n",
            "Epoch 1 Batch 3252 Loss 6.6692 Accuracy 0.0524\n",
            "Epoch 1 Batch 3253 Loss 6.6690 Accuracy 0.0524\n",
            "Epoch 1 Batch 3254 Loss 6.6687 Accuracy 0.0524\n",
            "Epoch 1 Batch 3255 Loss 6.6684 Accuracy 0.0525\n",
            "Epoch 1 Batch 3256 Loss 6.6682 Accuracy 0.0525\n",
            "Epoch 1 Batch 3257 Loss 6.6679 Accuracy 0.0525\n",
            "Epoch 1 Batch 3258 Loss 6.6677 Accuracy 0.0525\n",
            "Epoch 1 Batch 3259 Loss 6.6675 Accuracy 0.0525\n",
            "Epoch 1 Batch 3260 Loss 6.6673 Accuracy 0.0525\n",
            "Epoch 1 Batch 3261 Loss 6.6670 Accuracy 0.0525\n",
            "Epoch 1 Batch 3262 Loss 6.6668 Accuracy 0.0525\n",
            "Epoch 1 Batch 3263 Loss 6.6665 Accuracy 0.0525\n",
            "Epoch 1 Batch 3264 Loss 6.6663 Accuracy 0.0525\n",
            "Epoch 1 Batch 3265 Loss 6.6661 Accuracy 0.0525\n",
            "Epoch 1 Batch 3266 Loss 6.6658 Accuracy 0.0525\n",
            "Epoch 1 Batch 3267 Loss 6.6656 Accuracy 0.0525\n",
            "Epoch 1 Batch 3268 Loss 6.6654 Accuracy 0.0525\n",
            "Epoch 1 Batch 3269 Loss 6.6652 Accuracy 0.0525\n",
            "Epoch 1 Batch 3270 Loss 6.6650 Accuracy 0.0525\n",
            "Epoch 1 Batch 3271 Loss 6.6648 Accuracy 0.0525\n",
            "Epoch 1 Batch 3272 Loss 6.6646 Accuracy 0.0525\n",
            "Epoch 1 Batch 3273 Loss 6.6643 Accuracy 0.0525\n",
            "Epoch 1 Batch 3274 Loss 6.6640 Accuracy 0.0525\n",
            "Epoch 1 Batch 3275 Loss 6.6637 Accuracy 0.0525\n",
            "Epoch 1 Batch 3276 Loss 6.6634 Accuracy 0.0525\n",
            "Epoch 1 Batch 3277 Loss 6.6631 Accuracy 0.0525\n",
            "Epoch 1 Batch 3278 Loss 6.6629 Accuracy 0.0525\n",
            "Epoch 1 Batch 3279 Loss 6.6627 Accuracy 0.0525\n",
            "Epoch 1 Batch 3280 Loss 6.6625 Accuracy 0.0525\n",
            "Epoch 1 Batch 3281 Loss 6.6623 Accuracy 0.0526\n",
            "Epoch 1 Batch 3282 Loss 6.6620 Accuracy 0.0526\n",
            "Epoch 1 Batch 3283 Loss 6.6616 Accuracy 0.0526\n",
            "Epoch 1 Batch 3284 Loss 6.6614 Accuracy 0.0526\n",
            "Epoch 1 Batch 3285 Loss 6.6611 Accuracy 0.0526\n",
            "Epoch 1 Batch 3286 Loss 6.6609 Accuracy 0.0526\n",
            "Epoch 1 Batch 3287 Loss 6.6608 Accuracy 0.0526\n",
            "Epoch 1 Batch 3288 Loss 6.6605 Accuracy 0.0526\n",
            "Epoch 1 Batch 3289 Loss 6.6603 Accuracy 0.0526\n",
            "Epoch 1 Batch 3290 Loss 6.6599 Accuracy 0.0526\n",
            "Epoch 1 Batch 3291 Loss 6.6596 Accuracy 0.0526\n",
            "Epoch 1 Batch 3292 Loss 6.6593 Accuracy 0.0526\n",
            "Epoch 1 Batch 3293 Loss 6.6591 Accuracy 0.0526\n",
            "Epoch 1 Batch 3294 Loss 6.6589 Accuracy 0.0526\n",
            "Epoch 1 Batch 3295 Loss 6.6587 Accuracy 0.0526\n",
            "Epoch 1 Batch 3296 Loss 6.6585 Accuracy 0.0526\n",
            "Epoch 1 Batch 3297 Loss 6.6583 Accuracy 0.0526\n",
            "Epoch 1 Batch 3298 Loss 6.6580 Accuracy 0.0526\n",
            "Epoch 1 Batch 3299 Loss 6.6578 Accuracy 0.0526\n",
            "Epoch 1 Batch 3300 Loss 6.6575 Accuracy 0.0526\n",
            "Epoch 1 Batch 3301 Loss 6.6572 Accuracy 0.0526\n",
            "Epoch 1 Batch 3302 Loss 6.6569 Accuracy 0.0526\n",
            "Epoch 1 Batch 3303 Loss 6.6566 Accuracy 0.0527\n",
            "Epoch 1 Batch 3304 Loss 6.6564 Accuracy 0.0527\n",
            "Epoch 1 Batch 3305 Loss 6.6562 Accuracy 0.0527\n",
            "Epoch 1 Batch 3306 Loss 6.6560 Accuracy 0.0527\n",
            "Epoch 1 Batch 3307 Loss 6.6557 Accuracy 0.0527\n",
            "Epoch 1 Batch 3308 Loss 6.6554 Accuracy 0.0527\n",
            "Epoch 1 Batch 3309 Loss 6.6552 Accuracy 0.0527\n",
            "Epoch 1 Batch 3310 Loss 6.6550 Accuracy 0.0527\n",
            "Epoch 1 Batch 3311 Loss 6.6548 Accuracy 0.0527\n",
            "Epoch 1 Batch 3312 Loss 6.6546 Accuracy 0.0527\n",
            "Epoch 1 Batch 3313 Loss 6.6544 Accuracy 0.0527\n",
            "Epoch 1 Batch 3314 Loss 6.6543 Accuracy 0.0527\n",
            "Epoch 1 Batch 3315 Loss 6.6540 Accuracy 0.0527\n",
            "Epoch 1 Batch 3316 Loss 6.6538 Accuracy 0.0527\n",
            "Epoch 1 Batch 3317 Loss 6.6535 Accuracy 0.0527\n",
            "Epoch 1 Batch 3318 Loss 6.6532 Accuracy 0.0527\n",
            "Epoch 1 Batch 3319 Loss 6.6530 Accuracy 0.0527\n",
            "Epoch 1 Batch 3320 Loss 6.6527 Accuracy 0.0527\n",
            "Epoch 1 Batch 3321 Loss 6.6525 Accuracy 0.0527\n",
            "Epoch 1 Batch 3322 Loss 6.6522 Accuracy 0.0527\n",
            "Epoch 1 Batch 3323 Loss 6.6519 Accuracy 0.0527\n",
            "Epoch 1 Batch 3324 Loss 6.6517 Accuracy 0.0527\n",
            "Epoch 1 Batch 3325 Loss 6.6514 Accuracy 0.0527\n",
            "Epoch 1 Batch 3326 Loss 6.6512 Accuracy 0.0528\n",
            "Epoch 1 Batch 3327 Loss 6.6509 Accuracy 0.0528\n",
            "Epoch 1 Batch 3328 Loss 6.6507 Accuracy 0.0528\n",
            "Epoch 1 Batch 3329 Loss 6.6504 Accuracy 0.0528\n",
            "Epoch 1 Batch 3330 Loss 6.6501 Accuracy 0.0528\n",
            "Epoch 1 Batch 3331 Loss 6.6499 Accuracy 0.0528\n",
            "Epoch 1 Batch 3332 Loss 6.6496 Accuracy 0.0528\n",
            "Epoch 1 Batch 3333 Loss 6.6494 Accuracy 0.0528\n",
            "Epoch 1 Batch 3334 Loss 6.6492 Accuracy 0.0528\n",
            "Epoch 1 Batch 3335 Loss 6.6489 Accuracy 0.0528\n",
            "Epoch 1 Batch 3336 Loss 6.6487 Accuracy 0.0528\n",
            "Epoch 1 Batch 3337 Loss 6.6483 Accuracy 0.0528\n",
            "Epoch 1 Batch 3338 Loss 6.6481 Accuracy 0.0528\n",
            "Epoch 1 Batch 3339 Loss 6.6477 Accuracy 0.0528\n",
            "Epoch 1 Batch 3340 Loss 6.6475 Accuracy 0.0528\n",
            "Epoch 1 Batch 3341 Loss 6.6472 Accuracy 0.0528\n",
            "Epoch 1 Batch 3342 Loss 6.6470 Accuracy 0.0528\n",
            "Epoch 1 Batch 3343 Loss 6.6466 Accuracy 0.0528\n",
            "Epoch 1 Batch 3344 Loss 6.6464 Accuracy 0.0528\n",
            "Epoch 1 Batch 3345 Loss 6.6461 Accuracy 0.0528\n",
            "Epoch 1 Batch 3346 Loss 6.6459 Accuracy 0.0528\n",
            "Epoch 1 Batch 3347 Loss 6.6457 Accuracy 0.0528\n",
            "Epoch 1 Batch 3348 Loss 6.6455 Accuracy 0.0528\n",
            "Epoch 1 Batch 3349 Loss 6.6454 Accuracy 0.0528\n",
            "Epoch 1 Batch 3350 Loss 6.6452 Accuracy 0.0528\n",
            "Epoch 1 Batch 3351 Loss 6.6450 Accuracy 0.0528\n",
            "Epoch 1 Batch 3352 Loss 6.6446 Accuracy 0.0529\n",
            "Epoch 1 Batch 3353 Loss 6.6443 Accuracy 0.0529\n",
            "Epoch 1 Batch 3354 Loss 6.6440 Accuracy 0.0529\n",
            "Epoch 1 Batch 3355 Loss 6.6437 Accuracy 0.0529\n",
            "Epoch 1 Batch 3356 Loss 6.6436 Accuracy 0.0529\n",
            "Epoch 1 Batch 3357 Loss 6.6433 Accuracy 0.0529\n",
            "Epoch 1 Batch 3358 Loss 6.6431 Accuracy 0.0529\n",
            "Epoch 1 Batch 3359 Loss 6.6427 Accuracy 0.0529\n",
            "Epoch 1 Batch 3360 Loss 6.6425 Accuracy 0.0529\n",
            "Epoch 1 Batch 3361 Loss 6.6422 Accuracy 0.0529\n",
            "Epoch 1 Batch 3362 Loss 6.6420 Accuracy 0.0529\n",
            "Epoch 1 Batch 3363 Loss 6.6419 Accuracy 0.0529\n",
            "Epoch 1 Batch 3364 Loss 6.6416 Accuracy 0.0529\n",
            "Epoch 1 Batch 3365 Loss 6.6413 Accuracy 0.0529\n",
            "Epoch 1 Batch 3366 Loss 6.6411 Accuracy 0.0529\n",
            "Epoch 1 Batch 3367 Loss 6.6408 Accuracy 0.0529\n",
            "Epoch 1 Batch 3368 Loss 6.6406 Accuracy 0.0529\n",
            "Epoch 1 Batch 3369 Loss 6.6404 Accuracy 0.0529\n",
            "Epoch 1 Batch 3370 Loss 6.6402 Accuracy 0.0529\n",
            "Epoch 1 Batch 3371 Loss 6.6400 Accuracy 0.0529\n",
            "Epoch 1 Batch 3372 Loss 6.6397 Accuracy 0.0529\n",
            "Epoch 1 Batch 3373 Loss 6.6395 Accuracy 0.0529\n",
            "Epoch 1 Batch 3374 Loss 6.6392 Accuracy 0.0529\n",
            "Epoch 1 Batch 3375 Loss 6.6390 Accuracy 0.0530\n",
            "Epoch 1 Batch 3376 Loss 6.6388 Accuracy 0.0530\n",
            "Epoch 1 Batch 3377 Loss 6.6385 Accuracy 0.0530\n",
            "Epoch 1 Batch 3378 Loss 6.6382 Accuracy 0.0530\n",
            "Epoch 1 Batch 3379 Loss 6.6379 Accuracy 0.0530\n",
            "Epoch 1 Batch 3380 Loss 6.6377 Accuracy 0.0530\n",
            "Epoch 1 Batch 3381 Loss 6.6374 Accuracy 0.0530\n",
            "Epoch 1 Batch 3382 Loss 6.6371 Accuracy 0.0530\n",
            "Epoch 1 Batch 3383 Loss 6.6370 Accuracy 0.0530\n",
            "Epoch 1 Batch 3384 Loss 6.6366 Accuracy 0.0530\n",
            "Epoch 1 Batch 3385 Loss 6.6364 Accuracy 0.0530\n",
            "Epoch 1 Batch 3386 Loss 6.6362 Accuracy 0.0530\n",
            "Epoch 1 Batch 3387 Loss 6.6359 Accuracy 0.0530\n",
            "Epoch 1 Batch 3388 Loss 6.6356 Accuracy 0.0530\n",
            "Epoch 1 Batch 3389 Loss 6.6353 Accuracy 0.0530\n",
            "Epoch 1 Batch 3390 Loss 6.6351 Accuracy 0.0530\n",
            "Epoch 1 Batch 3391 Loss 6.6348 Accuracy 0.0530\n",
            "Epoch 1 Batch 3392 Loss 6.6346 Accuracy 0.0530\n",
            "Epoch 1 Batch 3393 Loss 6.6343 Accuracy 0.0530\n",
            "Epoch 1 Batch 3394 Loss 6.6340 Accuracy 0.0530\n",
            "Epoch 1 Batch 3395 Loss 6.6339 Accuracy 0.0530\n",
            "Epoch 1 Batch 3396 Loss 6.6336 Accuracy 0.0530\n",
            "Epoch 1 Batch 3397 Loss 6.6334 Accuracy 0.0530\n",
            "Epoch 1 Batch 3398 Loss 6.6332 Accuracy 0.0530\n",
            "Epoch 1 Batch 3399 Loss 6.6329 Accuracy 0.0531\n",
            "Epoch 1 Batch 3400 Loss 6.6327 Accuracy 0.0531\n",
            "Epoch 1 Batch 3401 Loss 6.6324 Accuracy 0.0531\n",
            "Epoch 1 Batch 3402 Loss 6.6322 Accuracy 0.0531\n",
            "Epoch 1 Batch 3403 Loss 6.6320 Accuracy 0.0531\n",
            "Epoch 1 Batch 3404 Loss 6.6318 Accuracy 0.0531\n",
            "Epoch 1 Batch 3405 Loss 6.6316 Accuracy 0.0531\n",
            "Epoch 1 Batch 3406 Loss 6.6313 Accuracy 0.0531\n",
            "Epoch 1 Batch 3407 Loss 6.6311 Accuracy 0.0531\n",
            "Epoch 1 Batch 3408 Loss 6.6308 Accuracy 0.0531\n",
            "Epoch 1 Batch 3409 Loss 6.6305 Accuracy 0.0531\n",
            "Epoch 1 Batch 3410 Loss 6.6302 Accuracy 0.0531\n",
            "Epoch 1 Batch 3411 Loss 6.6299 Accuracy 0.0531\n",
            "Epoch 1 Batch 3412 Loss 6.6297 Accuracy 0.0531\n",
            "Epoch 1 Batch 3413 Loss 6.6295 Accuracy 0.0531\n",
            "Epoch 1 Batch 3414 Loss 6.6292 Accuracy 0.0531\n",
            "Epoch 1 Batch 3415 Loss 6.6290 Accuracy 0.0531\n",
            "Epoch 1 Batch 3416 Loss 6.6288 Accuracy 0.0531\n",
            "Epoch 1 Batch 3417 Loss 6.6287 Accuracy 0.0531\n",
            "Epoch 1 Batch 3418 Loss 6.6284 Accuracy 0.0531\n",
            "Epoch 1 Batch 3419 Loss 6.6281 Accuracy 0.0531\n",
            "Epoch 1 Batch 3420 Loss 6.6279 Accuracy 0.0531\n",
            "Epoch 1 Batch 3421 Loss 6.6277 Accuracy 0.0531\n",
            "Epoch 1 Batch 3422 Loss 6.6275 Accuracy 0.0531\n",
            "Epoch 1 Batch 3423 Loss 6.6272 Accuracy 0.0531\n",
            "Epoch 1 Batch 3424 Loss 6.6269 Accuracy 0.0531\n",
            "Epoch 1 Batch 3425 Loss 6.6267 Accuracy 0.0532\n",
            "Epoch 1 Batch 3426 Loss 6.6264 Accuracy 0.0532\n",
            "Epoch 1 Batch 3427 Loss 6.6262 Accuracy 0.0532\n",
            "Epoch 1 Batch 3428 Loss 6.6259 Accuracy 0.0532\n",
            "Epoch 1 Batch 3429 Loss 6.6257 Accuracy 0.0532\n",
            "Epoch 1 Batch 3430 Loss 6.6255 Accuracy 0.0532\n",
            "Epoch 1 Batch 3431 Loss 6.6253 Accuracy 0.0532\n",
            "Epoch 1 Batch 3432 Loss 6.6250 Accuracy 0.0532\n",
            "Epoch 1 Batch 3433 Loss 6.6246 Accuracy 0.0532\n",
            "Epoch 1 Batch 3434 Loss 6.6243 Accuracy 0.0532\n",
            "Epoch 1 Batch 3435 Loss 6.6240 Accuracy 0.0532\n",
            "Epoch 1 Batch 3436 Loss 6.6237 Accuracy 0.0532\n",
            "Epoch 1 Batch 3437 Loss 6.6235 Accuracy 0.0532\n",
            "Epoch 1 Batch 3438 Loss 6.6233 Accuracy 0.0532\n",
            "Epoch 1 Batch 3439 Loss 6.6231 Accuracy 0.0532\n",
            "Epoch 1 Batch 3440 Loss 6.6228 Accuracy 0.0532\n",
            "Epoch 1 Batch 3441 Loss 6.6225 Accuracy 0.0532\n",
            "Epoch 1 Batch 3442 Loss 6.6222 Accuracy 0.0532\n",
            "Epoch 1 Batch 3443 Loss 6.6220 Accuracy 0.0532\n",
            "Epoch 1 Batch 3444 Loss 6.6217 Accuracy 0.0532\n",
            "Epoch 1 Batch 3445 Loss 6.6214 Accuracy 0.0532\n",
            "Epoch 1 Batch 3446 Loss 6.6212 Accuracy 0.0532\n",
            "Epoch 1 Batch 3447 Loss 6.6210 Accuracy 0.0532\n",
            "Epoch 1 Batch 3448 Loss 6.6208 Accuracy 0.0532\n",
            "Epoch 1 Batch 3449 Loss 6.6206 Accuracy 0.0532\n",
            "Epoch 1 Batch 3450 Loss 6.6205 Accuracy 0.0533\n",
            "Epoch 1 Batch 3451 Loss 6.6202 Accuracy 0.0533\n",
            "Epoch 1 Batch 3452 Loss 6.6200 Accuracy 0.0533\n",
            "Epoch 1 Batch 3453 Loss 6.6198 Accuracy 0.0533\n",
            "Epoch 1 Batch 3454 Loss 6.6196 Accuracy 0.0533\n",
            "Epoch 1 Batch 3455 Loss 6.6193 Accuracy 0.0533\n",
            "Epoch 1 Batch 3456 Loss 6.6190 Accuracy 0.0533\n",
            "Epoch 1 Batch 3457 Loss 6.6188 Accuracy 0.0533\n",
            "Epoch 1 complete - Loss 6.6188 Accuracy 0.0533\n",
            "Time taken for 1 epoch: 1170.17 secs\n",
            "\n",
            "Epoch 2: Re-initializing metrics as a workaround.\n",
            "Epoch 2 Batch 0 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 1 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 2 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 3 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 4 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 5 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 6 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 7 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 8 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 9 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 10 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 11 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 12 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 13 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 14 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 15 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 16 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 17 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 18 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 19 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 20 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 21 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 22 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 23 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 24 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 25 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 26 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 27 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 28 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 29 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 30 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 31 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 32 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 33 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 34 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 35 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 36 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 37 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 38 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 39 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 40 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 41 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 42 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 43 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 44 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 45 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 46 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 47 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 48 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 49 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 50 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 51 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 52 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 53 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 54 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 55 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 56 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 57 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 58 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 59 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 60 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 61 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 62 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 63 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 64 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 65 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 66 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 67 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 68 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 69 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 70 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 71 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 72 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 73 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 74 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 75 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 76 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 77 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 78 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 79 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 80 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 81 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 82 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 83 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 84 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 85 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 86 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 87 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 88 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 89 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 90 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 91 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 92 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 93 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 94 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 95 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 96 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 97 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 98 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 99 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 100 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 101 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 102 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 103 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 104 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 105 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 106 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 107 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 108 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 109 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 110 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 111 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 112 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 113 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 114 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 115 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 116 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 117 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 118 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 119 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 120 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 121 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 122 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 123 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 124 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 125 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 126 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 127 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 128 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 129 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 130 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 131 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 132 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 133 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 134 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 135 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 136 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 137 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 138 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 139 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 140 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 141 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 142 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 143 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 144 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 145 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 146 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 147 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 148 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 149 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 150 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 151 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 152 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 153 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 154 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 155 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 156 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 157 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 158 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 159 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 160 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 161 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 162 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 163 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 164 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 165 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 166 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 167 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 168 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 169 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 170 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 171 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 172 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 173 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 174 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 175 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 176 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 177 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 178 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 179 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 180 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 181 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 182 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 183 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 184 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 185 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 186 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 187 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 188 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 189 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 190 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 191 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 192 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 193 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 194 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 195 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 196 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 197 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 198 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 199 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 200 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 201 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 202 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 203 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 204 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 205 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 206 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 207 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 208 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 209 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 210 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 211 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 212 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 213 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 214 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 215 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 216 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 217 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 218 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 219 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 220 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 221 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 222 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 223 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 224 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 225 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 226 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 227 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 228 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 229 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 230 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 231 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 232 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 233 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 234 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 235 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 236 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 237 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 238 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 239 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 240 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 241 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 242 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 243 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 244 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 245 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 246 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 247 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 248 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 249 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 250 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 251 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 252 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 253 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 254 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 255 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 256 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 257 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 258 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 259 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 260 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 261 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 262 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 263 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 264 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 265 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 266 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 267 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 268 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 269 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 270 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 271 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 272 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 273 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 274 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 275 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 276 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 277 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 278 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 279 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 280 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 281 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 282 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 283 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 284 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 285 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 286 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 287 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 288 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 289 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 290 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 291 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 292 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 293 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 294 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 295 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 296 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 297 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 298 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 299 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 300 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 301 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 302 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 303 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 304 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 305 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 306 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 307 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 308 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 309 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 310 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 311 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 312 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 313 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 314 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 315 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 316 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 317 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 318 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 319 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 320 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 321 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 322 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 323 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 324 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 325 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 326 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 327 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 328 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 329 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 330 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 331 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 332 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 333 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 334 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 335 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 336 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 337 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 338 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 339 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 340 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 341 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 342 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 343 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 344 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 345 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 346 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 347 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 348 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 349 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 350 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 351 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 352 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 353 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 354 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 355 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 356 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 357 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 358 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 359 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 360 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 361 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 362 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 363 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 364 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 365 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 366 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 367 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 368 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 369 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 370 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 371 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 372 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 373 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 374 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 375 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 376 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 377 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 378 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 379 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 380 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 381 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 382 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 383 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 384 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 385 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 386 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 387 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 388 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 389 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 390 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 391 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 392 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 393 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 394 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 395 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 396 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 397 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 398 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 399 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 400 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 401 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 402 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 403 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 404 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 405 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 406 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 407 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 408 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 409 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 410 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 411 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 412 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 413 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 414 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 415 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 416 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 417 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 418 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 419 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 420 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 421 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 422 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 423 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 424 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 425 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 426 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 427 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 428 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 429 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 430 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 431 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 432 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 433 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 434 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 435 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 436 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 437 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 438 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 439 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 440 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 441 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 442 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 443 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 444 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 445 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 446 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 447 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 448 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 449 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 450 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 451 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 452 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 453 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 454 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 455 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 456 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 457 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 458 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 459 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 460 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 461 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 462 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 463 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 464 Loss 0.0000 Accuracy 0.0000\n",
            "Epoch 2 Batch 465 Loss 0.0000 Accuracy 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 12: Inference (Generating Responses) with Input Loop\n",
        "\n",
        "# Ensure these variables/objects are defined in previous cells and accessible:\n",
        "# - transformer (the instantiated Transformer model from Cell 8)\n",
        "# - tokenizer (from Cell 2)\n",
        "# - MAX_LENGTH (from Cell 3)\n",
        "# - ckpt_manager (from Cell 10)\n",
        "# - clean_text (from Cell 2)\n",
        "# - create_masks (from Cell 5)\n",
        "# - ckpt (from Cell 10) # Needed for ckpt.restore\n",
        "\n",
        "\n",
        "# Function to convert sequence of tokens back to text\n",
        "def tokens_to_text(sequence, tokenizer):\n",
        "    \"\"\"Converts a sequence of token IDs back to text string.\"\"\"\n",
        "    # Initialize list to store words\n",
        "    words = []\n",
        "    # Iterate through the sequence of token IDs\n",
        "    for token in sequence:\n",
        "        # Check if the token ID is valid (not 0 for padding) and not a special token\n",
        "        if token != 0:\n",
        "             # Get the word for the token ID from the tokenizer's index_word mapping\n",
        "            word = tokenizer.index_word.get(token, '') # Use .get to handle potential missing tokens\n",
        "            # Append the word to the list if it's not a special start/end token\n",
        "            if word not in ('<start>', '<end>'):\n",
        "                words.append(word)\n",
        "        else:\n",
        "             # Stop if a padding token is encountered (assuming post-padding)\n",
        "            break\n",
        "\n",
        "    # Join the list of words into a single string\n",
        "    # Handle cases where the sequence might be empty or only contain special tokens\n",
        "    return ' '.join(words) if words else ''\n",
        "\n",
        "\n",
        "# Function to generate a response given an input sentence\n",
        "def evaluate(sentence, transformer, tokenizer, max_length):\n",
        "    \"\"\"Generates a response token by token for a given input sentence.\"\"\"\n",
        "\n",
        "    # Preprocess the input sentence using the same cleaning as training data\n",
        "    cleaned_sentence = clean_text(sentence)\n",
        "    input_sequence = tokenizer.texts_to_sequences([cleaned_sentence])\n",
        "\n",
        "    # Pad the input sequence to the maximum length used during training\n",
        "    encoder_input = tf.keras.preprocessing.sequence.pad_sequences(input_sequence,\n",
        "                                                                maxlen=max_length,\n",
        "                                                                padding='post')\n",
        "    encoder_input = tf.constant(encoder_input) # Convert to TensorFlow tensor\n",
        "\n",
        "    # The decoder input starts with the <start> token\n",
        "    # We need to know the ID of the <start> token from the tokenizer\n",
        "    try:\n",
        "        start_token = tokenizer.word_index['<start>']\n",
        "        end_token = tokenizer.word_index['<end>']\n",
        "    except KeyError:\n",
        "        print(\"Error: '<start>' or '<end>' token not found in tokenizer vocabulary.\")\n",
        "        print(\"Please ensure these tokens were added during data preparation (Cell 2).\")\n",
        "        return \"Error: Special tokens not found.\", {}\n",
        "\n",
        "\n",
        "    # Initialize the decoder input sequence with the <start> token\n",
        "    decoder_input = tf.expand_dims([start_token], 0) # Shape (1, 1) -> batch size 1, sequence length 1\n",
        "\n",
        "    # List to store the output token IDs\n",
        "    output_tokens = []\n",
        "\n",
        "    # Generate tokens one by one up to max_length\n",
        "    # Use a loop limit to prevent infinite generation\n",
        "    for i in range(max_length): # Use max_length as a safety limit for generated sequence\n",
        "        # Create masks for the current step of decoding\n",
        "        # enc_padding_mask is based on the fixed encoder_input\n",
        "        # combined_mask is based on the growing decoder_input (look-ahead and padding)\n",
        "        # dec_padding_mask is based on the fixed encoder_input (for cross-attention)\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, decoder_input)\n",
        "\n",
        "        # Get the model's predictions for the next token\n",
        "        # Pass training=False for inference and use keyword arguments for masks\n",
        "        predictions, attention_weights = transformer(encoder_input,\n",
        "                                                     decoder_input,\n",
        "                                                     training=False, # Set training to False for inference\n",
        "                                                     enc_padding_mask=enc_padding_mask,\n",
        "                                                     look_ahead_mask=combined_mask, # Use the correct param name\n",
        "                                                     dec_padding_mask=dec_padding_mask) # Use the correct param name\n",
        "\n",
        "\n",
        "        # Select the last token prediction from the sequence dimension\n",
        "        # predictions.shape == (batch_size, current_seq_len, vocab_size)\n",
        "        # We are only interested in the prediction for the *last* token in decoder_input\n",
        "        predictions = predictions[:, -1:, :] # Shape (batch_size, 1, vocab_size)\n",
        "\n",
        "        # Get the predicted token ID by taking the argmax (most likely token)\n",
        "        # predicted_token_id will have shape (batch_size, 1)\n",
        "        predicted_token_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "        # Get the scalar token ID (assuming batch size is 1 for inference)\n",
        "        predicted_token_scalar = predicted_token_id.numpy()[0][0]\n",
        "\n",
        "        # Append the predicted token ID to the output tokens list\n",
        "        output_tokens.append(predicted_token_scalar)\n",
        "\n",
        "        # If the predicted token is the <end> token, stop generation\n",
        "        if predicted_token_scalar == end_token:\n",
        "            break\n",
        "\n",
        "        # Concatenate the predicted token to the decoder input for the next step\n",
        "        # decoder_input will grow by one token each iteration\n",
        "        decoder_input = tf.concat([decoder_input, predicted_token_id], axis=-1) # Shape (batch_size, current_seq_len + 1)\n",
        "\n",
        "\n",
        "    # Convert the list of output token IDs to a tensor and squeeze (remove batch dim)\n",
        "    # output_tokens is a Python list of ints, convert to numpy array first\n",
        "    output_tokens_np = np.array(output_tokens, dtype=np.int32)\n",
        "    # Convert numpy array to tensor\n",
        "    output_tokens_tensor = tf.constant(output_tokens_np)\n",
        "    # Squeeze the tensor to remove any extra dimensions (like batch size 1)\n",
        "    # tf.squeeze is not needed if we are converting a 1D list/array\n",
        "\n",
        "\n",
        "    # Convert the output token sequence back to text using the helper function\n",
        "    predicted_sentence = tokens_to_text(output_tokens_tensor.numpy(), tokenizer)\n",
        "\n",
        "    # Note: We are not returning attention weights in this version of evaluate\n",
        "    # for simplicity, but you could return the 'attention_weights' variable\n",
        "    # from the transformer call if needed for visualization.\n",
        "    return predicted_sentence, attention_weights\n",
        "\n",
        "\n",
        "# --- Inference Setup ---\n",
        "\n",
        "print(\"\\n--- Setting up Inference ---\")\n",
        "\n",
        "# Restore latest checkpoint if training was run\n",
        "# This loads the trained weights into the transformer and optimizer objects\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    # Use expect_partial() because the optimizer state might not fully match\n",
        "    # if you stopped training early or changed settings slightly.\n",
        "    # For pure inference, restoring only model weights is often sufficient.\n",
        "    # Assuming 'ckpt' object was created in Cell 10 and includes the 'transformer' and 'optimizer'\n",
        "    try:\n",
        "        ckpt.restore(ckpt_manager.latest_checkpoint).expect_partial()\n",
        "        print('Latest checkpoint restored for inference.')\n",
        "    except Exception as e:\n",
        "        print(f\"Error restoring checkpoint: {e}\")\n",
        "        print(\"Model weights are untrained or from initial load.\")\n",
        "        print(\"Inference may not produce meaningful results without training.\")\n",
        "else:\n",
        "     print('No checkpoint found. Model weights are untrained or from initial load.')\n",
        "     print('Inference may not produce meaningful results without training.')\n",
        "\n",
        "\n",
        "# --- Interactive Chat Loop ---\n",
        "\n",
        "print(\"\\n--- Start Chat ---\")\n",
        "print(\"Type 'quit' or 'exit' to end the chat.\")\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        user_input = input(\"You: \")\n",
        "\n",
        "        if user_input.lower() in ['quit', 'exit']:\n",
        "            print(\"Chat ended.\")\n",
        "            break\n",
        "\n",
        "        # Generate response using the evaluate function\n",
        "        # We discard attention_weights here\n",
        "        response, _ = evaluate(user_input, transformer, tokenizer, MAX_LENGTH)\n",
        "\n",
        "        # Print the generated response\n",
        "        print(f\"Bot: {response}\")\n",
        "        print(\"-\" * 20) # Separator for clarity\n",
        "\n",
        "    except tf.errors.OutOfRangeError:\n",
        "        print(\"Error: Dataset finished during evaluation? This is unexpected.\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during inference: {e}\")\n",
        "        # Continue the loop or break depending on desired error handling\n",
        "        # break # Uncomment to stop chat on any error\n",
        "        print(\"Attempting to continue chat...\")"
      ],
      "metadata": {
        "id": "pYoit4Uuw-Bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 13: Addressing Advanced Components & Next Steps\n",
        "print(\"--- Advanced Components & Next Steps ---\")\n",
        "\n",
        "print(\"\\nKey components mentioned by user:\")\n",
        "print(\"1. Core transformer architecture: Covered by implementing layers and structure.\")\n",
        "print(\"2. Advanced training techniques:\")\n",
        "print(\"   * Pre-training on vast text corpora: Requires *massive* datasets (terabytes), significant computational resources (clusters of GPUs for weeks/months), and complex distributed training setups. Our demo uses tiny data.\")\n",
        "print(\"   * Fine-tuning with reinforcement learning from human feedback (RLHF): A complex multi-stage process involving training a reward model from human preferences and then fine-tuning the language model using RL (e.g., Proximal Policy Optimization). Beyond the scope of a basic Colab demo.\")\n",
        "print(\"   * Constitutional AI approaches for safety and alignment: Advanced techniques building on RLHF, using AI feedback based on principles/constitutions. Also highly advanced and resource-intensive.\")\n",
        "print(\"3. System design components:\")\n",
        "print(\"   * Efficient inference infrastructure: For large models, requires specialized hardware (TPUs, high-end GPUs), optimized libraries (TensorRT, TensorFlow Lite), and deployment strategies (serving frameworks). Colab provides a single GPU, good for small models/demos.\")\n",
        "print(\"   * Prompt engineering and context management: Crucial for interacting with large models. Our demo's context is just the current turn. Real systems need robust history tracking and prompt formatting.\")\n",
        "print(\"   * Memory systems for conversation history: Requires storing and potentially summarizing or selecting relevant past conversation turns to feed into the model's context window. Our demo lacks this.\")\n",
        "print(\"   * Moderation layers for safety: Additional systems (often separate models or rule-based filters) to detect and prevent harmful, biased, or inappropriate responses. Our demo has no safety mechanisms.\")\n",
        "\n",
        "print(\"\\nSummary:\")\n",
        "print(\"This notebook provides a foundational implementation of the core Transformer *architecture* components (Self-Attention, FFN, Norm, Residual).\")\n",
        "print(\"It demonstrates basic data preparation, training loop setup, and inference *concept* on a tiny dataset.\")\n",
        "print(\"\\nTo build a truly effective chat system, you would need:\")\n",
        "print(\"1.  Vastly more data and computational resources for pre-training/training.\")\n",
        "print(\"2.  Implementation of advanced techniques like RLHF and potentially Constitutional AI.\")\n",
        "print(\"3.  Sophisticated system design for context management, efficient serving, and safety.\")\n",
        "print(\"\\nStarting with readily available large pre-trained models (like those from Hugging Face Transformers) and fine-tuning them on a smaller, domain-specific dataset is a far more practical path for building a functional chat system in Colab or with limited resources than training a powerful model from scratch.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4adfC21Xw-sG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}